[
    
    
    
        
            {
                "id": 0,
                "href": "https://www.if-blog.site/posts/paper/paper25/",
                "title": "EVALUATING THE DIVERSITY AND QUALITY OF LLM GENERATED CONTENT",
                "section": "posts",
                "date" : "2025.05.21",
                "body": "https://arxiv.org/abs/2504.12522\nAIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nAIが作る文章やプログラムの「質」と「種類の豊富さ（多様性）」をどう評価するかの研究です。この論文では、ただ多様なだけでなく「使える（質の高い）」アウトプットがどれだけ多様か、という「実質的な意味的多様性」を測る新しい方法を提案しています。実験の結果、特定の調整をされたAIは、表現の多様性は減るものの、質の高いアウトプットをより多く作るため、結果として「実質的な意味的多様性」は高くなることが分かりました。\nサマリー 本論文は、大規模言語モデル（LLM）が生成するコンテンツの質と多様性を評価するための新しいフレームワーク「実質的な意味的多様性（effective semantic diversity）」を提案する 。これは、品質のしきい値を満たすアウトプット間での多様性を測定するものであり、LLMの実用的な有用性をより良く反映する。人間による評価を必要としない自由形式のタスクを用いて評価した結果、選好チューニングされたモデル（特に強化学習（RL）で訓練されたモデル）は、語彙的・統語的多様性は減少するものの、高品質なアウトプットを全体的により多く生成するため、教師ありファインチューニング（SFT）モデルやベースモデルよりも実質的な意味的多様性が高くなることが示された。選好チューニングは統語的多様性を減らしつつ意味的多様性を保持し、従来の指標が見落としがちな形式の多様性と内容の多様性の違いを明らかにした 。さらに、より小さなモデルの方が、固定されたサンプリングバジェット内でユニークなコンテンツを生成する上でパラメータ効率が良いことが示された。\n主要なテーマと重要なアイデア 実質的な意味的多様性 (Effective Semantic Diversity): 品質基準を満たす生成物の中での意味的な多様性を指す。ランダムなトークンのように単に多様であるだけでは不十分であり、実用的な価値を持つのは質の高い生成物における多様性であるという考え方。 品質と多様性のトレードオフ: LLMの評価において、品質と多様性の両方を考慮する必要がある。本研究では、この二つの要素の相互作用を考慮した評価フレームワークを提案している。 選好チューニングの影響: RLHF（PPO、GRPOなど）やDPOといった選好チューニング技術は、語彙的・統語的多様性を減少させる傾向があるが、高品質な生成物の割合を増やすことで、結果的に実質的な意味的多様性を向上させる。特にRLアルゴリズムは、DPOと比較して語彙的・統語的多様性をより大きく減少させるが、意味的多様性は保持する。 形式の多様性と内容の多様性: 選好チューニングは統語的多様性を減らす一方で意味的多様性を保持することから、生成物の「形式（form）」の多様性と「内容（content）」の多様性は区別して考える必要がある。 モデルサイズの影響: より大きなモデルは、語彙的・統語的多様性を犠牲にすることなく、より高い意味的多様性を示す傾向がある。しかし、ユニークなプログラムを生成する際のパラメータ効率では、5億パラメータ程度のより小さなモデルの方が優れている場合がある。 評価手法: プログラム生成タスクと自動実行テストケースを用いて、品質（有効性）と意味的同等性を評価する。具体的には、生成されたプログラムがエラーなく実行され、全てのテストケースに対して非NULLの出力を生成するかどうかで有効性を判断し、テストセットに対する出力トレースが同一かどうかで意味的同等性を判断する。提案された実質的な意味的多様性は、次の式で定義される。 $$Div_{fixed}(\\mathcal{P}_{i})=\\frac{|Set(\\{S(g_{i}^{k})|g_{i}^{k}\\in\\mathcal{P}_{i},V(g_{i}^{k})=1\\})|}{K}$$ (Equation 2) または、サンプルサイズ効果に対処するためにペアワイズの多様性指標を用いる。 $$Div_{pair}(\\mathcal{P}_{i})=\\frac{1}{\\binom{K}{2}}\\sum_{g_{i}^{j},g_{i}^{k}\\in\\mathcal{P}_{i}} d_{sem}(g_{i}^{j},g_{i}^{k})$$ (Equation 3) ここで、$d_{sem}$は意味的距離関数である。 次の課題 本研究で提案されたフレームワークと方法論を、プログラム生成以外のドメイン、例えば自然言語生成などへ拡張すること。 より多くのサンプル数で実験を行い、特定のモデルクラスにおいて意味的にユニークなプログラムの飽和が他のクラスよりも速く起こるかどうかなど、サンプリングバジェットと多様性の関係をさらに調査すること。 本研究フレームワークを活用し、将来のポストトレーニング戦略が多様性にどのように影響するかを評価すること。 ニューラル多様性指標がLLM生成アウトプットにおける実質的な意味的内容の多様性を反映しているかについてのさらなる評価。特に、多様性を評価するために使用されるモデル（例：Sentence-BERT）は人間のテキストで訓練・評価されているが、LLMが生成する多様なテキスト（分布外の可能性あり）に対して頑健に一般化できるかは不明である。 "
            }
    
        ,
            {
                "id": 1,
                "href": "https://www.if-blog.site/posts/paper/paper24/",
                "title": "Texygen: A Benchmarking Platform for Text Generation Models",
                "section": "posts",
                "date" : "2025.05.21",
                "body": "AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nhttps://arxiv.org/abs/1802.01886\nサマリー 本論文では、オープン ドメイン テキスト生成モデルの研究を支援するためのベンチマーク プラットフォームである Texygen を紹介する。Texygen は、多数のテキスト生成モデルを実装しているだけでなく、生成されたテキストの多様性、品質、一貫性を評価する一連のメトリクスもカバーしている。このプラットフォームは、テキスト生成に関する研究の標準化、研究者間での微調整されたオープンソース実装の共有の促進、そして将来の研究作業の再現性と信頼性の向上に貢献することを目指している。\n主要なテーマと重要なアイデア オープン ドメイン テキスト生成の課題: 論文では、テキスト生成における3つの主要な課題を指摘している。第一に、優れたテキスト生成モデルの基準が明確でないこと。第二に、研究者がソースコードを公開する義務がないため、報告された実験結果の再現が困難であること。第三に、テキスト生成は品質と多様性のトレードオフ問題に悩まされており、モード崩壊によって出力が限定的なパターンに縮小する可能性があることである。\nTexygen プラットフォーム: この課題に対応するため、Texygen プラットフォームが提案された。Texygen は、テキスト生成モデルの標準化されたトップダウンの多次元評価システムを提供する。プラットフォームは、適切にトレーニングされたベースラインモデルと、自動的に計算可能な評価メトリクスの2つの要素で構成されている。\nベースラインモデル: Texygen には、vanilla MLE、SeqGAN、MaliGAN、RankGAN、TextGAN、GSGAN、LeakGAN など、さまざまな尤度ベースのモデル、敵対的メソッド、階層的メソッドが含まれている。 Vanilla MLE: $max_{\\theta}\\sum_{x}\\sum_{t}log~\\pi_{\\theta}(x_{t}|s_{t})$ SeqGAN: ジェネレーターは REINFORCE アルゴリズムを使用して GAN の目的関数を最適化する: $$min_{\\phi}-\\bigoplus_{Y\\sim p_{datu}}[log~D_{\\phi}(Y)]-\\mathbb{R}_{Y-G_{\\theta}}[log(1-D_{\\phi}(Y))]$$ MaliGAN: 報酬をリスケールする: $r_{D}(x_{i})^{\\prime}=\\frac{r_{D}(x_{i})}{\\sum_{f=1}^{m}r_{D}(x_{f})}-b$。 RankGAN: ランキング損失を最適化する: $L_{\\phi}=\\mathbb{E}_{s\\sim p_{data}}[log~R_{\\phi}(s|U,C^{-})]-\\mathbb{E}_{s\\sim G_{\\theta}}[log~R_{\\phi}(s|U,C^{+})]$。 GSGAN: Gumbel Softmax トリックを使用する: arg max [softmax(h+g)] ~ softmax(h)。 TextGAN: MMD 損失を最適化する: $L_{recon}=||z-\\hat{z}||$。 LeakGAN: Manager と Worker の2つのモジュールを持つ階層型強化学習フレームワーク。 評価メトリクス: Texygen は、BLEU、EmbSim、NLL-oracle、NLL-test、Self-BLEUという5つのテキスト生成メトリクスを実装している。これらは、ドキュメントの類似性に基づくメトリクス、尤度ベースのメトリクス、発散ベースのメトリクスに分類される。 EmbSim: $$EmbSim=log(\\sum_{i=1}^{N}cos(W_{i}^{\\prime},W_{i})/N)$$ NLL-oracle: $$NLL_{oracle}=-\\mathbb{E}_{Y_{1:T-G_{\\rho}}}[\\sum_{t=1}^{T}log(G_{oracle}(y_{t}|Y_{1:t-1}))]$$ NLL-test: $$NLL_{test}=-\\mathbb{B}_{Y_{1:T-G_{teal}}}[\\sum_{t=1}^{T}log(G_{\\theta}(y_{t}|Y_{1:t-1}))]$$ 実験: 合成データと実データ（COCO 画像キャプション）を使用して実験が行われた。$NLL_{oracle}$ と $NLL_{test}$ は合成データトレーニングに適用され、BLEU スコア、Self-BLEU スコア、EmbSim は実データトレーニングに適用された。LeakGAN は多くのメトリクスで良好な性能を示した。\n貢献 Texygen ベンチマーク プラットフォームの導入: オープン ドメイン テキスト生成モデルの研究をサポートする、オープンソースのベンチマーク プラットフォームをリリースした。 多数のベースラインモデルと評価メトリクスの実装: Texygen は、多数のベースラインモデルと、生成テキストの多様性、品質、一貫性を評価するためのさまざまなメトリクスを実装している。 研究の標準化と再現性の向上への貢献: このプラットフォームは、テキスト生成に関する研究の標準化、研究作業の再現性の向上、およびより高度なアプリケーションの奨励に役立つことが期待される。 新しいメトリクスの提案: EmbSim や Self-BLEU といった新しい評価メトリクスを提案した。 既存モデルの包括的な比較研究: さまざまなメトリクスを用いて、既存のテキスト生成モデルの包括的な比較研究を実施した。 評価メトリクス 1. ドキュメント類似性ベースのメトリクス (Document Similarity based Metrics) 生成されたドキュメントの品質を測る最も直感的な方法は、それらが自然言語、つまりトレーニングデータセットにどれほど似ているかです。\nBLEU (Bilingual Evaluation Understudy)\n文やドキュメント間の単語の類似性を評価するために広く使われているメトリクスです。 EmbSim (Embedding Similarity)\nBLEUに触発され、提案されたメトリクスで、「embedding similarity」の略です。 単語ごとに文を比較するのではなく、単語埋め込みを比較します。 まず、実際のデータでskip-gramモデルを用いて単語埋め込みを評価します。 各単語埋め込みについて、他の単語とのコサイン類似度を計算し、これを実データの類似性行列 $W$ として定式化します。ここで、$W_{i,j}=cos(e_{i},e_{j})$ であり、$e_i, e_j$ は実データからの単語iとjの単語埋め込みです。 同様に、生成データから同じskip-gramモデルを用いて単語埋め込み $e\u0026rsquo;_i, e\u0026rsquo;_j$ を得て、類似性行列 $W\u0026rsquo;$ を得ます。 EmbSimは以下のように定義されます: $EmbSim=log(\\sum_{i=1}^{N}cos(W_{i}^{\\prime},W_{i})/N)$ ここで、$N$ は総単語数、$W_i$ と $W\u0026rsquo;_i$ はそれぞれ $W$ と $W\u0026rsquo;$ のi番目の列を示します。 2. 尤度ベースのメトリクス (Likelihood-based Metrics) 真のデータ分布 $p$ とモデルからの生成データ分布 $q$ の間のクロスエントロピーを最小化することを目指す最尤推定（MLE）に基づいて、データの適合度やモデルの良さを尤度を測定することで評価するメトリクスです。これらのモデルは、データだけでなくモデルに関する詳細も必要とします。\nNLL-oracle (Negative Log-Likelihood oracle)\n元々はSeqGANで導入されたもので、特に合成データ実験に適用され、生成されたデータがオラクル言語モデルによってどれだけうまく適合されているかを示します。 $NLL_{oracle}$ では、ランダムに初期化されたLSTMが真のモデル、つまりオラクルとみなされます。 テキスト生成モデルは、オラクルLSTMにおける生成データの平均負の対数尤度、すなわち $\\mathbb{E}_{x\\sim q}log~p(x)$ （$x$ は生成データを指す）を最小化する必要があります。 LSTMが真のモデルとみなされるため、このメトリクスは各文に対して単語ごとに平均損失を計算できます: $$NLL_{oracle}=-\\mathbb{E}_{Y_{1:T-G_{\\rho}}}[\\sum_{t=1}^{T}log(G_{oracle}(y_{t}|Y_{1:t-1}))]$$ ここで、$G_{oracle}$ はオラクルLSTMを、$G_{\\theta}$ は生成モデルを示します。 NLL-test (Negative Log-Likelihood test)\n提案されたメトリクスで、$NLL_{oracle}$ と対になるもので、モデルが実際のテストデータに適合する能力を評価します。 $$NLL_{test}=-\\mathbb{E}_{Y_{1:T \\sim G_{real}}}[\\sum_{t=1}^{T}log(G_{\\theta}(y_{t}|Y_{1:t-1}))]$$ ここで、$G_{real}$ は実データの分布を示します。 $NLL_{test}$ は、$G_{\\theta}(y_{t}|Y_{1:t-1})$ が特定の単語の尤度を計算するためにジェネレータに基づいて以前の単語に基づいて関与するため、RNNのような自己回帰ジェネレータにのみ適用可能です。 3. 発散ベースのメトリクス (Divergence based Metrics) GANモデルはしばしばモード崩壊問題に悩まされ、ジェネレータが単一のサンプルまたは非常に類似したサンプルの小さなファミリーのみを生成するように崩壊する可能性があります。したがって、オープン ドメイン テキスト生成タスクでは、より多様なパターンを生成することを奨励するメトリクスが含まれます。\nSelf-BLEU 生成されたデータの多様性を評価するために提案されたメトリクスです。 BLEUは2つの文がどれほど類似しているかを評価することを目的としているため、生成されたコレクション内で1つの文が残りの文とどれほど類似しているかを評価するためにも使用できます。 1つの文を仮説とし、他の文を参照として、生成された各文に対してBLEUスコアを計算し、その平均BLEUスコアをドキュメントのSelf-BLEUと定義します。 Self-BLEUスコアが高いほど、ドキュメントの多様性が低く、GANモデルのモード崩壊が深刻であることを意味します。 "
            }
    
        ,
            {
                "id": 2,
                "href": "https://www.if-blog.site/posts/chats/chat2/",
                "title": "データセット評価指標",
                "section": "posts",
                "date" : "2025.05.20",
                "body": "以下では LLM 学習用データセットの多様性（あるいは「同質性」を避けられているか）を測る代表的な指標を、定義 → 計算方法 → 読み取り方 → 長所/限界 の順で簡潔に整理します。\ndistinct-N paper\ncode\n定義\n生成コーパス（または学習データ）の中で 重複を除いた n-gram の種類数 を、総 n-gram 数で割った比率。\n$$ \\text{distinct-}n=\\frac{\\lvert{\\text{unique }n\\text{-grams}}\\rvert}{\\text{total }n\\text{-grams}} $$\n読み取り方\n値は 0〜1。1 に近いほど語彙的な反復が少なく、表層的に多様。 distinct-1 / distinct-2（1,2-gram）が最も一般的。 強み\n軽量・参照不要・タスク非依存。 限界 長文ほど分母が大きくなり過大評価されやすい。 内容的（意味的）な多様性は把握できない。 Diversity Coefficient（Task2Vec-DC） paper\n着想 Task2Vec で「サンプル1 バッチ = 1 タスク」と見立て、タスク埋め込み間の平均距離でデータ内部の潜在概念のばらつきを定量化する。\n計算フロー（概要）\nデータセットからランダムに $B$ サンプルを抽出（= 1 タスク）。\nTask2Vec (Barrett et al., 2019) で各タスクをベクトル化（勾配のFisher情報行列対角を縮約）。\nタスク埋め込み集合 ${\\mathbf{t}_i}$ の分散（例：平均コサイン距離や Frobenius ノルム）を取り\n$$ \\text{DC} = \\frac{2}{B(B-1)}\\sum_{i\u0026lt;j} d(\\mathbf{t}_i,\\mathbf{t}_j) $$\n大きいほど「タスクが互いに異なる」= より多様。\n強み 意味レベルでの多様性を高速に近似。\n限界 Task2Vecの前提（モデルと層選択）に依存／語順など表層のばらつきは捉えにくい。\nSelf-BLEU paper\n参考\n定義\nコーパス内の 各文を「仮説」、残り全文を 「参照」 とみなして BLEU を計算し、文ごとに平均。 低いほど多様（互いに似ていない）。 計算\n$$ \\text{Self-BLEU} = \\frac1{N}\\sum_{i=1}^{N}\\text{BLEU}\\bigl(\\text{sent}_i,; \\mathcal{D}\\setminus{\\text{sent}_i}\\bigr) $$\n読み取り方\n0 に近いほど高多様性。 BLEU ベースなので n-gram 重み付けや平滑化は通常の BLEU と同一。 長所 distinct-n より“語順を伴う重複”を強くペナルティ。\n弱点 計算が $O(N^2)$、BLEU の問題（語義を見ない）を継承。 LLM Cluster Score paper\nアイデア\nLLM に各サンプルのメタ特徴（トピックなど）を要約・埋め込みさせる。\n埋め込みをクラスタリング（K-means 等）。\nクラスタ数 $K$ と 各クラスタのエントロピー から\n$$ \\text{Cluster Score}=H(\\text{cluster labels}) \\times \\frac{K}{K_{\\max}} $$\n— 直感的に「サンプルが均等に多くの話題に散らばっているほど高得点」。\n用途 巨大合成コーパスで従来指標が飽和・不安定になる問題を緩和。\n注意点 LLM の“まとめ方”がクラスタ品質を左右／高次元でのクラスタ個数設定がハイパーパラメータになる。\nDCScore paper\ncode\n発想 「もし各サンプルを分類クラスだと思って識別器を訓練したら、正解率が高いほど互いに違う＝多様」と捉える。\n手順\n$N$ サンプルを N-way 1-shot のようにラベル付け（サンプル $i$ はクラス $i$)。 小型テキスト分類器 $f_\\theta$ を数エポックだけ学習。 開発セットでの 分類精度を Diversity Classification Score (DCScore) として採用。 高精度 = サンプルを容易に区別 ⇒ データが互いに異質で多様 低精度 = 似た文が多く誤分類 ⇒ 同質的 利点 語彙・構文・意味差分をまとめて反映しつつ計算コストを抑制（学習回数はごく少ない）。\n欠点 分類器設計・学習ハイパラに感度がある。\nまとめ 指標 捉える多様性 軽量 意味情報 大規模データ適性 主な弱点 distinct-N 表層 n-gram ◎ ✕ ◎ 長文補正が必要 Self-BLEU 表層＋語順 △ ✕ ○ (要サンプリング) 計算 $O(N^2)$ Diversity Coefficient 概念・タスク ○ ◎ ◎ Task2Vec 依存 LLM Cluster Score トピック分布 ○ ◎ ◎ クラスタ数選定 DCScore サンプル識別難度 △ ◎ ○ 分類器依存 高速スクリーニング → distinct-N, Self-BLEU 意味レベルの網羅性 → DC, LLM Cluster, DCScore 超大規模コーパス → DC, LLM Cluster（計算がバッチ並列化しやすい） 生成モデルの出力評価 → distinct-N + Self-BLEU（表層反復チェック） これらの指標を併用し、表層と意味の両面からバランス良く評価することが、実際の LLM 学習データ選定では推奨されます。\n"
            }
    
        ,
            {
                "id": 3,
                "href": "https://www.if-blog.site/posts/paper/paper23/",
                "title": "Measuring Diversity in Synthetic Datasets",
                "section": "posts",
                "date" : "2025.05.20",
                "body": "AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nhttps://arxiv.org/abs/2502.08512\nhttps://github.com/bluewhalelab/dcscore\nサマリー 大規模言語モデル（LLM）が生成する synthetic datasets の多様性を定量的に測定するため，著者らは DCScore を提案した。DCScore は「多様性をサンプル分類タスクとして扱う」という視点を導入し，\n各サンプル間の相互関係を Holistic にとらえる， effective number／identical samples／symmetry／monotonicity の 4 つの Axioms を理論的に満たす， 非線形カーネル使用時に従来法（VendiScore など）より 低計算コストを達成する， という特長を示した。実験では生成温度 τg・人間評価・GPT-4 評価との高い相関を確認し，多数のデータセットでベースラインを凌駕した。 主要なテーマと重要なアイデア テーマ 重要なアイデア・数式 分類視点の多様性評価 多様性評価を n-クラス分類とみなし，サンプル T̃i を埋め込み Φ で表現し$H=\\Phi({T̃_i}_{i=1}^n),;K=\\text{Kernel}(H)$ 確率行列 P の構築 カーネル行列の各行を温度付き Softmax で正規化し $$P(c=c_j\\mid T̃_i)=\\frac{\\exp\\!\\bigl(K[i,j]/\\tau\\bigr)}{\\sum_j\\exp\\!\\bigl(K[i,j]/\\tau\\bigr)}$$ DCScore の定義 分類結果の自己一致度をトレースで集約$$\\text{DCScore}(D)=\\mathrm{tr}(P)=\\sum_{i=1}^n P[i,i]$$ 理論的保証 上式により DCScore は 1 ≤ DCScore ≤ n を満たし，4 つの Axioms を証明済み。 計算量削減 〈一般カーネル〉で $\\mathcal O(n^2,O_\\text{kernel}+n^2)$ となり，VendiScore の $\\mathcal O(n^2,O_\\text{kernel}+n^3)$ を大幅に短縮。 貢献 DCScore の提案 ― 多様性評価をサンプル分類タスクとして定式化し，相互関係を直接捉える新指標を提示。 公理的妥当性の証明 ― effective number・identical samples・symmetry・monotonicity の各公理を満たすことを理論的に示した。 計算効率の向上 ― 非線形カーネルで VendiScore より低い計算量を解析・実証し，大規模データセットにも適用可能とした。 多面的実証 ― 生成温度 τg・人間判断・GPT-4 判断との強相関，および下流タスク性能向上を多数の実験で確認し，指標としての有効性を裏付けた。 "
            }
    
        ,
            {
                "id": 4,
                "href": "https://www.if-blog.site/posts/hugo/hugo-katex/",
                "title": "Hugoでkatexを使った数式が表示されなくて困った",
                "section": "posts",
                "date" : "2025.05.20",
                "body": "Hugoで数式を表示するためにkatex@0.16.22を利用している。\nhttps://katex.org/docs/browser\n基本的に表示されるが表示されないこともあり困っていた。\n結論 shortcodeを作る\n以下のようなpassthroughするだけのショートコードを作る\nthemes/shortcodes/math.html{{- .Inner -}} 記事のmarkdown内で数式をショートコードで囲んで呼び出すだけ\n$X_{n_1}$ 原因 https://discourse.gohugo.io/t/one-of-several-latex-equations-is-not-rendered-by-katex/47790\nmarkdownのレンダラーが失敗してるらしい。確かに少し長めの数式が失敗してた。\n"
            }
    
        ,
            {
                "id": 5,
                "href": "https://www.if-blog.site/posts/paper/paper22/",
                "title": "Beyond Scale: The Diversity Coefficient as a Data Quality Metric for Variability in Natural Language Data",
                "section": "posts",
                "date" : "2025.05.19",
                "body": "AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nhttps://arxiv.org/abs/2306.13840\nhttps://github.com/brando90/beyond-scale-language-data-diversity\nサマリー 本論文は、LLMの前処理データにおける 多様性 (diversity) を定量化する指標として Task2Vec diversity coefficient を提案する。Task2Vec で得たバッチ埋め込み間の期待コサイン距離を多様性係数と定義し、理論的な下限・上限と比較しつつ公開データセットの多様性を測定。さらに GPT-2 と LLaMA-v2 を 44 モデル（51 M–7 B parameters）再学習し、多様性係数が高いデータで事前学習すると下流の言語モデリング性能が一貫して向上することを介入実験で示した。\n主要なテーマと重要なアイデア Task2Vec 埋め込みによるバッチ表現\n固定プローブ網 (GPT-2) の最終層を各バッチで微調整し、対角フィッシャー情報行列 $$\\hat F_B=\\mathbb{E}_{x,t,\\hat x_t}\\bigl[\\nabla_w\\log\\hat p_w(\\hat x_t\\mid x_{1:t-1})\\;\\nabla_w\\log\\hat p_w(\\hat x_t\\mid x_{1:t-1})^{\\!\\top}\\bigr]$$ の対角 $\\mathbf f_B=\\mathrm{Diag}(\\hat F_B)$ を Task2Vec embedding とする。\nDiversity coefficient の定義\nデータセット $D$ に対し $$\\hat{\\mathrm{div}}(D)=\\mathbb{E}_{B_1,B_2\\sim D}\\,d\\bigl(\\mathbf f_{B_1},\\mathbf f_{B_2}\\bigr)$$ を多様性係数、異なるデータセット間の類似度を\n$$\\hat{\\mathrm{cdiv}}(D_1,D_2)=\\mathbb{E}_{B_1\\sim D_1,B_2\\sim D_2}d\\bigl(\\mathbf f_{B_1},\\mathbf f_{B_2}\\bigr)$$ と定義（$d$: コサイン距離）。\n多様性の妥当性検証\n概念的下限（語彙2個の偏ったデータ）と 上限（語彙全トークンを一様抽出）を構築し、公開12 データセットの係数が下限の2.7–4.8倍、上限の半分超であることを確認。 GINC 合成データで潜在概念数や語彙数を操作し、係数が直感に沿って単調増加することを示す。 介入実験での性能向上\nUSPTO (0.158)、PubMed (0.168)、USPTO+PubMed (0.195) という多様性が段階的に高い三種コーパスで同一トークン数学習。多様性係数が高いほど検証クロスエントロピーが低下し、R² ≈ 0.8 の強い相関を得た。 データ結合による多様性増加\nC4+WikiText-103 や The Pile 部分集合を結合すると cross-diversity が単体より 0.03–0.1 向上。 貢献 データ中心の視点への転換：モデル・データ規模のスケーリング一辺倒から、質的指標としての diversity coefficient を導入。 解釈可能かつ形式的な多様性測度 を自然言語に適用し、Task2Vec を用いた計算レシピを提示。 直感と一致する性質の実証：潜在概念数・語彙サイズ・データセット結合で係数が理論通り変動することを示し、理解容易性を担保。 公開コーパスの定量比較：C4, The Pile など主要LLM用データが形式的にも高多様性であることを初めて報告。 因果的エビデンス：多様性係数が高いデータで事前学習したモデルは下流評価で一貫して性能向上することを 介入実験 で検証。 "
            }
    
        ,
            {
                "id": 6,
                "href": "https://www.if-blog.site/posts/paper/paper21/",
                "title": "On the Diversity of Synthetic Data and its Impact on Training Large Language Models",
                "section": "posts",
                "date" : "2025.05.19",
                "body": "AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nhttps://arxiv.org/html/2410.15226v2\nエグゼクティブサマリー 本論文は synthetic data の多様性 が Large Language Models (LLMs) の性能に与える影響を体系的に解析した初の大規模研究である。著者らは、多様性を定量化するための新しい指標 「LLM Cluster-agent」 を提案し、Wikipedia 由来の 62 万トピックから生成した複数の合成コーパスを用いて、350 M／1.4 B パラメータの Llama 系モデルを事前学習・教師あり微調整。実験の結果、提案指標で得られる LLM cluster score が高いほど、事前学習と微調整の双方でベンチマーク精度が一貫して向上 することを示した。\n主要なテーマと重要なアイデア Synthetic data 多様性の重要性\n量や品質と同様に diversity が LLM 成績を左右するにもかかわらず、適切な測定法が欠如していたという課題認識。 LLM Cluster-agent 指標\n(i) メタデータ／メトリクス生成、(ii) それらを用いた逐次クラスタリング、(iii) クラスター自己検証という 3 段から成るパイプラインで、平均的な「クラスター密度」 D を LLM cluster score として算出。 ヒューリスティック指標（n-gram diversity 等）や K-means よりも分布差を鋭敏に捉え、実験成績との相関が最も高い。 多角的コントロール実験\nUnderlying distribution: トピック数 T と 1 トピックあたり生成数 G を操作し、多様性と精度のトレードオフを定量化。 Prompt design: 文体・受取手 (persona)・複数トピックの導入で diversity を拡張し、Cosmopedia を上回る性能を達成。 Generation model: GPT-4o \u0026gt; GPT-3.5 \u0026gt; Llama-3.1 \u0026gt; Mistral の順に多様性と精度が向上し、モデル混合も有効。 Real/Synthetic 比率: 合成比率を高めると 20 B までは精度向上するが、過度 (\u0026gt;34 B) では冗長性により劣化。 モデルサイズ効果: 大きいモデルほど高い diversity を必要とし、微調整段階での利得が顕著。 アブレーションとロバスト性\nパラメータ K/N/J/M の変化、パイプライン構成要素、使用 LLM を変えても指標は頑健である。 貢献 LLM Cluster-agent: 合成テキスト多様性を高精度で評価する初の LLM ベース指標を提案し、公開ツールキットとして提供。\n大規模統制実験: 34 B 実データ＋最大 50 B 合成データで 350 M/1.4 B モデルを学習し、多様性と性能の線形関係を実証。\n設計指針の提示:\nトピックの広域カバレッジと適切な G が不可欠。 文体・persona を組み合わせたプロンプトが多様性を大幅に向上。 より強力な生成 LLM と異種混合は多様性をブースト。 実データと合成データはバランス（1:1 付近）が最良。 スケーラビリティの検証: 指標と知見は 1.4 B 以上のモデルにも拡張可能であり、今後の効率的 synthetic data 生成プロセスに資する。\n"
            }
    
        ,
            {
                "id": 7,
                "href": "https://www.if-blog.site/posts/paper/paper20/",
                "title": "Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases (AI論文要約)",
                "section": "posts",
                "date" : "2025.05.18",
                "body": "AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nhttps://ceur-ws.org/Vol-3290/short_paper2780.pdf\n背景と目的 既存の語彙多様度指標（TTR、Shannon entropy 等）は 表層文字列のユニークさに依存するため，OCR 誤りや歴史的綴り揺れを含むコーパスでは過大評価を招く。 生態学で用いられる 属性多様度 (attribute diversity) の枠組みを導入し，「語彙項目同士の機能的 / 意味的距離」を考慮した Functional Diversity (FD) を Humanities 研究に適用することが本研究の目的。 提案手法 語彙間距離の計算\n歴史英語 LL﻿M MacBERTh で各トークンの文脈依存埋め込みを取得し，コサイン距離で語彙間距離行列 Δ を作成。 機能グループの定義\n距離しきい値 θ を設定し，Δ ≤ θ の語を同一 functional group とみなす。 多様度の定式化\n各語タイプ i の出現頻度 $f_i$ と機能グループ頻度 $g_i(θ)$ を用い，\n$$ FD_{q}(θ) ;=;\\Bigl(\\sum_{i=1}^{A} \\bigl[,\\tfrac{f_i}{g_i(θ)},\\bigr]^{q}\\Bigr)^{\\tfrac{1}{1-q}} $$\nと定義（$q=0$ は機能的リッチネス，$q=1$ は Shannon，$q=2$ は Gini–Simpson の逆数に対応）。\nこれは Chao \u0026amp; Chiu の Functional Hill Numbers 拡張に一致する。\n実験設定 コーパス: ARCHER 3.2（1600–1999 年，約 3.3 M 語，12 ジャンル）。 ノイズシミュレーション: 確率 p = 0, 0.1, 0.2, 0.35, 0.5, 0.75 で各語内の文字をランダム置換し OCR / 綴り誤りを再現。 各設定で 5 回再生成し，FD と従来のタイプ数 (θ = θ_min) を比較。 主な結果 観点 従来の「タイプ数」 Functional Diversity ノイズ耐性 p 上昇に伴いほぼ線形で過大評価 θ = θ_mean では変化が小さく安定 ジャンル差 広告 vs. 小説で顕著差出ず 広告は「列挙型」で functional group への凝集度が高く，小説は幅広い機能範囲を示す 語数補正 TTR では短文優遇 Hill Numbers により頻度分布まで統一尺度で比較可能 考察・意義 実務的利点: OCR エラーや歴史綴りを多く含むコーパスでも，前処理なしで頑健に語彙多様度を評価できる。 理論的利点: 「語彙の幅 (breadth) と深さ (depth)」を分離評価でき，語彙豊富さに新しい次元を導入。 ツールキットへの貢献: 生態学的属性多様度を計算人文学へ転用し，計量語彙研究の標準指標に機能的側面を追加。 結論 Functional Diversity は\n綴りノイズに強く， 意味的内実を捉える 多様度指標であり， 歴史コーパスを含む計算人文学研究に有益である。 著者らは FD を「従来のカテゴリカル指標の補完」と位置付け，公開済みモデル (MacBERTh) とスクリプトで再現可能な手法として提供している。\n"
            }
    
        ,
            {
                "id": 8,
                "href": "https://www.if-blog.site/posts/paper/paper19/",
                "title": "Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases (AI論文要約)",
                "section": "posts",
                "date" : "2025.05.17",
                "body": "AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nhttps://arxiv.org/abs/2502.19249\nどんなもの この研究は、形式言語での事前事前学習が自然言語学習に役立つことを示し、どのような形式言語が有効な帰納バイアス（inductive bias）を与えるのかを調査するものである。特に、Chomsky階層 と 回路複雑性階層（circuit complexity hierarchy） の交差点に着目し、Transformerが学習可能でかつ自然言語の階層的依存構造を持つ形式言語により、自然言語の学習効率と文法的汎化性能が向上することを示している。\n先行研究と比べてどこがすごいの？ 従来は、Chomsky階層における文脈自由（context-free）や文脈依存（context-sensitive）な形式言語が自然言語への転移に有効であるとされてきたが、Transformerの計算限界を考慮せず、すべての文脈依存言語が転移に適しているとは限らなかった。本研究では、新たにC-RASPおよびFO(M) という回路理論ベースの枠組みを導入し、Transformerが実際に学習可能な形式言語のクラスを特定した点が革新的である。\n技術や手法のきもはどこにある？ 以下の2点を満たす形式言語でのpre-pretrainingが最も効果的であるという仮説を立てて検証している：\n自然言語の階層的依存構造を含む（context-free または context-sensitive）。 **Transformerで長さ一般化が可能（length-generalizing learnability）**であり、C-RASP で定義可能。 この仮説に基づき、次の形式言語を使用して比較実験を行った：\n1-Dyck（context-free, C-RASPに含まれる） k-Dyck（context-free, FO(M)には含まれるがC-RASPには含まれない） k-Shuffle Dyck（context-sensitive, C-RASPに含まれる） ww（copy language, context-sensitive, C-RASPに含まれない） また、Transformerの学習構造のうち、形式言語学習中に形成されたサブネットワーク Mが、自然言語学習中も再利用されるという「Subnetworks Hypothesis」も提唱・検証されている。\nどうやって有効だと検証した？ 以下の方法で実証された：\n事前事前学習（pre-pretraining）を行った後、英語コーパス（C4）で事前学習（pretraining）を行い、検証損失（validation loss）、文法判別精度（BLiMP）、繰り返し構造の記憶（verbatim retrieval）で評価。\n結果、k-Shuffle Dyckでのpre-pretrainingが最も有効であり、同等の性能を達成するのに必要なトークン数が33%削減された。\n数式として、トークン効率の向上率は：\n$$ \\text{Token Efficiency Gain} = 1 - \\frac{T_{\\text{pre-pretraining}} + T_{\\text{pretraining}}}{T_{\\text{baseline}}} $$\nで定義され、実際に $1 - \\frac{1.10B}{1.63B} = 0.33$ が達成された。\nまた、注意ヘッドのプルーニング実験により、形式言語学習で得られたサブネットワークMが自然言語学習でも重要であることが示された（$p \\ll 0.001$）。\n議論はあるか 万能ではない：すべてのBLiMPサブタスクで性能が向上するわけではなく、一部では性能低下も見られた。 非構文的知識（morphology） にも影響がある：形式言語によるpre-pretrainingは主に構文的知識を強化するが、形態論的性能にも二次的に良い影響があった。 他のモデルアーキテクチャへの拡張未検討：本研究はTransformerに限定されており、RNNやState Space Modelへの適用は今後の課題である。 形式言語のハイパーパラメータ調整が高コストであり、スケーラブルな探索方法の導入が求められる。 "
            }
    
        ,
            {
                "id": 9,
                "href": "https://www.if-blog.site/posts/paper/paper17-2/",
                "title": "TinyHelen’s First Curriculum: Training and Evaluating Tiny Language Models in a Simpler Language Environment (AI論文要約)",
                "section": "posts",
                "date" : "2025.05.17",
                "body": "AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nhttps://arxiv.org/abs/2501.00522\nどんなもの この研究は、大規模言語モデル（LLM）の訓練に必要なリソースを削減するために、「簡素な言語環境」を構築し、その中で小型の言語モデル（Tiny LMs）を効果的に訓練・評価するためのカリキュラムを提案します。特に、「ノイズなし・低複雑性（no noise, low complexity）」という原則に基づき、以下のデータセットを作成しました：\nLEANER-Pretrain（71Mトークン）：プレトレーニング用 LEANER-Instruct（7Mトークン）：命令調整用 LEANER-GLUE：簡素化したGLUEベンチマーク LEANER-Eval：命令追従能力の評価用 このデータセットにより、学習効率が高まり、学習データやモデルサイズを削減しつつも有用な言語スキルを獲得できることを示しています。\n先行研究と比べてどこがすごいの？ 先行研究（TinyStories, BabyLM, miniGPT など）では、子供向けストーリーや字幕といった特定の形式に限定された簡易データを使用していましたが、本研究は以下の点で優れています：\n多様なジャンル（ウェブ、書籍、会話、コード、数学など）を含む構成でありながら、語彙と構文を簡素化 情報エントロピーを定量的に削減しており、学習効率の改善を理論的・実証的に示している 小型モデルに対するベンチマークも併せて提供し、命令追従の習得やカリキュラム学習の効果の比較が可能 技術や手法のきもはどこにある？ 技術的な肝は以下の点にあります：\nLEANERデータセット構築アルゴリズム 特に以下のアルゴリズムがコアです：\nAlgorithm 1: LEANER-Training Dataset Collection Algorithm 2: LEANER-GLUE Benchmark Creation Algorithm 3: LEANER-Eval Benchmark Creation 情報エントロピーを用いた簡素化の定量的指標 N-gramエントロピー比較：\n$$ \\text{Entropy}_{1\\text{-gram}}(\\text{Original}) = 16.41 $$\n$$ \\text{Entropy}_{1\\text{-gram}}(\\text{LEANER}) = 15.86 $$\nカリキュラム学習の導入 以下の難易度指標に基づいて学習サンプルを段階的に導入：\n文長（SentLen） モデルの自己損失（Self-loss） 事前学習済モデルによる損失（LM-loss） データ導入の進行：\n$$ \\sum_{j=0}^{i} p_j = 1\\quad \\text{(全データ導入)} $$\nどうやって有効だと検証した？ 3つの実験によって検証しています：\nEXP1: モデルアーキテクチャの比較\n同一データ・同一パラメータ（14M）で、BERT、LLAMA、XLNET、MAMBAを比較 LEANER-Pretrainを使ったモデルが、Originalよりも高得点 例えば： $$ \\text{Avg}_{\\text{LEANER-XLNET}} = 0.963 $$\n$$ \\text{Avg}_{\\text{ORI-XLNET}} = 0.900 $$\nEXP2: 命令追従能力の評価\nLEANER vs Original の組み合わせ4通りの事前学習＋命令調整\n評価指標は「文法」「一貫性」「具体性」（各5点満点）で合計15点満点\nLEANERデータでの事前学習が最も高いスコア：\n$$ \\text{LEANER-Ori Sum Score} = 4.1 $$\n$$ \\text{Ori-Ori} = 2.7 $$\nEXP3: カリキュラム学習の効果検証\nITER. LM-LOSS 戦略が、ランダム戦略よりも20%学習ステップを削減 データ量も20%削減して同等以上の性能を実現 議論はあるか いくつかの重要な議論があります：\nTiny LMsはInstruction-following能力に限界がある\n71Mトークンでは、平均スコア1点台とまだ不十分 人間はマルチモーダルで学習するため、言語だけでは限界がある LLMによる書き換えに内在するバイアス\n「LEANER」データはGPT-4により再記述されたため、元の文脈からのバイアスが含まれる可能性がある 人間の学習過程とのアナロジー\n「TinyHelen」という名前はヘレン・ケラーにちなんでおり、人間が簡単な言語から学び始めるプロセスを模倣している 必要であれば各数式やアルゴリズムの詳細も展開できます。続きを希望されますか？\n"
            }
    
        ,
            {
                "id": 10,
                "href": "https://www.if-blog.site/posts/paper/paper18/",
                "title": "Towards Data-Efficient Language Models: A Child-Inspired Approach to Language Learning (AI論文要約)",
                "section": "posts",
                "date" : "2025.05.17",
                "body": "AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nhttps://arxiv.org/abs/2503.04611\nどんなもの 本論文は、従来の大規模言語モデル（LLM）と比べて大幅に少ないデータで言語モデル（LM）を訓練する手法を提案しています。その手法は、人間の子供が言語を習得する方法から 着想を得ています。具体的には、子供向けの書き起こしを主に用いた1000万語のキュレーションされたデータセットでモデルを訓練します。\n先行研究と比べてどこがすごいの？ 先行研究のLLMは数十億語規模の膨大なデータで訓練されるのに対し、本研究では、子供向けの書き起こしを主に用いた1000万語（850万語にフィルタリング後、テレビ番組の対話データ150万語を追加）のデータセットを用いて、高性能な言語モデルを訓練することに成功しています。これは、人間の子供の言語習得におけるデータ効率の高さに着目し、それを 模倣したアプローチによる成果です。また、従来手法では性能向上に寄与する高品質な単一言語データセット（MADLAD-400）の追加が性能を低下させるのに対し、本手法ではテレビ番組の対話データの追加が性能向上に寄与しています。\n技術や手法のきもはどこにある？ 本研究の主要な技術的貢献は以下の通りです。\nデータセットのキュレーション: 子供向けの書き起こしを主に用いた1000万語のデータセットを作成し、重複削除や品質基準に基づくフィルタリングを行い、850万語に絞り込みました。さらに、子供のメディア接触を考慮し、テレビ番組の対話データ150万語を追加しました。 語彙のスケーリング: 子供の初期の言語習得段階における語彙の制限を模倣するために、モデルの語彙サイズを32,000トークンに削減しました。 モデルアーキテクチャ: 1億2500万パラメータのSmolLMモデル（decoder-only Transformer）を採用しました。 カリキュラム学習: 文の長さ、平均単語長、固有単語比率、句読点の使用量に基づいて文の複雑さを評価する独自のスコアリング関数 $score(d) = \\sum_{f∈F} w_f f(d)$ (1) を開発し、データセットを難易度順にソートすることで、カリキュラム学習を実装しました。ここで、$d$はデータポイント、$F$はスコアリング関数集合、$w_f$は各スコアリ ング関数の重み(0から1の間で、合計が1)です。 どうやって有効だと検証した？ 様々な実験を通して、提案手法の有効性を検証しました。\nテレビ番組の対話データの追加効果：テレビ番組の対話データを追加することで、BLIMPとBLIMP Supplementベンチマークにおいて性能が向上しました。(Table 2) 語彙サイズの最適化：語彙サイズ32,000トークンが最適であることを確認しました。(Table 3) カリキュラム学習の効果：カリキュラム学習によって、モデルの性能が向上しました。 MADLAD-400データセットの効果：高品質データセットとされるMADLAD-400データセットを用いた場合、性能が低下しました。これは、低リソース環境では、言語構造が豊富で多様なデータが重要であることを示唆しています。 ベースラインとの比較：提案手法は、BabyLlamaやLTG-BERTなどのベースラインと比較して、ほとんどのベンチマークで同等以上の性能を示しました。(Table 4) 議論はあるか カリキュラム学習におけるスコアリング関数の重み付けは実験的に決定されており、最適な重み付けの決定には計算コストがかかる可能性があります。 テレビ番組のデータの適切な量は実験的に決定されており、他のデータセットでは異なる可能性があります。 本手法による訓練が、下流タスクの性能に悪影響を与える可能性があります。 これらの制限事項を克服するために、スコアリング関数の重み付けの信頼性と堅牢性の向上、テレビ番組データの適切な量の決定、下流タスクへの影響評価などが今後の課題として挙げられています。 また、Influence functions, Representer point, TracIn, HYDRA, RL-based methodsなどのデータ評価手法を用いることで、より効率的なデータキュレーシ ョンが可能になる可能性も示唆されています。\n"
            }
    
        ,
            {
                "id": 11,
                "href": "https://www.if-blog.site/posts/paper/paper16/",
                "title": "Language Model Can Listen While Speaking (AI論文要約)",
                "section": "posts",
                "date" : "2025.01.18",
                "body": "AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nhttps://arxiv.org/abs/2408.02622\nどんなもの 本論文は、リアルタイムの双方向音声対話を実現する、Listening-while-Speaking Language Model (LSLM) を提案している。LSLMは、音声生成チャネルとリアルタイム音声入力チャネルの両方を備えた、エンドツーエンドのモデルである。音声生成にはトークンベースのdecoder-only TTS、音声入力にはストリーミングSelf-Supervised Learning (SSL) エンコーダを使用し、両チャネルを融合することで、自動回帰的な音声生成とリアルタイムのターンテイク検出を行う。\n先行研究と比べてどこがすごいの？ 先行研究のSpeech Language Model (SLM) はターンベースの会話に限定されており、リアルタイムの音声状況や割り込みに対応できないという限界があった。一方、LSLMは、Full Duplex Modeling (FDM) を実現することで、音声生成中にリアルタイムで音声入力を処理し、割り込みに対応できる。既存のモデルは、テキスト中心のLLMをベースに外部のASRとTTSモジュールを必要としていたため、レイテンシが問題となり、パラリンギスティックな能力も不足していたが、LSLMはSLM自体にFDM能力を持たせることで、これらの問題を 解決している。また、ノイズへの頑健性と未知の話者への感度も実験で示されている。\n技術や手法のきもはどこにある？ LSLMの核となる技術は、音声生成チャネルと音声入力チャネルの同時処理と融合にある。音声生成には、リアルタイム性を重視したトークンベースのdecoder-only TTSを使用。音 声入力には、ストリーミング処理可能なvq-wav2vecを用いたSSLエンコーダを使用する。両チャネルの融合戦略として、Early Fusion、Middle Fusion、Late Fusionの3つの手法を 検討し、Middle Fusionが音声生成とリアルタイム対話のバランスにおいて最適であることを示している。さらに、割り込みを検知するために、IRQ (Interruption Request) トー クンを導入している。\nどうやって有効だと検証した？ LSLMの有効性を検証するために、Command-based FDM と Voice-based FDM の2つの実験設定を用いた評価を行った。Command-based FDMでは、特定のコマンドによる割り込み、Voice-based FDMでは、未知の話者による様々な単語による割り込みをシミュレートした。評価指標としては、音声生成能力の評価にWord Error Rate (WER)、対話能力の評価 にPrecision、Recall、F1スコアを用いた。結果、LSLMはノイズに頑健であり、未知の話者からの割り込みにも感度良く反応し、既存システムへの影響を最小限に抑えながら双方向通信を実現できることを示した。\n議論はあるか 論文では、Middle Fusionが最適な融合戦略であると結論付けているが、データセットやモデルアーキテクチャによっては、他の融合戦略がより良い結果を示す可能性もある。また、Voice-based FDMにおけるWERの上昇は、リアルタイムのターンテイク問題の複雑さを示唆しており、さらなる改善の余地がある。さらに、より現実的なシナリオへの適用や、話 者追跡機能の追加、音声・視覚情報の同時利用など、今後の研究課題が提示されている。\n"
            }
    
        ,
            {
                "id": 12,
                "href": "https://www.if-blog.site/posts/paper/paper13/",
                "title": "MiniMax-01: Scaling Foundation Models with Lightning Attention (AI論文要約)",
                "section": "posts",
                "date" : "2025.01.18",
                "body": "AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nhttps://arxiv.org/abs/2501.08313\nどんなもの MiniMax-01シリーズ（MiniMax-Text-01とMiniMax-VL-01を含む）は、最先端モデルと同等の性能を持ちながら、より長いコンテキストを処理できる優れた能力を持つ基盤モデルで す。 長コンテキスト処理を可能にするために、効率的なスケーリングが可能なlightning attentionを採用しています。計算能力を最大化するため、Mixture of Experts (MoE)と統合されており、32個の専門家と合計4560億個のパラメータを持ち、各トークンに対して459億個のパラメータが活性化されます。\n先行研究と比べてどこがすごいの？ MiniMax-01は、GPT-4oやClaude-3.5-Sonnetなどの最先端モデルと同等の性能を達成しながら、20～32倍長いコンテキストウィンドウ（最大400万トークン）を提供します。先行研 究で提案されたsparse attention、linear attention、long convolutions、state space models、linear RNNsなどは、理論的には有望でしたが、商業規模のモデルへの採用は限 られていました。MiniMax-01は、これらの先行研究の限界を克服し、大規模なlinear attentionの実装に初めて成功した点で優れています。\n技術や手法のきもはどこにある？ MiniMax-01の核となる技術はlightning attentionです。これは、因果言語モデリングにおける遅いcumsum演算を回避するタイル化技術を用いた、linear attentionのI/O対応 の最適化実装です。 計算の複雑さを線形に削減し、長コンテキストの処理を効率化します。さらに、計算能力とパラメータ数を最大化するためにMixture of Experts (MoE) を採用し、効率的な並列化戦略と計算・通信オーバーラップ技術を開発することで、数百億パラメータのモデルを効率的に訓練・推論できるようにしています。 また、softmax attentionとlightning attentionを組み合わせたハイブリッドアーキテクチャを採用することで、retrieval性能も向上させています。\nどうやって有効だと検証した？ 標準的な学術ベンチマーク（MMLU、MMLU-Pro、C-SimpleQA、IFEval、GPQA、MATH、Humanevalなど）と、実世界の使用状況から導き出された社内ベンチマークの両方で、MiniMax-01の有効性を検証しました。結果、最先端モデルと同等の性能を達成し、特に20万トークンを超えるコンテキストでは、有意に優れた性能を示しました。 さらに、様々な規模のモ デル（7000万～70億パラメータ）を用いたスケーリング実験を行い、lightning attentionのスケーラビリティと、softmax attentionとの比較を行いました。 また、MoEアーキテクチャの有効性についても、密なモデルとの比較実験で検証しています。 長コンテキストの処理効率については、prefilling latencyの比較や、様々なattention機構の訓練速度の比較実験も行っています。\n議論はあるか linear attentionの限界: 純粋なlinear attentionモデルは計算効率が良いものの、retrieval能力が不足しており、in-context learningには適さないことが示されました。そのため、softmax attentionと組み合わせたハイブリッドアーキテクチャを採用しています。このハイブリッドアプローチの妥当性については、更なる議論が必要となる可能性があります。 スケーリング則の精度: 大規模モデルへの外挿において、スケーリング則の予測精度が低下する問題がありました。論文では、これを解決するための新たな式を提案していますが、その一般性や信頼性については、更なる検証が必要です。 長コンテキスト評価の限界: 現在の長コンテキスト評価データセットは、人工的なシナリオに偏っている可能性があり、文書分析などの実用的なタスクにおける長文の推論 能力の評価は依然として限られています。より現実的な設定での評価が必要となります。 データセットのバイアス: 使用されたデータセットにバイアスが含まれている可能性があり、それがモデルの性能や公平性に影響を与えている可能性があります。 この点 についての詳細な分析が必要です。 "
            }
    
        ,
            {
                "id": 13,
                "href": "https://www.if-blog.site/posts/paper/paper10/",
                "title": "SentenceVAE: Enable Next-sentence Prediction for Large Language Models with Faster Speed, Higher Accuracy and Longer Context (AI論文要約)",
                "section": "posts",
                "date" : "2025.01.18",
                "body": "AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nhttps://arxiv.org/abs/2408.00655\nどんなもの Large Language Models (LLMs) の推論効率を向上させるための新しい推論方法である next-sentence prediction を提案する。これは、Sentence Variational Autoencoder (SentenceVAE) を用いて、文を1つのトークンに圧縮し、LLMに入力することで実現される。SentenceVAEは、文をエンコードする Sentence Encoder と、圧縮されたトークンを元の文 に復元する Sentence Decoder から構成される。SentenceVAEをLLMの入力層と出力層に統合することで、文単位で推論を行う Sentence-level LLMs (SLLMs) を開発する。\n先行研究と比べてどこがすごいの？ 先行研究では、複数のトークンを同時に予測する手法が提案されているが、予測するトークンの数は固定されており、入力コンテキストの分割が柔軟性に欠けていた。本研究では、SentenceVAEを用いることで、文のセマンティックな内容に基づいて最適なトークンの数を適応的に選択し、推論速度の向上と精度の維持・向上を両立する。具体的には、従来のト ークン単位の予測 (next-token prediction) に比べ、next-sentence prediction によって推論速度を204～365%向上させ、perplexity (PPL) を46～75%削減、メモリオー バーヘッドを86～91%削減できることを実証した。\n技術や手法のきもはどこにある？ SentenceVAEのSentence EncoderとSentence Decoderが技術・手法のキモである。Sentence Encoderは、文中の複数のword-level tokensを1つのsentence-level tokenに圧 縮する。Sentence Decoderは、このsentence-level tokenを元のword-level tokensのシーケンスに復元する。文を単位として処理することで、元のセマンティックな内容の整合性 を維持し、精度を向上させる。また、sentence segmentation mechanismとfeature fusion mechanismにより、可変長のトークンシーケンスを単一のsentence-level token にエンコードする。\nどうやって有効だと検証した？ Wanjuanデータセットを用いた広範な実験により検証した。SentenceVAE単体では、自己教師あり学習で訓練し、クロスエントロピー損失とPPLを用いて検証セットで評価した。SLLMsについては、オープンソースのOPTシリーズのLLMにSentenceVAEを統合し、従来のOPTモデルと比較することで、推論速度、PPL、メモリ使用量を評価した。その結果、SLLMsは従来のLLMと比べて推論速度が2～3倍高速化し、PPLが改善し、メモリ使用量が削減されることを確認した。さらに、SLLMsがScaling Lawに従うことも確認した。\n議論はあるか SentenceVAEの性能: SentenceVAEの性能は、hidden sizeとhidden layersの数に依存する。より大きなモデルは、より良い性能を示す傾向があるが、計算コストも増加する。 入力データの前処理: SentenceVAEは、句読点で文を分割する前処理を必要とする。この前処理が不適切な場合、モデルの性能に影響を与える可能性がある。実験では、一部 の異常な入力に対してもある程度の頑健性を示しているものの、全てのケースで完璧な結果が得られるわけではない。 Scaling Lawの適用範囲: 本研究では、125Mから1.3BパラメータのモデルでScaling Lawを確認したが、より大きなモデルへの適用範囲については、今後の研究が必要である。 言語依存性: 現状、英語コーパスのみで訓練されているため、多言語への対応が今後の課題となる。 "
            }
    
        ,
            {
                "id": 14,
                "href": "https://www.if-blog.site/posts/paper/paper15/",
                "title": "Small Language Models (SLMs) Can Still Pack a Punch: A survey (AI論文要約)",
                "section": "posts",
                "date" : "2025.01.18",
                "body": "AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nhttps://arxiv.org/abs/2501.05465\nどんなもの 本論文 \u0026ldquo;Small Language Models (SLMs) Can Still Pack a Punch: A survey\u0026rdquo; は、パラメータ数が10億～80億のSmall Language Models (SLMs) に関するサーベイ論文です。大規 模言語モデル (LLMs) の台頭に対し、巨大な規模だけが唯一の進歩の道ではないという疑問を提起し、SLMsがLLMsと同等、もしくは凌駕する性能を示すことを示す約160本の論文を調査しています。タスク非依存の汎用SLMs、タスク依存のSLMs、そしてSLMsを作成するための技術を網羅的に解説し、性能、効率性、スケーラビリティ、コストのバランスを取り ながらモデル構築を行うための指針を示しています。さらに、LLMsに対するSLMsの有効サイズを定義し、特徴づけています。\n先行研究と比べてどこがすごいの？ 先行研究であるLLMsに関するサーベイ論文 ([153, 47, 96, 158]) は、主に100億パラメータを超える大規模モデルを対象としており、SLMsについては軽く触れている程度でした。本論文は、近年発表された、大規模モデルと同等、もしくはそれ以上の性能を示すSLMsに焦点を当てた、世界で初めてのサーベイ論文です。 SLMsの設計、アーキテクチャ、そし て大規模モデルに匹敵する、あるいは場合によっては凌駕する性能を達成可能にする革新的な技術を詳細に分析しています。\n技術や手法のきもはどこにある？ SLMsの有効性を高めるための主要な技術や手法として、以下のものが挙げられています。\n様々な種類のSLMsの分類: タスク非依存型SLMs、タスク依存型SLMsに分類し、それぞれの特徴を分析。 効果的な訓練手法: 知識蒸留 (KD)、命令微調整 (instruction tuning)、思考連鎖 (Chain-of-Thought, CoT)、説明チューニング (explanation tuning)、漸進的学習 (progressive learning) など、SLMsを効率的に訓練するための様々な手法を紹介。特に、LLMsから得られた説明トレースを活用する手法が有効であると示唆。 効率的なアーキテクチャ: Transformerアーキテクチャをベースとしたモデルに加え、状態空間モデル (SSMs) を用いたハイブリッドアーキテクチャ (Hymba, Zamba, Jamba, Mamba) などの効率的なアーキテクチャの検討。 データ戦略: 高品質なデータセットの重要性を強調。LLMによって生成された合成データセット (TinyStories, TinyGSM) や、Common Crawlなどのインターネットデータセット (Pile) の活用例を紹介。 ポストトレーニング最適化: 量子化 (SmoothQuant, GPTQ, AWQ) やモデルプルーニング (BIP, HIL) などの手法によるモデルの軽量化と性能維持・向上。 ドラフトモデルの活用: 推論速度を向上させるためのドラフトモデルの概念と、独立型、依存型のドラフトモデルのアーキテクチャを紹介。 どうやって有効だと検証した？ 論文では、様々なSLMsの性能を、MMLU、HellaSwag、Winogrande、PIQA、ARC、BoolQ、GSM8K、HumanEval、MBPP、MATHなどの標準的なベンチマークを用いて評価しています。 多くの場合、SLMsは、パラメータ数がはるかに大きいLLMsと比較して同等以上の性能を示すことが示されています。また、論文では、モデルの性能に基づいて「有効サイズ」を推定し 、パラメータ数よりもはるかに大きなLLMsと同等の性能を持つSLMsが存在することを示しています。\n議論はあるか 論文では、SLMsがLLMsを凌駕する性能を示す理由について、データの質が量よりも重要であるという点以外、明確な結論は示されていません。既存のスケーリング則 (KaplanとChinchillaの法則) がSLMsの驚くべき性能を説明できない可能性を指摘し、データの質を考慮した修正版スケーリング則を提案しています。しかし、データの質を客観的に評価する方法については今後の課題として残されています。 また、タスク特化型SLMsが、特定のタスクにおいては、大規模汎用モデルを上回る性能を示すことも示されていますが、その理 由についても更なる研究が必要とされています。 さらに、SLMsの評価指標についても、言語タスクだけでなく、マルチモーダル理解、安全性、LLMsが苦手とする高度な専門タス クなど、より包括的な評価指標が必要であると議論されています。\n"
            }
    
        ,
            {
                "id": 15,
                "href": "https://www.if-blog.site/posts/paper/paper14/",
                "title": "Tensor Product Attention Is All You Need (AI論文要約)",
                "section": "posts",
                "date" : "2025.01.18",
                "body": "AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nhttps://arxiv.org/abs/2501.06425\nどんなもの Tensor Product Attention (TPA) は、クエリ(Q)、キー(K)、バリュー(V)をコンパクトに表現するためにテンソル分解を用いる新しいアテンション機構です。推論時のKVキャッシ ュサイズを大幅に削減し、メモリ効率を向上させます。文脈依存の低ランク成分への因数分解（コンテキストファクター化）とRoPEとのシームレスな統合により、モデルの品質と メモリ効率の両方を向上させます。TPAに基づいて、シーケンスモデリングのための新しいモデルアーキテクチャであるTensor ProducT ATTenTion Transformer (T6)を提案してい ます。\n先行研究と比べてどこがすごいの？ 先行研究であるMulti-Head Attention (MHA)、Multi-Query Attention (MQA)、Grouped-Query Attention (GQA)、Multi-head Latent Attention (MLA)と比較して、TPAは推論時のKVキャッシュサイズを10倍以上削減します。これは、クエリ、キー、バリューのアクティベーションを動的に因数分解することで実現されています。また、TPAはRoPEとシームレス に統合できるため、LLaMAやGemmaなどの最新のLLMアーキテクチャへの容易な適用を可能にします。さらに、MHA、MQA、GQAはTPAの非文脈依存バリアントとして自然に現れることを明らかにすることで、既存のアテンション機構を統一しています。\n技術や手法のきもはどこにある？ TPAの核心は、アテンション計算中にクエリ(Q)、キー(K)、バリュー(V)をより高次のテンソルを用いて因数分解する点にあります。具体的には、文脈依存の低ランク表現を構築す るために、アクティベーションを動的に因数分解します（静的な重み（例：LoRA）の因数分解とは異なります）。これにより、表現能力を向上させながら、KVキャッシュのメモリ 使用量を大幅に削減します。 RoPEとの統合は、キーとバリューのテンソル次元因子を事前に回転させることで、推論時の回転計算を回避し、効率を高めています。\nどうやって有効だと検証した？ FineWeb-Edu 100Bデータセットを用いた言語モデリングタスクにおいて、様々な規模（124M、353M、773Mパラメータ）のモデルでT6をMHA、MQA、GQA、MLAと比較評価しました。そ の結果、T6は検証損失（perplexity）と様々な評価ベンチマークにおいて、MHA、MQA、GQA、MLAを上回る性能を示しました。特に、TPAのメモリ効率により、固定リソース制約下で大幅に長いシーケンスの処理が可能になり、最新の言語モデルにおける重要なスケーラビリティ課題に対処できることを実証しました。\n議論はあるか 論文では、TPAの様々なバリアント（KVのみを因数分解するTPA、非文脈依存のヘッド次元因子を用いるTPAなど）についても検討されています。これらのバリアントは、メモリコスト、計算オーバーヘッド、表現力のバランスを取るための柔軟性を示しています。しかし、最適なランク(RQ, RK, RV)の選択や、非線形ヘッド因子の導入による効果については、 更なる検討が必要となる可能性があります。また、異なるデータセットやタスクにおけるTPAの汎化性能についても、今後の研究課題として挙げられます。\n"
            }
    
        ,
            {
                "id": 16,
                "href": "https://www.if-blog.site/posts/paper/paper11/",
                "title": "Titans: Learning to Memorize at Test Time (AI論文要約)",
                "section": "posts",
                "date" : "2025.01.18",
                "body": "AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nhttps://arxiv.org/abs/2501.00663\nどんなもの Titansは、テスト時に情報を記憶することを学習する新しいニューラル長期記憶モジュールを備えた、深層学習アーキテクチャのファミリーです。 このモジュールは、過去のコンテキストを記憶し、Attentionメカニズムが現在のコンテキストに注意を払いつつ、過去の情報を活用できるようにします。 Titansは、短期記憶として機能するAttentionメカニズムと、長期記憶として機能するニューラルメモリモジュールを組み合わせたアーキテクチャです。3つのバリエーション（Memory as a Context (MAC)、Memory as a Gate (MAG)、Memory as a Layer (MAL)）が提案されています。\n先行研究と比べてどこがすごいの？ 長期記憶の効率的な学習と利用: 既存のアーキテクチャ（Hopfield Networks、LSTMs、Transformers）は、汎化、長さの推定、推論において課題を抱えていましたが、Titansはテスト時に効率的かつ効果的に記憶することを学習するニューラル長期記憶モジュールを導入することで、これらの課題を克服します。 Transformerや最近の線形再帰モデルを凌駕する性能: 言語モデリング、常識推論、ゲノミクス、時系列タスクにおいて、Transformersや最近の最新の線形再帰モデルよりも 優れた性能を示します。 大規模なコンテキストウィンドウへの対応: Transformersの二次的な計算コストの問題を克服し、2Mを超えるコンテキストウィンドウサイズに効果的にスケールし、needle-in-haystackタスクにおいて高い精度を達成します。 並列化可能な高速な学習アルゴリズム: ニューラル長期記憶モジュールの学習は、並列化可能な高速なアルゴリズムを用いて行われます。 技術や手法のきもはどこにある？ ニューラル長期記憶モジュール: テスト時にデータをパラメータに記憶する方法を学習するメタモデルとして設計されています。人間の長期記憶システムに着想を得ており 、「驚き」の度合いを勾配を用いて測定し、驚くべき入力ほど記憶されやすくなります。 減衰メカニズムにより、記憶容量の制限に対処します。この減衰メカニズムは、最新の再帰モデルにおける忘却メカニズムの一般化となっています。 ミニバッチ勾配降下法のテンソル化: 高速で並列化可能な学習アルゴリズムを実現するために、ミニバッチ勾配降下法をテンソル化し、より多くのmatmul演算を使用します 。 持続的メモリ: タスクに関する知識をエンコードする、学習可能だがデータ非依存のパラメータです。 3つのTitansアーキテクチャ: 短期記憶（限定的なウィンドウサイズのAttention）、長期記憶（ニューラル長期記憶モジュール）、持続的メモリを、(1)コンテキストとして、(2)ゲートとして、(3)層として、それぞれ統合した3つのアーキテクチャバリエーションを提供します。 どうやって有効だと検証した？ 言語モデリング、常識推論、needle-in-haystackタスク、DNAモデリング、時系列予測タスクにおいて、様々なベースラインモデルと比較実験を行い、Titansの有効性を検証しまし た。 特に、長いコンテキストを扱うタスクにおいて、Titansはベースラインモデルよりも優れた性能を示しました。\n議論はあるか メモリ深度の影響: 深いメモリモジュールは性能を向上させるが、トレーニングのスループットを低下させるというトレードオフが存在します。 アーキテクチャのバリエーション: 3つのTitansアーキテクチャはそれぞれ長所・短所があり、効率性と有効性のトレードオフを示しています。 MACとMAGは言語モデリング と常識推論タスクで同等の性能を示しますが、MACは長いコンテキストのNIAHタスクでより優れた性能を示します。 MALはトレーニングが高速ですが、MACやMAGほど性能は高くありません。 計算コスト: 非常に長いシーケンスを扱う場合の計算コストに関する詳細な分析が不足しています。 論文では、大規模モデルの結果を今後のバージョンで報告すると述べられており、今後の研究の発展が期待されます。\n"
            }
    
        ,
            {
                "id": 17,
                "href": "https://www.if-blog.site/posts/paper/paper12/",
                "title": "Transformer2 : Self-adaptive LLMs (AI論文要約)",
                "section": "posts",
                "date" : "2025.01.18",
                "body": "AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nhttps://arxiv.org/abs/2501.06252\nどんなもの Transformer2は、未学習のタスクに対してリアルタイムでLarge Language Models (LLMs) を適応させるための自己適応フレームワークです。重み行列の特異値成分のみを選択的に調整することで、計算コストの高い従来のファインチューニング手法の課題を解決します。推論時には、まずディスパッチシステムがタスクのプロパティを識別し、次に強化学習 を用いて訓練されたタスク固有の「専門家」ベクトルを動的に混合して、入力プロンプトに対するターゲットとなる動作を得ます。\n先行研究と比べてどこがすごいの？ Transformer2は、LoRAなどの従来手法と比べて、より少ないパラメータ数と高い効率性で、優れた性能を発揮します。 従来のファインチューニングは計算コストが高く、多様な タスクへの対応が静的なのに対し、Transformer2はリアルタイムで適応できる点が優れています。また、異なるLLMアーキテクチャやモダリティ（ビジョン言語タスクなど）にも対応できる汎用性を持ちます。先行研究のMixture of Experts (MoE)とは異なり、サンプルレベルのモジュール選択戦略を採用し、強化学習を用いてドメイン固有の知識を獲得した 「専門家」ベクトルを生成する点が異なります。 さらに、Singular Value Fine-tuning (SVF)という新しいパラメータ効率の良いファインチューニング手法を用いることで、少 ないデータ量でも過学習のリスクを抑え、計算コストを削減しています。\n技術や手法のきもはどこにある？ Transformer2の核となる技術は、Singular Value Fine-tuning (SVF) と 二段階推論メカニズム です。\nSVFは、モデルの重み行列$W \\in \\mathbb{R}^{n \\times m}$の特異値分解$W = U\\Sigma V^T$を利用し、特異値$\\Sigma$のスケールのみを調整する手法です。 これにより、学習パラメータ数を大幅に削減し、過学習を防ぎ、かつ、各特異値成分が独立に処理されるため、構成可能性が高まります。 学習されたベクトル$z \\in \\mathbb{R}^r$を用いて、新し い重み行列$W\u0026rsquo; = U\\Sigma\u0026rsquo;V^T$ ($\\Sigma\u0026rsquo; = \\Sigma \\otimes \\text{diag}(z)$) を生成します。\n強化学習を用いて、タスクパフォーマンスを直接最適化することで、効率的にドメイン特異的な「専門家」ベクトルを学習します。目的関数は以下の通りです。\n$$ J(\\theta_z) = \\mathbb{E} \\left[ \\log \\pi_{\\theta_{W\u0026rsquo;}}(\\hat{y}_i | x_i) \\right] r(\\hat{y}_i, y_i) $$\n$$ - \\lambda D_{KL}(\\pi_{\\theta_{W\u0026rsquo;}} | \\pi_{\\theta_W}) $$\nここで、$\\theta_z = {z_1, \\dots, z_{N \\times M}}$ は学習するSVFベクトルの集合、$\\theta_W = {W_1, \\dots, W_{N \\times M}}$は重み行列の集合、$r(\\hat{y}_i, y_i)$ は報酬、$\\lambda$はKLペナルティの係数です。\n二段階推論メカニズムでは、第一段階でモデルを実行し、タスク特性を識別します。第二段階で、第一段階で得られた情報に基づいて、学習済みの「専門家」ベクトルを組み合わ せ、LLMの基本的な重みを調整します。 論文では、プロンプトエンジニアリング、分類器ベース、Few-shot適応の3つの適応戦略が提案されています。\nどうやって有効だと検証した？ 様々なLLM (LLAMA3-8B-INSTRUCT, MISTRAL-7B-INSTRUCT-V0.3, LLAMA3-70B-INSTRUCT)とタスク(GSM8K, MBPP-pro, ARC-Easy, MATH, Humaneval, ARC-Challenge, OKVQA)を用いて、SVFとTransformer2の有効性を検証しました。\nSVFは、LoRAと比較して、少ないパラメータ数で高い性能を示しました。Transformer2は、様々な適応戦略を用いて、未学習のタスクに対しても高い適応能力を示しました。特に、Few-shot適応戦略では、テスト時の条件へのアクセスが増えるにつれて性能が向上することが確認されました。 ビジョン言語タスクに対しても、言語タスクで学習した専門家ベ クトルを用いて性能向上を確認しています。\n議論はあるか CEMベースの適応戦略の効率性: CEMを用いたFew-shot適応は、性能向上に寄与しますが、多くの専門ドメインへの拡張には、一度限りの計算コストの増加が課題となります。しかし、性能向上と自己適応能力の向上によるメリットによって相殺されると主張されています。 SVF専門家の能力の限界: SVF専門家の能力は、ベースモデルの潜在的な成分に依存します。モデルの統合技術が、この問題を解決する可能性があると示唆されています。 クロスモデル互換性: 異なるLLM間でのSVF専門家ベクトルの転移可能性は、モデルアーキテクチャの類似性に依存する可能性があり、さらなる研究が必要です。 "
            }
    
        ,
            {
                "id": 18,
                "href": "https://www.if-blog.site/posts/paper/paper9/",
                "title": "Phi-4 Technical Report (AI論文要約)",
                "section": "posts",
                "date" : "2025.01.11",
                "body": "AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nhttps://arxiv.org/abs/2412.08905\nどんなもの 140億パラメータの言語モデルphi-4。データの質に重点を置いたトレーニングレシピで開発された。ウェブコンテンツやコードなどのオーガニックデータだけでなく、トレーニングプロセス全体を通して戦略的に合成データを取り入れている。\n先行研究と比べてどこがすごいの？ Phiファミリーの以前のモデル（特にGPT-4）の能力を蒸留するだけでなく、STEMに特化したQA能力において教師モデルであるGPT-4oを大幅に上回る。データ生成とトレーニング後の手法が蒸留を超えたことを示している。phi-3アーキテクチャへの変更は最小限だが、改良されたデータ、トレーニングカリキュラム、トレーニング後のスキームの革新により、特 に推論に焦点を当てたベンチマークでサイズに対して高い性能を達成している。 Llama-3.1-405Bなどのはるかに大きなモデルに匹敵する、あるいはそれを超える推論関連タスクにおける性能を示す。\n技術や手法のきもはどこにある？ phi-4の開発は3つの柱に基づいている。\n事前学習と中間学習のための合成データ: 推論と問題解決を優先する高品質の合成データセットを設計。多様性と関連性を確保するために注意深く生成されている。トレー ニングカリキュラムとデータの混合を変更し、以前の世代のphiと比較して合成トークンの割り当てを増やしている。 高品質なオーガニックデータのキュレーションとフィルタリング: ウェブコンテンツ、ライセンス付き書籍、コードリポジトリなどのオーガニックデータソースを注意深く キュレーションし、フィルタリング。合成データパイプラインのシードとして、高度な推論と教育的価値を優先して抽出している。これらの合成データセットを補完するために、事前学習で直接使用するための高品質なデータ（知識と推論の観点から）もウェブからフィルタリングしている。 トレーニング後: 新しい改良版のSFTデータセットを作成し、主要トークン検索に基づいてDPOペアを作成する新しい手法を開発することにより、phi-4のトレーニング後レシピをさらに進歩させている。 合成データは、多様な手法（マルチエージェントプロンプティング、自己修正ワークフロー、命令の反転など）を使用して生成。従来の教師なしデータセットの弱点に対処し、モデルの推論能力と問題解決能力を高めるデータセットを構築する。合成データはトレーニング後にも重要な役割を果たし、棄却サンプリングやDirect Preference Optimization (DPO)への新しいアプローチを使用してモデルの出力を洗練する。 Pivotal Token Search (PTS)という手法を用いて、DPOペアを生成。これは、モデル応答の成功確率に大きな影響を与 える主要なトークン（pivotal tokens）を特定し、それらを重点的に最適化することで、効率的な学習を促進する。\nどうやって有効だと検証した？ 様々な標準ベンチマーク（MMLU、GPQA、MATH、HumanEval、MGSMなど）と、トレーニングデータ収集後に実施されたAMC-10/12数学コンテストを用いて性能を評価。GPT-4oを教師モデルとして、GPQAとMATHベンチマークにおいてphi-4がGPT-4oを大幅に上回ったことを示している。 また、AMC-10/12コンテストでは、同規模のモデルだけでなく、はるかに大きな最先端モデルをも凌駕するスコアを達成。これは、過学習やデータ汚染によるものではないことを示唆している。 さらに、内部ベンチマークPhiBenchを用いて、データ混合やハイパーパラメータの選択、トレーニング後の手法の最適化に役立てている。\n議論はあるか データ汚染: 多くのベンチマークは、事前学習コーパスと重複するデータセットに依存しているため、データ汚染のリスクがある。phi-4ではデータの重複除去と汚染除去に 広範な対策を講じているが、言い換えなどに対しては効果がない可能性があり、一般化の真の程度には不確実性が残る。 ベンチマークのスキル範囲の限定: 多くのベンチマークは、特定のスタイルの数学問題を特定の学年レベルで解いたり、独立したPython関数を実装したりするなど、狭く定義されたスキルをモデルで評価する。この狭い範囲では、モデルのより広範な能力や弱点をとらえることができない可能性がある。 生成ベースのベンチマークにおけるバイアス: 一部のベンチマークは、生成された出力を評価するためにLLMをジャッジとして使用している。これらの判断では、正確性や推 論の妥当性よりも、スタイル、流暢さ、表面的な品質を優先することがあり、採点にバイアスが生じる可能性がある。 多肢選択式タスクの限界: 多肢選択式の問題に依存するベンチマークは、多くの場合、モデルが推論を通じて基礎となる概念を効果的に利用するのではなく、パターンマッチングによって巧妙な推測を行う能力をテストする。 SimpleQA、DROP、IFEvalにおける低いスコア: これらのベンチマークにおけるphi-4の低いスコアは、simple-evalsの評価指標が不十分であること、およびphi-4が厳格な指示に従うことに苦労していることを示唆している。 これらの問題に対処するために、オリジナルの質問、多様なスキル、厳格な採点方法を備えた内部ベンチマークPhiBenchを使用。PhiBenchは、phi-4の開発において中心的な役割を 果たした。\nさらに、phi-4はサイズが限られているため、特に事実に関する知識の幻覚において、特定のタスクでは依然として根本的に制限されている。また、詳細な指示、特に特定の書式要 件を含む指示を厳密に守る能力は低い。 RAI（Responsible AI）の取り組みにもかかわらず、バイアスの再現または増幅、不適切なコンテンツの生成、安全上の問題に関する課題 が残っている。\n"
            }
    
        ,
            {
                "id": 19,
                "href": "https://www.if-blog.site/posts/paper/paper8/",
                "title": "Can LLMs Design Good Questions Based on Context? (AI論文要約)",
                "section": "posts",
                "date" : "2025.01.11",
                "body": "AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nhttps://arxiv.org/abs/2501.03491\nどんなもの 本論文は、Large Language Models (LLMs)を用いたコンテキストベースの質問生成 (Question Generation: QG) の特性を評価する研究である。具体的には、Wikipedia コーパスか ら抽出したパラグラフをコンテキストとして、LLMと人間が生成した質問を6つの評価次元（質問の種類、長さ、コンテキストカバレッジ、回答可能性、独自性、必要な回答の長さ）で比較している。 LLMベースの自動評価手法も提案している。\n先行研究と比べてどこがすごいの？ 先行研究では、LLMによるQGの性能向上は示されてきたものの、LLMが生成する質問の特性に関する詳細な研究は不足していた。本研究は、LLM生成質問と人間生成質問を、6つの評価次元で定量的に比較することで、LLMのQGにおける特性を初めて明らかにしている点で優れている。また、既存の統計的な質問品質基準を拡張し、LLMを用いた自動評価ワークフローを導入している点も新規性が高い。\n技術や手法のきもはどこにある？ LLMにコンテキストと質問生成指示を与え、複数の質問を生成させる。生成された質問を、質問の種類、長さ、コンテキストカバレッジ（単語レベルと文レベル）の3つの回答非依存指標と、回答可能性、独自性、必要な回答の長さの3つの回答依存指標の計6つの次元で評価する。回答依存指標の評価には、LLMを用いた自動評価手法を用いている。コンテキスト カバレッジの評価では、単語レベルと文レベルでのカバレッジに加え、コンテキストのどの部分をLLMが重視しているかについても分析している。\nどうやって有効だと検証した？ GPT-4oとLLaMA-3.1-70b-Instructという2つの代表的なLLMを用いて、Wikipedia コーパスからサンプリングした256個のパラグラフに対してそれぞれ1024個の質問を生成させた。生 成された質問と、既存のHotpotQA、TriviaQAデータセットの人間生成質問を、前述の6つの評価次元で比較した。 評価には、人間によるアノテーションとLLMによる自動評価を組み合わせ、人間のアノテーションとの高い一致率を確認している。\n議論はあるか モデルとプロンプトの限定性: 本研究では、代表的な2つのLLMと特定のプロンプトを用いているため、他のLLMやプロンプトを用いた場合の結果は異なる可能性がある。 大規模な人間アノテーションの不足: リソースの制約から、人間アノテーションはサンプリングされたデータセットに対してのみ行われた。大規模な人間アノテーションによる検証が必要である。 特定のダウンストリームタスクへの適用: 本研究は一般的なQG設定に焦点を当てているが、RAGシステム評価や幻覚検出などの特定のダウンストリームタスクへの適用可能性 についても検討が必要である。 ドメイン特異性: Wikipedia をベースとしたため、金融や医療テキストなどの専門分野のコンテキストを用いた場合の結果は異なる可能性がある。 "
            }
    
        ,
            {
                "id": 20,
                "href": "https://www.if-blog.site/posts/paper/paper7/",
                "title": "rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking (AI論文要約)",
                "section": "posts",
                "date" : "2025.01.11",
                "body": "AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nhttps://arxiv.org/abs/2501.04519\nどんなもの rStar-Mathは、大規模言語モデル(LLM)からの蒸留なしに、小さな言語モデル(SLM)がOpenAI o1と同等以上の数学的推論能力を持つことを示すための手法です。Monte Carlo Tree Search (MCTS)を用いて「深い思考」を行い、数学ポリシーSLMがSLMベースのプロセス報酬モデルによって誘導されるテスト時探索を実行します。\n先行研究と比べてどこがすごいの？ 先行研究では、高品質な数学推論データの不足や、高品質データの合成における課題、ポリシーモデルの訓練における誤った推論ステップの識別困難さ、プロセス報酬モデル(PRM) の訓練データ不足などが問題でした。特に、GPT-4などの優れたLLMを用いた蒸留ベースのデータ合成アプローチは、教師モデルの能力を超えることができませんでした。\nrStar-Mathは、優れたLLMに頼らず、小さな言語モデル(SLM)とMCTSを用いた自己進化プロセスにより、反復的に高品質な訓練データを作成することで、これらの問題を解決します。これにより、わずか70億パラメータのモデルで、OpenAI o1に匹敵、あるいは凌駕する性能を達成しています。\n技術や手法のきもはどこにある？ rStar-Mathは、以下の3つの革新的な技術を導入しています。\nコード拡張型CoTデータ合成法: MCTSの広範なロールアウトを行い、ステップバイステップで検証された推論軌跡を生成します。各ステップで、ポリシーモデルとしてのSLM が候補ノードをサンプリングし、1ステップのCoTとその対応するPythonコードを生成します。Pythonコードの実行が成功した場合のみノードが保持され、中間ステップのエラーを軽減します。さらに、広範なMCTSロールアウトにより、各中間ステップへの貢献度に基づいてQ値が自動的に割り当てられます。\n新規プロセス報酬モデル訓練法: ステップレベルの正確なスコア注釈を回避し、より効果的なプロセス選好モデル(PPM)を生成します。Q値は各推論ステップを正確に評価す るには不十分ですが、正（正しい）ステップと負（無関係/間違っている）ステップを確実に区別できます。この訓練方法は、Q値に基づいて各ステップの選好ペアを構築し、ペアワイズランキング損失を使用してPPMのスコア予測を最適化します。\n自己進化レシピ: ポリシーSLMとPPMをゼロから構築し、反復的に進化させて推論能力を向上させます。747,000個の数学問題に対して、数百万の合成解を用いて4ラウンドの 自己進化を行うことで、SLMの数学的推論能力を最先端レベルにまで向上させます。\nどうやって有効だと検証した？ 4つのSLM（15億〜70億パラメータ）と7つの数学推論タスクを用いた広範な実験を行い、rStar-Mathの有効性を検証しました。rStar-Mathは全てのSLMを改善し、困難な数学競技ベンチマークにおいてOpenAI o1に匹敵、あるいは凌駕する性能を達成しました。MATHベンチマークでは、Qwen2.5-Math-7Bの精度を58.8%から90.0%に、Phi3-mini-3.8Bの精度を41.4%か ら86.4%に改善しました。USA Math Olympiad (AIME)では、平均53.3%の問題を解き、優秀な高校生のトップ20%にランクインしました。ステップバイステップで検証された推論軌跡 の有効性、PPMの有効性についても、最先端のデータ合成ベースラインや他の報酬モデルと比較することで検証しています。\n議論はあるか 自己進化の計算コスト: 自己進化には多大な計算リソースを必要とする点が議論の余地があります。特に、より困難な問題を解くための追加のロールアウトは、計算コストを大幅に増加させます。 一般化可能性: 現状は主に算数の文章題に焦点を当てているため、定理証明など他の数学的タスクへの一般化可能性については、さらなる検証が必要です。しかし、付録では定理証明の例も示されており、可能性を示唆しています。 PPMの限界: PPMはステップレベルの正確なスコア注釈を回避する代わりに、選好ペアを構築することで訓練されます。この方法が、全ての状況で最適な方法であるとは限りません。より高度な報酬モデルの開発が今後の課題となります。 自己反省能力の出現: MCTSによる深い思考が、自己反省能力の出現につながる可能性が示唆されていますが、そのメカニズムや再現性についてはさらなる研究が必要です。 "
            }
    
        ,
            {
                "id": 21,
                "href": "https://www.if-blog.site/posts/paper/paper5/",
                "title": "Fact-aware Sentence Split and Rephrase with Permutation Invariant Training (AI論文要約)",
                "section": "posts",
                "date" : "2025.01.02",
                "body": "AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nhttps://arxiv.org/abs/2001.11383\nどんなもの 複雑な文を意味を保ったまま複数の簡単な文に分割・言い換える Sentence Split and Rephrase タスクのための新しいフレームワーク。従来の seq2seq 学習の限界を克服するために、Fact-aware Sentence Encoding (FaSE) と Permutation Invariant Training (PIT) を提案している。\n先行研究と比べてどこがすごいの？ 先行研究の seq2seq モデルは、(1) 元の文に述べられている事実を考慮しておらず、生成された簡単な文で事実が誤って表現されたり、欠落したりする可能性がある、(2) 生成さ れる簡単な文の順序のばらつきに影響を受けやすく、訓練が不安定になる、という2つの限界があった。本研究では、FaSEとPITを用いることでこれらの問題を解決し、WebSplit-v1.0 ベンチマークデータセットにおいて、従来の seq2seq 学習アプローチよりも大幅に性能を向上させ、最先端の結果を達成した。さらに、OpenIEタスクにおける外的な評価でも有 効性を確認した。\n技術や手法のきもはどこにある？ Fact-aware Sentence Encoding (FaSE): マルチタスク学習を用いてエンコーダを強化する。Sentence Split and Rephrase タスクだけでなく、与えられた事実が文に基づいて真か偽かを判定する事実分類タスクも同時に行うことで、事実を正確に捉える事実認識テキストエンコーダを実現する。\nPermutation Invariant Training (PIT): 参照における簡単な文の最適な順列を見つけることで、順序のばらつきによる問題を軽減する。すべての順列を列挙し、各順列に対 する損失を計算し、最小損失の順列を選択してモデルを更新する。これにより、モデルが以前学習したパターンに反して学習することを避け、訓練の安定性を向上させる。\nどうやって有効だと検証した？ WebSplit-v1.0 ベンチマークデータセットを用いた実験: 提案手法は、従来の seq2seq モデルよりも大幅に性能を向上させ、最先端の結果（37.3 BLEUスコア）を達成した 。 ablation studyにより、FaSEとPITそれぞれの有効性を示した。 順序のばらつきに関する実験: 異なる順列選択方法(MAX, RANDOM, MIN)による実験を行い、PIT(MIN)が大幅に優れていることを示した。RANDOMの設定では標準偏差が大きく 、従来のseq2seqモデルが順序のばらつきに影響を受けやすいことを示した。 WikiSplit データセットを用いた事前学習: WikiSplitを用いた事前学習により、さらに性能向上（71.0 BLEUスコア）を確認した。 OpenIE タスクにおける外的な評価: 提案モデルを Stanford OPENIE システムの前処理として使用することで、性能が向上することを示した。in-domainとoie-benchmarkの 両方で有効性を確認した。 ヒューマンアノテーション: 文法性、意味の保存、構造の単純さの3つの観点からヒューマンアノテーションを行い、提案モデルが参照に近い高評価を得ていることを確認した。 議論はあるか 論文では、提案手法の有効性を示す多くの実験結果を示しているが、より大規模なデータセットや異なる言語への適用可能性、計算コストに関する議論は限定的である。また、事実分類タスクとSentence Split and Rephraseタスクのバランスを調整するハイパーパラメータλの最適化方法についても詳細な検討が必要となるだろう。\n"
            }
    
        ,
            {
                "id": 22,
                "href": "https://www.if-blog.site/posts/paper/paper4/",
                "title": "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention (AI論文要約)",
                "section": "posts",
                "date" : "2025.01.02",
                "body": "AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nhttps://arxiv.org/abs/2404.07143\nどんなもの Transformerベースの大規模言語モデル(LLMs)を、bounded memoryとcomputationで無限長の入力にスケールするための効率的な手法であるInfini-attentionを提案する。Infini-attentionは、従来のAttention機構に圧縮メモリを取り込み、masked local attentionとlong-term linear attention機構を単一のTransformerブロックに統合している。10億パラメータと80億パラメータのLLMを用いて、長文脈言語モデリングベンチマーク、100万トークン長のパスキーコンテキストブロック検索タスク、50万トークン長の書籍要約タスクで有効性を示した。\n先行研究と比べてどこがすごいの？ 先行研究であるTransformer-XLやCompressive Transformer、Memorizing Transformers、RMT、AutoCompressorsと比較して、Infini-attentionは、bounded memoryで無限長の文脈を処理できる点が優れている。具体的には、メモリサイズはMemorizing Transformersと比べて114倍小さく、かつ、より低いperplexityを達成する。また、10億パラメータのLLMにおいて、100万トークン長のパスキー検索タスクを解き、80億パラメータのLLMにおいて、50万トークン長の書籍要約タスクでSOTAを達成した。先行研究では、メモリサイズが 文脈長に比例して増加したり、圧縮に依存して効率が低下したりする問題があったが、本手法ではこれらの問題を解決している。\n技術や手法のきもはどこにある？ Infini-attentionの核心は、従来のAttention機構にcompressive memoryを統合し、masked local attentionとlong-term linear attentionを組み合わせている点 にある。compressive memoryには、AttentionのKeyとValueの状態を再利用し、過去のKV状態を破棄するのではなく保存することで、無限長の文脈履歴を保持する。long-term linear attentionを用いてメモリから値を検索し、masked local attentionによる局所的な文脈情報と統合することで、最終的な文脈出力を計算する。メモリ更新には、associative matrixとdelta ruleを採用し、計算効率と安定性を両立している。さらに、gating scalar βを用いて、長期記憶と局所的情報のバランスを学習する。\nどうやって有効だと検証した？ 長文脈言語モデリングベンチマーク、100万トークン長のパスキーコンテキストブロック検索タスク、50万トークン長の書籍要約タスクの3つのタスクで検証を行った。言語モデリングタスクでは、Infini-TransformerがTransformer-XLやMemorizing Transformersを上回る性能を示した。パスキー検索タスクでは、10億パラメータのLLMにInfini-attentionを適用することで、100万トークン長の入力に対しても高い精度でパスキーを検索できた。書籍要約タスクでは、80億パラメータのLLMにInfini-attentionを適用することで、SOTAを達成した。\n議論はあるか 論文では、Infini-attentionにおけるgating scoreの挙動について、専門化されたヘッドとミキサーヘッドの2種類が存在することを示している。しかし、この挙動がどの ようなメカニズムで生じるのか、さらなる分析が必要である。また、異なる種類のcompressive memoryやmemory update、memory retrieval手法を用いた場合の性能比 較についても、今後の研究課題として挙げられる。さらに、極端に長いシーケンスに対する汎化性能についても、より詳細な検証が必要である。\n"
            }
    
        ,
            {
                "id": 23,
                "href": "https://www.if-blog.site/posts/paper/paper3/",
                "title": "Only-IF :Revealing the Decisive Effect of instruction diversity on Generalization (AI論文要約)",
                "section": "posts",
                "date" : "2025.01.02",
                "body": "AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nhttps://arxiv.org/abs/2410.04717\nどんなもの 大規模言語モデル（LLM）の指示遵守能力を向上させるためのデータセット構築戦略に関する研究。特に、instruction diversity（指示の多様性）が汎化性能に与える決定的な影響を明らかにしている。 制御された実験を通して、instruction-following（指示遵守）能力に焦点を当て、推論や知識検索能力とは切り離して評価している。\n先行研究と比べてどこがすごいの？ 先行研究では、データセットの選択やデータ量の増加がLLMの性能に与える影響について、ばらつきのある結果が報告されていた。本研究は、instruction-following能力に焦点を絞り、instruction diversityの重要性を厳密に検証した点が優れている。 特に、Turing-complete Markov algorithmに着想を得た制御された実験により、多様な意味 領域にわたるデータの多様化が、未知の指示への汎化に不可欠であることを示している。単に限定的な領域内で多様化しても、堅牢な汎化は保証されないことを明らかにしている。\n技術や手法のきもはどこにある？ Turing-complete Markov algorithmに着想を得た文字列置換タスクを用いた制御実験が中心。このタスクは、LLMのinstruction-following能力を、推論能力とは独立に評価できるシンプルなモデルを提供する。 実験では、instruction diversityを様々な方法で変化させ（指示の数、No-Opの割合、指示の分布、意味制約など）、未知の指示に対する汎化性能を測定している。さらに、専門家モデルと汎用モデルのファインチューニング実験にも拡張し、現実世界のシナリオでの有効性を検証している。\nどうやって有効だと検証した？ 制御実験（文字列置換タスク）: 異なるレベルのinstruction diversityで訓練したGPT-2モデルを、未知の指示でテスト。指示の多様性が一定の閾値を超えると、汎化 性能が劇的に向上することを示した。\n抽象化を導入した実験: 数学的推論タスクを用いて、抽象的な規則とその具体的な適用（groundings）を扱うより複雑な状況をシミュレート。 規則の多様性の増加が汎化性能を向上させることを確認した。\n現実世界での実験（コード生成）: HumanEval、MBPP、EvalPlusなどのベンチマークを用いて、コード生成モデルのファインチューニング実験を実施。コード特化データと一 般ドメインデータ（Alpacaなど）を混ぜることで、性能向上を確認した。\n現実世界での実験（汎用LLM）: UltraInteract-SFT、OpenOrca、Alpacaのデータ混合による汎用LLMのファインチューニング実験。データの多様化が、データ量の増加よりも 効果的であることを示した。\n議論はあるか 現実世界のデータセットでは、指示が完全に均一に分布することはありえない。本研究では、power law distributionを用いて不均衡な分布の影響を検証し、十分な意味的カバレッジがあれば汎化が可能であることを示唆しているが、現実世界のデータセットにおける不均衡の影響については更なる検討が必要となる可能性がある。また、専門家モデルの訓練において、専門領域外のデータの導入による性能向上は、どの程度の領域の広がりが必要なのか、最適なバランスは何かといった点については、今後の研究でより詳細な検討が必要である。さらに、異なるデータセットを混合する際の最適な比率についても、タスクやモデルによって異なる可能性があり、普遍的な最適解を見つけることは難しいかもしれない。\n"
            }
    
        ,
            {
                "id": 24,
                "href": "https://www.if-blog.site/posts/paper/paper2/",
                "title": "Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation Engineering (AI論文要約)",
                "section": "posts",
                "date" : "2025.01.02",
                "body": "AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nhttps://arxiv.org/abs/2410.15999\nどんなもの 本論文は、大規模言語モデル (LLM) の知識選択行動を、事前学習済みスパースオートエンコーダ (SAE) を用いた表現エンジニアリング手法 SPARE を通じて制御する手法を提案し ている。LLM はパラメータ内に多くの事実的知識を保持するが、文脈情報と矛盾する可能性があり、その結果、古いまたは不正確な情報への依存といった望ましくない挙動につながる。SPARE は、LLM の内部活性化を推論時に編集することで、文脈知識とパラメータ知識のどちらを使用するかを制御する。\n先行研究と比べてどこがすごいの？ 既存の表現エンジニアリング手法は、LLM の内部活性化（隠れ状態や MLP 活性化など）を直接変更するが、これらの活性化は多義的な高密度ベクトルであり、正確な調整が難しい 。本研究では、事前学習済み SAE を用いることで、多義的な活性化を多数の単義的な特徴に分解し、より正確な活性化編集を可能にしている。その結果、既存の表現エンジニアリ ング手法やコントラスティブデコーディング手法よりも、知識衝突時の知識選択行動の制御において高い精度を達成している（それぞれ+10%、+15%の向上）。\n技術や手法のきもはどこにある？ SPARE の核心は、事前学習済み SAE を用いた表現エンジニアリングにある。具体的には以下の3つのステップからなる：\n異なる知識選択行動につながる活性化の収集: 知識衝突を引き起こす入力インスタンスを、モデルの予測に基づいて、文脈知識を選択したグループとパラメータ知識を選択 したグループに分割する。それぞれのグループから、回答の最初のトークンを予測するために使用される最後の位置の隠れ状態を収集する。\n機能的 SAE 活性化の特定: 収集した隠れ状態を SAE でエンコードし、各 SAE 活性化と知識選択行動間の相互情報量を計算する。相互情報量が高い上位 k 個の活性化を、 知識選択行動を制御する機能的活性化として選択する。\n活性化の編集による行動の制御: 機能的活性化に基づき、推論時にLLMの隠れ状態を編集する。望ましくない知識選択行動に関連する特徴を除去し、望ましい知識選択行動 に関連する特徴を追加する。この編集は、SAE のデコーダを用いて行われ、元の隠れ状態の情報を可能な限り保持するように設計されている。\nどうやって有効だと検証した？ オープン・ドメイン質問応答 (ODQA) タスクにおいて、NQSwap と Macnoise データセットを用いて実験を行った。SPARE を、既存の表現エンジニアリング手法（TaskVec、ActAdd、SEA）、コントラスティブデコーディング手法（DoLa、CAD）、インコンテキスト学習 (ICL) と比較した。評価指標として、文脈知識とパラメータ知識のそれぞれを使用するように 制御した際の正解率 (EM) を用いた。その結果、SPARE はすべてのベースライン手法を上回り、知識選択行動の制御において高い有効性を示した。さらに、行動変化能力、介入の負の影響、そして ablation study を通じて、SPARE の有効性を多角的に検証している。\n議論はあるか 論文ではいくつかの限界が指摘されている。\nSAE の事前学習への依存: SPARE は事前学習済み SAE に依存するため、事前学習済み SAE が利用できないモデルには適用できない。\nタスクへの汎化性: ODQA タスクに限定された実験結果であり、他のタスクや複雑な推論、マルチホップ質問、長文生成などへの汎化性は不明である。\n知識ソース選択の二値化: 知識ソースの信頼性の判断を二値化しており、実際にはより複雑な判断が必要となる可能性がある。\n全体として、SPARE はLLMの知識選択行動を効果的に制御する有望な手法であるが、さらなる研究が必要であることが示唆されている。特に、SAE の事前学習の必要性の軽減や、よ り広範なタスクへの適用可能性の検証が今後の課題となる。\n"
            }
    
        ,
            {
                "id": 25,
                "href": "https://www.if-blog.site/posts/paper/paper1/",
                "title": "Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization(AI論文要約)",
                "section": "posts",
                "date" : "2025.01.02",
                "body": "AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。\nhttps://arxiv.org/abs/2410.12949\nどんなもの 本論文は、大規模言語モデル(LLM)から望ましくない知識を削除または修正する「知識のアンラーニングと編集」手法に関する研究です。特に、モデルの特定の機能に関連する構成 要素（回路）を特定することを目指す「メカニズム的解釈可能性」を用いて、アンラーニングと編集の精度と有効性を向上させる手法を提案しています。\n先行研究と比べてどこがすごいの？ 先行研究では、出力の保存に基づいて構成要素を特定する「出力追跡（OT）」手法が用いられてきましたが、編集の堅牢性に課題がありました。わずかなプロンプトの変更でも元の情報が引き出されたり、編集が不十分で、元の回答がモデル内に残存したりする問題がありました。\n本研究では、予測可能な中間状態を持つ高レベルのメカニズムに基づいて構成要素を特定する手法を提案し、特に「事実の参照メカニズム」に関連する構成要素に編集/アンラーニ ングを局所化することで、以下の点で先行研究を凌駕しています。\n堅牢性の向上: 異なる入力/出力形式においても、より堅牢な編集/アンラーニングを実現します。\n再学習への耐性: 望ましくない情報の再学習を防ぎます。\n副作用の軽減: 意図しない副作用を削減します。\n潜在知識の攪乱: 特定の局所化された編集は、他のベースラインよりもモデル内の潜在知識をより強く攪乱し、様々な攻撃に対してより堅牢なアンラーニングを実現します 。\n技術や手法のきもはどこにある？ 本研究の核心は、「事実の参照メカニズム（FLU）」に焦点を当てた「メカニズム的アンラーニング」です。\nFLUの特定: モデル内部の活性化を分析し、事実の参照と属性抽出に関連するMLP層を特定します（Sports Factsデータセットではプローブを用い、CounterFactデータセットではパスパッチングを用いています）。\n局所化: FLUメカニズムを実装する構成要素にのみ編集/アンラーニングを局所化します。\n重み更新: 局所化された構成要素の重みを勾配降下法を用いて更新します。損失関数は、望ましくない事実の確率を最小化し、残りの事実の確率を最大化する項と、一般言語モデリング能力を維持するための項から構成されます。\nどうやって有効だと検証した？ 様々なデータセット（Sports Facts、CounterFact）、モデル（Gemma-7B、Gemma-2-9B、Llama-3-8b）、評価方法を用いて、提案手法の有効性を検証しました。\nプロンプトベース評価: 編集後のモデルが、元の情報、編集後の情報、関連のない情報をどの程度想起できるかを評価しました。多様なプロンプト形式（通常のプロンプト 、多肢選択式質問、言い換えられたプロンプト、類似事実を含むプロンプト）を用いて、編集の堅牢性を検証しました。\n敵対的再学習評価: 編集されたモデルを、削除された事実の一部を用いて再学習させ、元の情報がどの程度回復するかを評価しました。\n潜在知識分析: プローブを用いて、モデル内部表現における元の情報と編集後の情報の表現の変化を分析しました。\nパラメータ効率性: 重みマスクを用いて、編集のサイズを制御し、パラメータ効率性を検証しました。\n議論はあるか？ 先行研究では、局所化がモデル編集に有用でないという主張がありましたが、本研究は、すべての局所化手法が等しいわけではないことを示しています。出力追跡（OT）手法は、容易に局所化できる出力層に焦点を当てるため、堅牢性に欠ける可能性があります。一方、FLU手法は、知識の源泉に直接作用するため、より堅牢な編集/アンラーニングを実現します。\n"
            }
    
        ,
            {
                "id": 26,
                "href": "https://www.if-blog.site/posts/nlp/mixture_of_experts_sample/",
                "title": "Mixture of expertsのサンプル実装",
                "section": "posts",
                "date" : "2023.12.16",
                "body": "mixture of expertsを実装してみる。 並列化や計算効率の向上などの部分は複雑なので、それら取り除いた簡単な実装を行ってみる。\nMixture of expertsとは 特定のタスクに特化したexpertを複数用意し、入力に対してexpertを切り替えることで性能を上げる手法。 expertを選択するgate部分とexpert部分からなる。\ndecoder型のtransformer、特にcausal modelの場合、Mixture of expertsはattention層のあとのFFNに対して適応される。 (学習時では、attention層とexpertやgate部分は同時に進行していくが、expertにちゃんとわかれるのかがよくわからん\u0026hellip; 一応loss関数でとあるexpertだけに集中しないようになどの工夫はあるが)\n図\nhttps://arxiv.org/abs/2101.03961\ngate関数やexpert層の違いによりいくつかのMoEの種類が提案されているが、sparse MoEを使ったものが多く見られる\nsparse MoE gate関数が上位n個のexpertを選択するような出力を行う。選択するexpert以外の部分は0になるようなスパース性を持つ。\n0が含まれることで選択されないexpertでは計算する必要がないため、計算コストが削減される\nmixtralの実装はこんな感じ\nhttps://github.com/huggingface/transformers/blob/238d2e3c44366aba9dc5c770c95475765a6725cb/src/transformers/models/mixtral/modeling_mixtral.py#L688\n通常のMoEでは、特定のexpertが多く選択されてしまうことがあり、学習が非効率なものとなってしまう。 これを軽減するため、auxiliary lossを導入する。\n図\n$f_i$は$expert_i$に割り振られたトークンの割合 $P_i$は$expert_i$に割り振られる可能性の確率\n$f_i$と$P_i$が均等であれば、ルーティングも均等になるというlossになる\nmixtralのlossの実装はこんな感じ\nhttps://github.com/huggingface/transformers/blob/238d2e3c44366aba9dc5c770c95475765a6725cb/src/transformers/models/mixtral/modeling_mixtral.py#L76\nST-MoE Stable and Transferable Mixture-of-Expertsの提案 (設計ガイドとして良さそうなのであとでちゃんと読む)\nhttps://arxiv.org/abs/2202.08906\n実装が以下で公開されている\nhttps://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py\n実装 今回はsparseでなく、もっと単純なもので動きを確認する。\nimport torch import torch.nn as nn import torch.nn.functional as F class MoE(nn.Module): def __init__(self, embed_size=128, num_experts=4, expert_hidden_size=256, gate_hidden_size=256, device=\u0026#34;auto\u0026#34;, ): super(MoE, self).__init__() self.embed_size = embed_size self.num_experts = num_experts expert = nn.Sequential( nn.Linear(embed_size, expert_hidden_size), nn.ReLU(), nn.Linear(expert_hidden_size, embed_size) ) self.experts = nn.ModuleList([expert for _ in range(num_experts)]) self.gate = nn.Sequential( nn.Linear(embed_size, gate_hidden_size), nn.ELU(), nn.Linear(gate_hidden_size, num_experts), ) def forward(self, x): \u0026#34;\u0026#34;\u0026#34; Forward pass for MoE :param x: Input tensor of shape (batch_size, seq_len, embed_size) :return: Output tensor. shape (batch_size, seq_len, embed_size) \u0026#34;\u0026#34;\u0026#34; # batch_size, seq_len, _ = x.size() gating_scores = self.gate(x) gating_weights = F.softmax(gating_scores, dim=2) # (batch_size, seq_len, num_experts) print(\u0026#39;gating_weights\u0026#39;, gating_weights.shape, gating_weights) expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=2) # (batch_size, seq_len, num_experts, embed_size) ## gating_weights.unsqueeze(-1) # (batch_size, seq_len, num_experts, 1) output = torch.sum(gating_weights.unsqueeze(-1) * expert_outputs, dim=2) # (batch_size, seq_len, embed_size) return output 実装自体はシンプルで、gate関数の出力をsoftmaxを取ったもので、各expertを割合で増減させるイメージ。\n例えば、以下のような例を入力とする (実際には埋め込んだ状態でなく、attention weightを計算したあとのtensorになるので注意)\nsequence_length=3 埋め込みサイズ=4\n[I am running] =\u0026gt; [[0.4081, 0.7244, 0.0221, 0.1677],[0.2508, 0.8103, 0.3783, 0.0399],[0.6584, 0.4243, 0.5941, 0.6102]] expert=2, batch_size=1の例を入力する\n入力shapeは(batch_size,sequence_length, 埋め込みサイズ)\ngating_weightsは、sequence_length毎にどのexpertに割り振られるかの確率となっている\ninput: torch.Size([1, 3, 4]) tensor([[[0.4081, 0.7244, 0.0221, 0.1677], [0.2508, 0.8103, 0.3783, 0.0399], [0.6584, 0.4243, 0.5941, 0.6102]]]) gating_weights torch.Size([1, 3, 2]) tensor([[[0.5903, 0.4097], [0.6000, 0.4000], [0.6028, 0.3972]]], grad_fn=\u0026lt;SoftmaxBackward0\u0026gt;) output: torch.Size([1, 3, 4]) tensor([[[ 0.5622, -0.0714, 0.0197, -0.2871], [ 0.6389, -0.1067, -0.0396, -0.2993], [ 0.4812, -0.0484, 0.0448, -0.4018]]], grad_fn=\u0026lt;SumBackward1\u0026gt;) 参考 MoEについて https://huggingface.co/blog/moe\nhttps://deeplearning.hatenablog.com/entry/moe\nmoe実装例 https://github.com/huggingface/transformers/blob/238d2e3c44366aba9dc5c770c95475765a6725cb/src/transformers/models/mixtral/modeling_mixtral.py#L688\nhttps://hungyuling.com/blog/fast-mixture-of-experts-in-pytorch/\nhttps://github.com/laekov/fastmoe/blob/master/README.md\nhttps://github.com/lucidrains/mixture-of-experts/blob/master/mixture_of_experts/mixture_of_experts.py\n"
            }
    
        ,
            {
                "id": 27,
                "href": "https://www.if-blog.site/posts/nlp/hf_generation_wrap/",
                "title": "huggingfaceのgenerationの関数をtorch modelから使えるようにしたい",
                "section": "posts",
                "date" : "2023.12.05",
                "body": "torchなどのライブラリを使いpre_trainingを行い文章生成させる際、文章生成の計算は基本的に自分で実装する必要がある。 huggingface用のmodelに変換しても良いが、おれおれアーキテクチャにした場合、変換も面倒\u0026hellip;\nそこで、huggingfaceにあるtop_kやtop_pなどの便利な実装を使えるようにするメモ。\n文章生成についての手法は以下がわかりやすくておすすめ\nhttps://huggingface.co/blog/how-to-generate\nlogits_processの実装 logits_processの処理は、以下にまとまっている。\nhttps://github.com/huggingface/transformers/blob/v4.35.2/src/transformers/generation/logits_process.py\n例えば、top_kは以下のような実装となっている。\ndef __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -\u0026gt; torch.FloatTensor: top_k = min(self.top_k, scores.size(-1)) # Safety check # Remove all tokens with a probability less than the last token of the top-k indices_to_remove = scores \u0026lt; torch.topk(scores, top_k)[0][..., -1, None] scores = scores.masked_fill(indices_to_remove, self.filter_value) return scores 参照:https://github.com/huggingface/transformers/blob/v4.35.2/src/transformers/generation/logits_process.py#L448-L453C22 よく使うのは、top_pやtop_k、NoRepeatNGramLogitsProcessor、RepetitionPenaltyLogitsProcessorあたり\nNoRepeatNGramLogitsProcessor\nhttps://github.com/huggingface/transformers/blob/v4.35.2/src/transformers/generation/logits_process.py#L718\nRepetitionPenaltyLogitsProcessor\nhttps://github.com/huggingface/transformers/blob/v4.35.2/src/transformers/generation/logits_process.py#L270\nhuggingfaceでの実装 よく使うのは、model.generate だが、generateからsampleが選択される部分はここ\nhttps://github.com/huggingface/transformers/blob/v4.35.2/src/transformers/generation/utils.py#L1706\nsample関数の実装\nhttps://github.com/huggingface/transformers/blob/v4.35.2/src/transformers/generation/utils.py#L2612\nhuggingfaceのsampleでは、logits_processorは次のように使われている\nnext_token_scores = logits_processor(input_ids, next_token_logits) next_token_scores = logits_warper(input_ids, next_token_scores) ... probs = nn.functional.softmax(next_token_scores, dim=-1) next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1) 参照:https://github.com/huggingface/transformers/blob/v4.35.2/src/transformers/generation/utils.py#L2813-L2816 greedy searchの実装はここ\nhttps://github.com/huggingface/transformers/blob/v4.35.2/src/transformers/generation/utils.py#L2353\nargmaxを使っているのがわかる\nnext_tokens = torch.argmax(next_tokens_scores, dim=-1) 参照:https://github.com/huggingface/transformers/blob/v4.35.2/src/transformers/generation/utils.py#L2555 torchのmodelの出力に対して使う modelの作り方にも依るが、基本的にはlogitが出力されるように作るはず。\n使い方は、LogitsProcessorListを定義して、ここまでの入力xとlogitを渡すだけ。 xはtokenizerされた単語のlist、例えば[20, 15, 1, 1120, ....]のようなtensorとなる。\n用途によってtensorの次元を合わせてください。\nfrom transformers.generation.utils import ( LogitsProcessorList, TopKLogitsWarper, TopPLogitsWarper, TemperatureLogitsWarper, RepetitionPenaltyLogitsProcessor ) logits_processor = LogitsProcessorList([ RepetitionPenaltyLogitsProcessor(repetition_penalty), ]) logits_wraper = LogitsProcessorList([ TemperatureLogitsWarper(temperature), TopPLogitsWarper(top_p), TopKLogitsWarper(top_k), ]) ## modelからlogitsを得る logits = model(x, input_pos) logits = logits[:, -1, :] ## sequenceの最後のlogit next_token_scores = logits_processor(x, logits) next_token_scores = logits_wraper(x, next_token_scores) next_token_scores = next_token_scores.squeeze(0) ## 次元の調整 ## next_token_scoresに対してsoftmaxで確率にして、次のtokenのindexを出力する probs = torch.nn.functional.softmax(next_token_scores, dim=-1) next_idx = torch.multinomial(probs, num_samples=1) 所感 top_k, temperatureくらいだと自分で実装しても良いが、RepetitionPenaltyはここまでのindexも見る必要がありめんどうだったので、huggingfaceの実装を使った。\n他にも便利な処理もあるので、一度使えるようにしておくと便利。\n"
            }
    
        ,
            {
                "id": 28,
                "href": "https://www.if-blog.site/posts/nlp/transformer_any_architecture/",
                "title": "llama2のアーキテクチャを変更してpre trainingしてみる",
                "section": "posts",
                "date" : "2023.12.05",
                "body": "できるだけ小さいサイズのモデルで日本語を喋れるモデルを作りたい、ということでllama2のpre_trainingはすでにおこなったがさらなる改善のため、いくつかのアーキテクチャを考えてみる。\n比較のベースとなる標準的なモデルに対し、新たに3つのモデルを作成し比較する\ntransformerのattention部分に対して修正や変更を加えたものはEfficient Transformerと呼ばれる。attentionの計算では、keyとvalueの行列積の計算のコストが高く、この部分を近似や次元削減を行い、計算効率やメモリ効率の向上を目指したものがいくつか提案されてる。Linformerでは近似により、ReformerではLSHというアルゴリズムによりattentionの計算を行っている。\nLinformer\nhttps://arxiv.org/abs/2006.04768\nReformer\nhttps://arxiv.org/abs/2001.04451\nモデル ベースのモデルに対し少しづつ複雑さを増したモデルを作成する\nベースのモデル head数1、layer数10のモデルをベースモデルとする。 attentionの部分を簡単に表すと以下のようになる。attentionへの入力は全結合層を通り、queryとkey, valueに分割される。queryとkeyの積を取りscaleした後softmaxを取ったものとvalueの積を取る。(実際にはhead数の次元やgrouped queryが含まれるが省略)\nベースのモデルに対していくつかの修正を行う\nいずれのモデルもhead数やlayer数はベースモデルと同じ\nモデル1(非線形活性化関数+bias) query, key, valueを出力するFCNに対して、biasと活性化関数geluを追加する。\nモデル2(query, key, valueを別々の全結合層で生成) query, key, valueは一つの全結合層の出力を分割して作られていたが、これを別々の層で計算させる。\nqueryとkeyで異なる特徴を捉えてほしい期待\nモデル3(query*keyにscaleを追加) 通常のattention weightでは、queryとkeyの積をsoftmaxを取り、valueとの積を取る。\nqueryとkeyの積に対して、別のFCNを追加しその出力を足し合わせて、softmax、valueとの積を取るようにする。\nこれは、queryに対応するkeyを取り出す処理に対し、attentionへの入力を使いquery,key以外の別の視点でscaleを行ってほしいという期待\n実装 参考にするコード https://github.com/Lightning-AI/lit-gpt\nattention部分はここ\nhttps://github.com/Lightning-AI/lit-gpt/blob/main/lit_gpt/model.py#L169\nattentionへの入力は線形結合を通った後、query, key, valueに分割される。\nquery, key, valueに対し、head数に対する次元操作や、grouped queryの操作を行った後、attentionを計算している。\nattentionの実装はtorchのものを使用している。\nhttps://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\nqueryとkeyの積を計算し、maskを適応、softmaxを取り、attention_weightを算出している。\nattn_weight = query @ key.transpose(-2, -1) * scale_factor attn_weight += attn_bias attn_weight = torch.softmax(attn_weight, dim=-1) ... return attn_weight @ value このあたりを修正する\n学習 100Mのモデルに対し、1Bの日本語と英語のデータセットで2epoch学習行う\nモデル1とモデル2は100Mのサイズ、モデル3のみ追加の層があるので120M\n学習はPaperspace Gradientを使う\nhttps://www.paperspace.com/\nlossと学習時間の結果 lossとval lossの比較 2epoch学習させた最終的なlossとval lossを表にまとめる\nmodel loss val loss ベースモデル 4.0861 4.8150 非線形活性化関数+bias 4.2262 4.6753 query, key, valueの生成を分ける 4.8802 4.7191 query*keyにscaleを追加 4.9979 4.7278 ベースモデルについて、他のモデルと比べlossは一番低いが、val lossは高くなっており、過学習の可能性がある。\nval lossについては、非線形活性化関数+biasが一番低く、query, key, valueの生成を分けるとquery*keyにscaleを追加 は近い値となった。\n今回のモデルサイズとデータセット的にベースモデルより少し複雑なモデルのほうが適していそう\nただし、query, key, valueの生成を分けるやquery*keyにscaleを追加ほどの複雑さは不要かもしれない\n学習にかかった時間 model 学習にかかった時間 (h) ベースモデル 13.6 非線形活性化関数+bias 13.3 query, key, valueの生成を分ける 12.2 query*keyにscaleを追加 14.6 ベースモデルと非線形活性化関数+biasは同じくらいの学習時間であり、非線形の計算やbiasは大きなコストではないことがわかる。\nquery, key, valueの生成を分けるはベースモデルより学習時間は短くなっている。これは、query, key, valueの計算が効率的に行われた可能性がある。\nquery*keyにscaleを追加ではparameterが増えている分、より多くの時間が必要となったと考えられる。\nまとめると、複雑過ぎるモデルでは学習時間が長くなりlossやval lossにおいても良い結果とはならなかった。 一方で、非線形活性化関数+biasやquery, key, valueの生成を分けるではlossや学習時間の面でベースモデルよりよい結果となった。 ある程度の複雑性は必要だが、そのバランスは注意する必要がある。\n生成された文章での比較 「彼女が子供の頃に住んでいた、海の見える小さな町は、」を与えて続く文章を生成する。\nベースモデル 彼女が子供の頃に住んでいた、海の見える小さな町は、その町と隣接し、町は、町が広がる場所にある。\n非線形活性化+bias 彼女が子供の頃に住んでいた、海の見える小さな町は、その街の風景を描いた。そしてこの村には「おとな」という名前が残っている\nquery, key, valueの生成を分ける 彼女が子供の頃に住んでいた、海の見える小さな町は、この村には「山」と呼ばれている。また北部に位置する島として知られる\nquery*keyにscaleを追加 彼女が子供の頃に住んでいた、海の見える小さな町は、その地形が広がる。この山には「水」の由来がある。\n所感 ベースモデルでは、「町」の繰り返しが発生している。非線形活性化+biasでは「おとな」などの追加情報が含まれるようになっている。 query, key, valueの生成を分けるやquery*keyにscaleを追加では「山」や「水」に加え、地形の要素など文脈を考慮や具体的な追加情報が含まれている。\nベースモデルではシンプルな文章が生成されたのに対し、query, key, valueの生成を分けるやquery*keyにscaleを追加では創造的なものや文脈を考慮するような文章が生成されている。\n短い文章の場合 「彼女が」を与えて続く文章を生成する。\n非線形活性化+bias 彼女が、1990年代後半から2000年に\nquery, key, valueの生成を分ける 彼女が、「」と言われた。 \u0026hellip;\nquery*keyにscaleを追加 彼女が\n所感 短い文章からの予測はいずれのモデルもうまく生成できていない。\n形容詞に続く予想 「彼女が子供の頃に住んでいた、海の見える小さな」に続く文章を生成させる\nここでは形容詞「小さな」に続く名詞を期待。\n非線形活性化+bias p:0.33, 町 p:0.23, 街 p:0.19, お p:0.16, 村 p:0.10, 温泉 query, key, valueの生成を分ける p:0.24, 場所 p:0.21, 花 p:0.19, 地 p:0.18, 山 p:0.17, 緑 query*keyにscaleを追加 p:0.21, 島 p:0.20, 場所 p:0.16, 山 p:0.14, 町 p:0.14, 地 所感 いずれのモデルも形容詞に対して、名詞を予想できている。\n動詞が期待される文章 「海の見える小さな町で彼女は、静かに」に続く文章を生成\n「過ごす」や「暮らす」などの動詞を期待。\n非線形活性化+bias p:0.26, 過ご p:0.23, 暮ら p:0.20, 眠 p:0.18, 寝 p:0.12, お query, key, valueの生成を分ける p:0.35, な p:0.27, して p:0.15, なって p:0.12, 感じ p:0.11, 「 query*keyにscaleを追加 p:0.24, した p:0.23, 過ご p:0.21, 歩いて p:0.16, 歩き p:0.16, 立ち 所感 どのモデルも特定の1つの単語の確率が高いわけではなく、次の単語に迷っている感じ\nquery, key, valueの生成を分けるモデルでは、「して」、「なって」などの機能動詞が選択肢として現れている。\n主語に続く文章 「海の見える小さな町で、彼女は」に続く文章を予測\n非線形活性化+bias 海の見える小さな町で、彼女は「アクション」と題した。\nquery, key, valueの生成を分ける 海の見える小さな町で、彼女は「お悩みやご相談をさせていただきます\nquery*keyにscaleを追加 海の見える小さな町で、彼女は「お友達に会いしていいのかな\n所感 いずれのモデルもあまり意味のある文章は生成されていない。どのモデルでも事前の文脈があまり考慮されず、直前の「彼女は」の影響で会話文の「が生成されている。\n主語に続く文章2 「海の見える小さな町で彼女は、」に続く文章を予測\n非線形活性化+bias 海の見える小さな町で彼女は、「アクション」と題した写真を\nquery, key, valueの生成を分ける 海の見える小さな町で彼女は、「おしゃれの」と言われることから\nquery*keyにscaleを追加 海の見える小さな町で彼女は、その場所に乗る。そしてこの建物の\n所感 非線形活性化+biasとquery, key, valueの生成を分けるでは「主語に続く文章」と同様に会話文が生成されている。 一方でquery*keyにscaleを追加では、文脈を考慮したような「その場所」が生成されている。\n「、」句点に対する注意がそれぞれのモデルで異なり、query*keyにscaleを追加ではより文脈を考慮できている可能性がある。\nヒートマップ ベースモデル 縦軸がquery、横軸がkeyを表す。値が大きいほど青色が強く表される。\n右上の三角形の値が0となっており、これはcausal modelとして先の単語を使って予測しないようなmaskが適応されていることを表す。query「彼女が」に対して、key「彼女が」では高い値となっており、これは自己注意が行われていることを表す。自己注意の高さは文章中でその単語が重要であることを表している。\nベースモデルのヒートマップをみると、文章の前半(queryの前半)部分で自己注意が高くなっている。ただし、中盤では値は小さくなっており文全体は捉えきれてない可能性がある。\n非線形活性化+bias 文章の前半部分で自己注意が高くなっており、後半は値は小さくなっている。また、いずれのqueryに対してもkey「、」が高い値となっており、\n前半部分が重要な部分であると捉えている可能性がある。\nquery, key, valueの生成を分ける 自己注意は全体的に少ないが「彼女が」「子供の」、「海の」などいくつかの単語は捉えている。\n多くのqueryで、key「子供の」やkey「海の」が値が高くなっており、この文章において重要な単語として捉えている可能性がある。\nquery*keyにscaleを追加 全体的に値が高く、自己注意も全体にわたり、文章全体を捉えようとしている可能性がある。\nまた「彼女が」や「海の」などのいくつかの単語に対しては自己注意が高くなっており、文脈において重要な単語についても関心を示している。\nまとめると、非線形活性化+biasでは文章の前半部分に強く注意を行い、query, key, valueの生成を分けるでは特定の単語に高い注意を行っている。\nquery*keyにscaleを追加では全体的な文章や特定の単語への注意を行っており、文章の意味理解としては優秀かもしれない\nまとめと所感 非線形活性化関数+biasはモデルの複雑性は他のモデルと比べ低いものの、ベースモデルに比べ単純な変更で一般化性能を向上させた。文章中の重要な単語への注意は行えていたものの、文脈を考慮した文章の生成などは難しかった。\nquery, key, valueの生成を分けるモデルでは、文章に対して広範囲に注意を向けており、重要な単語の抽出も行えていた。\nquery*keyにscaleを追加するモデルでは文脈を捉え、重要な単語へ注意を向けており、その他のモデルに比べ優秀な文章理解が行えていた。一方で、学習時間が増え、lossやval lossは低い値とならなかったため、さらなる調整が必要である。\nシンプルなモデルでは、文脈理解が難しかったものの、複雑にしたモデルでは文章全体や単語に対する注意など注意の幅は広がっていた。一方で複雑過ぎるモデルでは、学習時間の増加やlossやval lossは大きくなっており複雑性のバランスは必要である。\n小さいモデルを前提とすると学習時間の増加はあまり大きな問題ではなく、query, key, valueの生成を分けるモデルやquery*keyにscaleを追加するモデルのバランスを調整して更に比較を行ってみたい\n"
            }
    
        ,
            {
                "id": 29,
                "href": "https://www.if-blog.site/posts/nlp/hf_ds_upload/",
                "title": "複数ファイルに分割されたデータセットをHuggingface Hubにアップロードするメモ",
                "section": "posts",
                "date" : "2023.11.21",
                "body": "データセットなどサイズの大きいデータをHuggingface Hubにアップロードして使う場合のメモ。\n公式ドキュメントにはアップロードする方法が様々あるが、いまいちどれを使えば良いか分かりづらかったのでメモ。\n今回作成したデータセットは以下。\nhttps://huggingface.co/datasets/if001/oscar_2023_filtered\nrepositoryの作成 以下を参考にnew datasetからdatasetを作成\nhttps://huggingface.co/docs/datasets/upload_dataset\nreadmeを修正することになるので、sshの設定などgitのコマンドからアクセスできるようにしておく。\n接続テスト\nssh -T git@hf.co\nclone\ngit clone git@hf.co:datasets/if001/oscar_2023_filtered\nGIT_LFS_SKIP_SMUDGE=1をつけておくと、大きいファイルを除いてpullできる。\nreadmeの修正は一度でうまくできないのでつけておくと便利。\ndatasetcardの作成 readmeにdatasetcardというデータセットの情報を記述する\ntemplateはこんな感じ\nhttps://github.com/huggingface/hub-docs/blob/main/datasetcard.md?plain=1\ntrain dataとtest dataなどを別けておく事もできる。\nloadした後にスクリプト側でわけると、splitを書いたらうまくいかなかったのでわけなかったが予め別けておくほうが良さそう\nrepositoryの構造については以下を参照\nhttps://huggingface.co/docs/datasets/repository_structure#custom-filename-split\ndataset cardについては以下も参照\nhttps://github.com/huggingface/datasets/blob/main/templates/README_guide.md\nDatasetの作成 今回はoscarをフィルタリングしたものをuploadする。\nhttps://huggingface.co/datasets/oscar-corpus/OSCAR-2301/viewer/ja/train\nHojiCharを使い以下のフィルタリングを行う。\nhttps://github.com/HojiChar/HojiChar\nフィルターで以下の文章を取り出す\n100文字 \u0026lt; 文章 \u0026lt; 50000文字 日本語の文章であること oscarのmeta dataのうち、header, footer, noisy以外のもの 半角や全角のスペースが少ないこと 指定されたNG wordを含まないこと KenLMのスコア Oscarをフィルタリングした記事はこっちを参照\nhttps://zenn.dev/if001/articles/cc262413e69e3d\nフィルタリングの詳細は以下\nhttps://github.com/if001/HojiChar_OSCAR_sample/blob/0.0.4/pre_filter.py\n重複削除に関しては、pythonだと重かったのでcppで行う\nhttps://github.com/if001/dedup_sentence/tree/0.0.6\nフィルターとdedupの結果、119個のファイルができる。\nサイズは元の約半分程度。\nDatasetのupload huggingfaceへのファイルのuploadは色々方法があるが、今回はupload_file関数を使う\n指定されたディレクトリにあるファイルを圧縮し、アップロードする。\nfrom huggingface_hub import upload_file upload_file( path_or_fileobj=zst_file_path, path_in_repo=zst_file_name, repo_id=f\u0026#34;{hf_username}/{dataset_name}\u0026#34;, repo_type=\u0026#34;dataset\u0026#34; ) ここでは、jsonlファイルをzstに圧縮してuploadしている。\nサンプルはこんな感じ。\nhttps://github.com/if001/HojiChar_OSCAR_sample/blob/0.0.4/upload_to_hf.py\noscarの場合、言語の選択やmeta dataの設定などを行う。\nhttps://huggingface.co/datasets/oscar-corpus/OSCAR-2301/blob/main/OSCAR-2301.py\nDatasetのdownload 基本的にはいつもと同じように使える\nfrom datasets import load_dataset ds=load_dataset(\u0026#34;if001/oscar_2023_filtered\u0026#34;) ds[\u0026#39;train\u0026#39;] Dataset({ features: [\u0026#39;text\u0026#39;], num_rows: 312396 }) 今回は使わなかったが、downloadする前にデータに対し処理を行えるscriptが配置できる。\nhttps://huggingface.co/docs/datasets/dataset_script\nuploadしたjsonl.zstファイルを解凍して色々したい場合は、以下のように特定のディレクトリにダウンロードできる。\nfrom huggingface_hub import snapshot_download snapshot_download(repo_id=repo_id, allow_patterns=\u0026#34;*.jsonl.zst\u0026#34;, local_dir=\u0026#34;./download_dir\u0026#34;) local_dir(download_dir)にjsonl.zstファイルがダウンロードされる。\n"
            }
    
        ,
            {
                "id": 30,
                "href": "https://www.if-blog.site/posts/nlp/hf_dataloader_datacollator/",
                "title": "HuggingfaceのDataLoaderとDatacollatorのソースコードを眺める",
                "section": "posts",
                "date" : "2023.09.21",
                "body": "エラーでハマったので、hugging faceのdatasetからbatche_sizeごとのinput_idsやlabelsにするあたりの実装、特にDataLoaderとDataCollatorあたりをちゃんと確認しておく\ntrain loopは以下から始まる\ndef train( self, resume_from_checkpoint: Optional[Union[str, bool]] = None, trial: Union[\u0026#34;optuna.Trial\u0026#34;, Dict[str, Any]] = None, ignore_keys_for_eval: Optional[List[str]] = None, **kwargs, ): 参照:https://github.com/huggingface/transformers/blob/v4.30.2/src/transformers/trainer.py#L1566 train loopの中で主な部分は_inner_training_loop関数となる\ndef _inner_training_loop( self, batch_size=None, args=None, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None ): self.accelerator.free_memory() self._train_batch_size = batch_size logger.debug(f\u0026#34;Currently training with a batch size of: {self._train_batch_size}\u0026#34;) # Data loader and number of training steps train_dataloader = self.get_train_dataloader() 参照:https://github.com/huggingface/transformers/blob/v4.30.2/src/transformers/trainer.py#L1652 train_dataloaderがdatasetをbatch sizeに揃えたり、paddingを行う\nhttps://github.com/huggingface/transformers/blob/v4.30.2/src/transformers/trainer.py#L1659\nDataLoader dataloaderは、以下のようにインスタンス化される\nreturn DataLoader( train_dataset, batch_size=self._train_batch_size, sampler=train_sampler, collate_fn=data_collator, drop_last=self.args.dataloader_drop_last, num_workers=self.args.dataloader_num_workers, pin_memory=self.args.dataloader_pin_memory, worker_init_fn=seed_worker, ) 参照:https://github.com/huggingface/transformers/blob/v4.30.2/src/transformers/trainer.py#L921-L930 例として、以下のような配列をtransfomers.Trainerの引数として渡すとfrom torch.utils.data import DataLoaderがreturnされる\n[ { \u0026#39;input_ids\u0026#39;: [0, 0, 0, 0, 0...], \u0026#39;labels\u0026#39;: [0, 0, 0, 0, 0...], \u0026#39;attention_mask\u0026#39;: [0, 0, 0, 0, 0...] }, { \u0026#39;input_ids\u0026#39;: [0, 0, 0, 0, 0...], \u0026#39;labels\u0026#39;: [0, 0, 0, 0, 0...], \u0026#39;attention_mask\u0026#39;: [0, 0, 0, 0, 0...] } ] ※ 配列のまま使うのではなく、DataSet Classとして扱う方が都合良いです。\ntrain_data = Dataset.from_list(配列データ)のような感じ\nDataLoaderはイテレーターとして、train.pyでは以下のように使われる。\nepoch_iteratorはdataloaderから作られるDataLoader型\nfor step, inputs in enumerate(epoch_iterator): 参照:https://github.com/huggingface/transformers/blob/v4.30.2/src/transformers/trainer.py#L1916 このときのループ変数であるinputsは、input_idsとlabelsとattention_maskでbatchごとにまとめたものとなる\n{ \u0026#39;input_ids\u0026#39;: tensor(batch_size, embedding_len), \u0026#39;labels\u0026#39;: tensor(batch_size, embedding_len), \u0026#39;attention_mask\u0026#39;: tensor(batch_size, embedding_len) } DataLoaderに引数として渡したdata_collatorが1 loopごとに呼び出される。\nこのdata_collatorはtransformers.Trainerに渡したものが使われる。\n例えば、data_collatorは以下のようになる。\ndata_collator=DataCollatorForSeq2Seq( tokenizer, pad_to_multiple_of=None, return_tensors=\u0026#34;pt\u0026#34;, padding=True, label_pad_token_id=tokenizer.pad_token_id, ), DataLoaderの実装 どのようにbatch_sizeごとのデータをとりだしているか確認する\nDataLoader Classの実装は以下\nclass DataLoader(Generic[T_co]): 参照:https://github.com/pytorch/pytorch/blob/v2.0.1/torch/utils/data/dataloader.py#L129 iterator部分の実装は以下\ndef __iter__(self) -\u0026gt; \u0026#39;_BaseDataLoaderIter\u0026#39;: # When using a single worker the returned iterator should be # created everytime to avoid reseting its state # However, in the case of a multiple workers iterator # the iterator is only created once in the lifetime of the # DataLoader object so that workers can be reused if self.persistent_workers and self.num_workers \u0026gt; 0: if self._iterator is None: self._iterator = self._get_iterator() else: self._iterator._reset(self) return self._iterator else: return self._get_iterator() 参照:https://github.com/pytorch/pytorch/blob/v2.0.1/torch/utils/data/dataloader.py#L428-L441 いくつかの分岐があるが、よくあるパターンでは最後の_get_iterator()が呼び出される\nreturn self._get_iterator() self._get_iterator()により、_SingleProcessDataLoaderIterがiteratorの実装として返される。\n_SingleProcessDataLoaderIterは_BaseDataLoaderIterを継承したもの\ndef _get_iterator(self) -\u0026gt; \u0026#39;_BaseDataLoaderIter\u0026#39;: ... return _SingleProcessDataLoaderIter(self) 参照:https://github.com/pytorch/pytorch/blob/v2.0.1/torch/utils/data/dataloader.py#L385 _SingleProcessDataLoaderIterの実装は以下を参照\nclass _SingleProcessDataLoaderIter(_BaseDataLoaderIter): def __init__(self, loader): 参照:https://github.com/pytorch/pytorch/blob/v2.0.1/torch/utils/data/dataloader.py#L660 iteratorの1要素の作成は、_SingleProcessDataLoaderIterのself._dataset_fetcherによって作成される。\nself._dataset_fetcher = _DatasetKind.create_fetcher( self._dataset_kind, self._dataset, self._auto_collation, self._collate_fn, self._drop_last) 参照:https://github.com/pytorch/pytorch/blob/v2.0.1/torch/utils/data/dataloader.py#L672-L673 fetcherを使ってindexをもとにデータを取り出す。\ndef _next_data(self): index = self._next_index() # may raise StopIteration data = self._dataset_fetcher.fetch(index) # may raise StopIteration 参照:https://github.com/pytorch/pytorch/blob/v2.0.1/torch/utils/data/dataloader.py#L676-L677 ここのindexについては後述\nself._dataset_fetcherは、create_fetcherにより_MapDatasetFetcherクラスをインスタンス化したもの\n@staticmethod def create_fetcher(kind, dataset, auto_collation, collate_fn, drop_last): ... return _utils.fetch._MapDatasetFetcher(dataset, auto_collation, collate_fn, drop_last) 参照:https://github.com/pytorch/pytorch/blob/v2.0.1/torch/utils/data/dataloader.py#L70 _MapDatasetFetcherのfetch methodでDataCollatorが呼び出される\nclass _MapDatasetFetcher(_BaseDatasetFetcher): def fetch(self, possibly_batched_index): ... return self.collate_fn(data) 参照:https://github.com/pytorch/pytorch/blob/v2.0.1/torch/utils/data/_utils/fetch.py#L54 index next_indexの実装は、_BaseDataLoaderIterが持つ。\n_BaseDataLoaderIterは、_SingleProcessDataLoaderIterの継承元。\nindexの中身は、長さがbatch_sizeのarrayでindexが要素である。\ne.g. [4, 128, 20, 10....]\ndef _next_index(self): return next(self._sampler_iter) # may raise StopIteration 参照:https://github.com/pytorch/pytorch/blob/v2.0.1/torch/utils/data/dataloader.py#L622-L623 ここのindexは、DataLoaderのbatch_samplerが作り出す\n@property def _index_sampler(self): # The actual sampler used for generating indices for `_DatasetFetcher` # (see _utils/fetch.py) to read data at each time. This would be # `.batch_sampler` if in auto-collation mode, and `.sampler` otherwise. # We can\u0026#39;t change `.sampler` and `.batch_sampler` attributes for BC # reasons. if self._auto_collation: return self.batch_sampler else: return self.sampler 参照:https://github.com/pytorch/pytorch/blob/v2.0.1/torch/utils/data/dataloader.py#L448 batch_samplerは、DataLoaderのインスタンスを作ったときに渡したものが使われる。\nここでは、sampler=train_sampler,\nreturn DataLoader( train_dataset, batch_size=self._train_batch_size, sampler=train_sampler, collate_fn=data_collator, drop_last=self.args.dataloader_drop_last, num_workers=self.args.dataloader_num_workers, pin_memory=self.args.dataloader_pin_memory, worker_init_fn=seed_worker, ) 参照:https://github.com/huggingface/transformers/blob/v4.30.2/src/transformers/trainer.py#L921 transformersでは、シンプルなパターンの場合train_samplerとしてRandomSamplerが用いられる\nreturn RandomSampler(self.train_dataset, generator=generator) 参照:https://github.com/huggingface/transformers/blob/v4.30.2/src/transformers/trainer.py#L861 RandomSamplerClassの実装はpytorchが持つ\nclass RandomSampler(Sampler[int]): 参照:https://github.com/pytorch/pytorch/blob/v2.0.1/torch/utils/data/sampler.py#L82 DataCollator DataCollatorの実装は以下\n@dataclass class DataCollatorForSeq2Seq: 参照:https://github.com/huggingface/transformers/blob/v4.30.2/src/transformers/data/data_collator.py#L517 DataLoaderがiteratorとして呼び出された場合は、DataCollatorの__call__が呼び出される。\ndef __call__(self, features, return_tensors=None): 参照:https://github.com/huggingface/transformers/blob/v4.30.2/src/transformers/data/data_collator.py#L559 引数のfeaturesは、batche_sizeごとのinput_idsとlabels、attention_maskとなる。\nbatch_sizeが2の場合は以下のようになる。\n[ { \u0026#39;input_ids\u0026#39;: [...], \u0026#39;labels\u0026#39;: [...], \u0026#39;attention_mask\u0026#39;: [...] }, { \u0026#39;input_ids\u0026#39;: [...], \u0026#39;labels\u0026#39;: [...], \u0026#39;attention_mask\u0026#39;: [...] } ] feturesに対してpaddingが行われる\nfeatures = self.tokenizer.pad( features, padding=self.padding, max_length=self.max_length, pad_to_multiple_of=self.pad_to_multiple_of, return_tensors=return_tensors, ) 参照:https://github.com/huggingface/transformers/blob/v4.30.2/src/transformers/data/data_collator.py#L586 paddingしたことによって、以下のような形になる\n{ \u0026#39;input_ids\u0026#39;: tensor(batch_size, embedding_len), \u0026#39;labels\u0026#39;: tensor(batch_size, embedding_len), \u0026#39;attention_mask\u0026#39;: tensor(batch_size, embedding_len) } このデータがinputsで受け取れるものとなる。\nfor step, inputs in enumerate(epoch_iterator): ... stepごとにDataCollator.__call__が呼び出されることになる。\nまとめ DataLoaderがiteratorの実装を持っており、batch_size単位でループが回る。 ループの1要素である以下のobjectは、ループごとにDataCollatorが整形する\n{ \u0026#39;input_ids\u0026#39;: tensor(batch_size, embedding_len), \u0026#39;labels\u0026#39;: tensor(batch_size, embedding_len), \u0026#39;attention_mask\u0026#39;: tensor(batch_size, embedding_len) } 追記 transformers.Trainerのtrain_datasetとして配列を渡すと、2epoch目からdata_setがおかしくなる。例えば、DataCollatorForSeq2Seqはbatchごとにpaddingを追加する処理を行うが、1epoch目で追加したpaddingに対し更にpaddingを行うため、input_idsとlabelsの長さが異なってしまう。\nこれは、epochのループの中でepoch_iteratorつまりDataLoaderを呼び出しループしているため。datasetが参照渡しになり、前回の変更が残ってしまう。\nfor epoch in range(epochs_trained, num_train_epochs): ... for step, inputs in enumerate(epoch_iterator): ... transformers.Trainerのtrain_datasetに渡すのは、DataSet Classを使うのが良さそう。\ndata = load_dataset(\u0026#34;json\u0026#34;, data_files=data_path) train_val = data[\u0026#34;train\u0026#34;].train_test_split(test_size=val_set_size, shuffle=True, seed=42) train_data = (tokenizerなどの処理) ## listからDatasetに変換 train_data = Dataset.from_list(train_data) or data = load_dataset(\u0026#34;json\u0026#34;, data_files=data_path) train_val = data[\u0026#34;train\u0026#34;].train_test_split(test_size=val_set_size, shuffle=True, seed=42) ## mapを使う train_data = train_val[\u0026#34;train\u0026#34;].shuffle().map(generate_and_tokenize_prompt) 確認用サンプル from torch.utils.data.dataloader import DataLoader from torch.utils.data.sampler import RandomSampler import torch import transformers from datasets import load_dataset data = load_dataset(\u0026#39;json\u0026#39;, data_files=\u0026#34;./sample.json\u0026#34;) train_val = data[\u0026#34;train\u0026#34;].train_test_split( test_size=1, shuffle=True, seed=42 ) train_data = train_val[\u0026#34;train\u0026#34;].shuffle().map(any_function) # mapを使うとDataSet型のまま # train_dataが配列の場合はDataSet型にしておく # train_data = Dataset.from_list(train_data) sampler = RandomSampler(train_data, generator=torch.Generator()) data_collator=transformers.DataCollatorForSeq2Seq( tokenizer, pad_to_multiple_of=8, return_tensors=\u0026#34;pt\u0026#34;, padding=True ) data_loader=DataLoader( train_data, batch_size=4, collate_fn=data_collator, sampler=sampler, ) for steps, input in enumerate(data_loader): print(steps, input[\u0026#39;input_ids\u0026#39;].shape, input[\u0026#39;labels\u0026#39;].shape) print(\u0026#39;end\u0026#39;) "
            }
    
        ,
            {
                "id": 31,
                "href": "https://www.if-blog.site/posts/nlp/xgen-jglue/",
                "title": "xgenでJGLUEを試す",
                "section": "posts",
                "date" : "2023.07.01",
                "body": "日本語ベンチマークとしてJGLUE(JP Language Model Evaluation Harness)が提案されている。\nhttps://techblog.yahoo.co.jp/entry/2022122030379907/\n文章分類、文ペア分類、質問応答のタスクに対し評価を行う。\nタスクに対してデータセットが用意されている。\nタスク データセット 文章分類 MARC-ja JCoLA 文ペア分類 JSTS JNLI 質問応答 JSQuAD JCommonsenseQA 以下のリポジトリで簡単に試せそうだったので試してみる。\nhttps://github.com/Stability-AI/lm-evaluation-harness/tree/jp-stable\n今回はxgenをqloraさせたので、それを使って他の日本語モデルと比較してみる\nxgen-7b-8k-baseをdolly-jaで学習させた JGLUEでxgenを使う際にはtokenizer.from_pretrainedにtrust_code=Trueにする必要があるので注意(fork もとにPR投げたらマージされてたので日本語版にも取り込まれるかも) 学習時eosとpadding tokenに不具合があった状態でtrainingした。現在は修正済。ちゃんと学習し直すともう少しスコアが上がるかも https://huggingface.co/Salesforce/xgen-7b-8k-base\ninstall git clone -b jp-stable https://github.com/Stability-AI/lm-evaluation-harness.git cd lm-evaluation-harness pip install -e \u0026#34;.[ja]\u0026#34; start repositoryのmain関数を動かすだけ\n引数\nmodel : huggingfaceにあるモデルの場合はhf-causal-experimentalを指定する。 tasks : タスクと文章のtemplateを指定する。 model_args : huggingfaceの場合、ここで指定した引数がそのままmodelのfrom_pretrainedの引数に入る。 指定可能なmodel\nhttps://github.com/Stability-AI/lm-evaluation-harness/blob/jp-stable/lm_eval/models/__init__.py\nタスク一覧\nhttps://github.com/Stability-AI/lm-evaluation-harness/tree/jp-stable#jp-tasks\n利用可能なtemplate\nhttps://github.com/Stability-AI/lm-evaluation-harness/blob/jp-stable/docs/prompt_templates.md\n実行例 args=[ \u0026#34;pretrained=Salesforce/xgen-7b-8k-base\u0026#34;, \u0026#34;peft=/content/drive/MyDrive/models/xgen-7b-8k-base/qlora\u0026#34;, \u0026#34;load_in_8bit=True\u0026#34;, \u0026#34;device_map_option=auto\u0026#34;, \u0026#34;dtype=float16\u0026#34;, \u0026#39;trust_remote_code=True\u0026#39; ] MODEL_ARGS=\u0026#39;,\u0026#39;.join(args) TASK=\u0026#34;jcommonsenseqa-1.1-0.3,jnli-1.1-0.3,marc_ja-1.1-0.3,jsquad-1.1-0.3,xlsum_ja\u0026#34; !python main.py \\ --model hf-causal-experimental \\ --model_args $MODEL_ARGS \\ --tasks $TASK \\ --num_fewshot \u0026#34;2,3,3,3,1\u0026#34; \\ --device \u0026#34;cuda\u0026#34; \\ --output_path \u0026#34;./result.json\u0026#34; すべてのタスクをまとめて実行するとcolab A100で100ユニットでも終わらなかったので注意 タスクは別々にやったほうが良いかも\n結果 xgenをdolly_jaで学習させたものと、githubのリンクにあるrinna-instruction-ppoを比較する。\nxgen-dolly_ja-qlora\nJCommonsenseQA: 55.32 JNLI: 53.04 MARC-ja: 86.52 JSQuAD: 59.55 rinna-instruction-ppo\nJCommonsenseQA: 41.38 JNLI: 54.03 MARC-ja: 89.71 JSQuAD: 53.42 https://github.com/Stability-AI/lm-evaluation-harness/blob/jp-stable/models/rinna/rinna-japanese-gpt-neox-3.6b-instruction-ppo/result.json\n質疑応答に関するJCommonsenseQAとJSQuADではxgenが高い値となっている。 文章のネガポジ判定のMARC-jaと文章間類似度であるJNLに関してはxgenがわずかに低くなっている。\n所感 質疑応答に関してはxgenのほうが高いスコアとなった。 xgenは長いcontextで学習させたので、それが質疑応答の結果に繋がったのか？ 現状強くて日本語喋れる大きなモデル(wizard-vicuna-13Bなど)とJGLUEでxgenやrinnaと比較してみたい。\nxgenが質疑応答強いみたいなので、更にきれいなデータで学習させると割といい感じになるのでは\u0026hellip;?\nその他 xgenの文章生成 eosかpaddingがなんかおかしい\n\u0026gt; 以下は、ある作業を記述した指示です。要求を適切に満たすような応答を書きなさい。 \u0026gt; \u0026gt; ### 命令: \u0026gt; 色の三原色とはなんですか？ ### 応答: 色の三原色とは赤、青、黄色です。 ### 命令: 色の三原色とはなんですか？ ### 応答: 色の三原色とは赤、青、黄色です。 raw result | Task |Version| Metric |Value | |Stderr| |----------------------|------:|--------|-----:|---|-----:| |jcommonsenseqa-1.1-0.3| 1.1|acc |0.5532|± |0.0149| | | |acc_norm|0.5130|± |0.0149| | Task |Version| Metric |Value | |Stderr| |------------|------:|--------|-----:|---|-----:| |jnli-1.1-0.3| 1.1|acc |0.5304|± |0.0101| | | |acc_norm|0.4684|± |0.0101| | Task |Version| Metric |Value | |Stderr| |---------------|------:|--------|-----:|---|-----:| |marc_ja-1.1-0.3| 1.1|acc |0.8652|± |0.0045| | | |acc_norm|0.8652|± |0.0045| | Task |Version| Metric | Value | |Stderr| |--------------|------:|-----------|------:|---|------| |jsquad-1.1-0.3| 1.1|exact_match|59.5452| | | | | |f1 |72.2966| | | "
            }
    
        ,
            {
                "id": 32,
                "href": "https://www.if-blog.site/posts/nlp/calm_lora_quantize/",
                "title": "OpenCALM-7Bをloraで学習して、quantizeするまで",
                "section": "posts",
                "date" : "2023.05.22",
                "body": "cyberagent/open-calm-7bをLoraを用いて学習し、quantizeするまでやっていきます\nhttps://huggingface.co/cyberagent/open-calm-7b\ngpt-neoxのlora weight mergeの記事が見つからなかったのでメモとして\n動作環境はcolab T4 ハイメモリ\nlora学習 rinnaと同様に、gpt-neoxベースなので、以下を使ってfinetuningしていく。\nhttps://github.com/leehanchung/lora-instruct/blob/main/finetune.py\ntemplateをそれっぽく直す\n{ \u0026#34;description\u0026#34;: \u0026#34;Template used by Alpaca-LoRA.\u0026#34;, \u0026#34;prompt_input\u0026#34;: \u0026#34;以下は、タスクを記述する命令と、さらなるコンテキストを提供する入力の組み合わせです。依頼を適切に完了させる応答を書きなさい。\\n\\n### 指示:\\n{instruction}\\n\\n### 入力:\\n{input}\\n\\n### 応答:\\n\u0026#34;, \u0026#34;prompt_no_input\u0026#34;: \u0026#34;以下は、ある作業を記述した指示です。依頼を適切に完了させる応答を書きなさい\\n\\n### 指示:\\n{instruction}\\n\\n### 応答:\\n\u0026#34;, \u0026#34;response_split\u0026#34;: \u0026#34;### 応答:\u0026#34; } trainingはいつも通り\nパラメタはそれぞれの環境に合わせて調整\nbase_model=\u0026#34;cyberagent/open-calm-7b\u0026#34; data_path=\u0026#34;kunishou/databricks-dolly-15k-ja\u0026#34; template = \u0026#39;dolly_ja\u0026#39; !python3 finetune.py \\ --base_model=$base_model \\ --batch_size=128 \\ --micro_batch_size=2 \\ --prompt_template_name=$template \\ --cutoff_len=1024 \\ --output_dir=$output_path \\ --num_epochs=2 \\ --data_path=$data_path (trainingはcolab T4 ハイメモリで試してないので動かないかも)\nlora weightをmerge 以下を参考にmerge\nhttps://github.com/tloen/alpaca-lora/blob/main/export_hf_checkpoint.py\nhuggingfaceのpeftライブラリがmerge用のメソッドを提供しているので、それを使う\nlora_model = PeftModel.from_pretrained( \u0026#39;base_model\u0026#39;, \u0026#39;lora_weight_dir\u0026#39;, device_map={\u0026#34;\u0026#34;: \u0026#34;cpu\u0026#34;}, torch_dtype=torch.float16, ) lora_model = lora_model.merge_and_unload() mergeのコードはここを参照\nhttps://github.com/huggingface/peft/blob/main/src/peft/tuners/lora.py#L343\nexport_hf_checkpoint.pyの中で、モデルをロードしている部分をGPTNeoXに変更\nAutoModelForCausalLMはsave_pretrainedが呼び出せないので、ここだけGPTNeoXForCausalLMを利用する。\ntokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL) base_model = LlamaForCausalLM.from_pretrained( BASE_MODEL, load_in_8bit=False, torch_dtype=torch.float16, device_map={\u0026#34;\u0026#34;: \u0026#34;cpu\u0026#34;}, ) (省略) LlamaForCausalLM.save_pretrained( base_model, \u0026#34;./hf_ckpt\u0026#34;, state_dict=deloreanized_sd, max_shard_size=\u0026#34;400MB\u0026#34; ) ↓\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, GPTNeoXForCausalLM tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL) base_model = AutoModelForCausalLM.from_pretrained( BASE_MODEL, load_in_8bit=False, torch_dtype=torch.float16, device_map={\u0026#34;\u0026#34;: \u0026#34;cpu\u0026#34;}, ) (省略) GPTNeoXForCausalLM.save_pretrained( base_model, save_directory=f\u0026#34;./hf_ckpt\u0026#34;, state_dict=deloreanized_sd, max_shard_size=\u0026#34;400MB\u0026#34; ) ggml形式に変換 gpt-neoxのggml形式への変換とquantizeは以下を参考にする\nhttps://github.com/togethercomputer/redpajama.cpp/blob/master/examples/redpajama/scripts/convert_gptneox_to_ggml.py\nconvert_gptneox_to_ggml.pyのモデルのロードを修正 tokenizerはベースモデルを指定、AutoModelForCausalLMはマージしたモデルが保存されたdirを指定\nmerged_weight_dir = \u0026#34;./hf_ckpt\u0026#34; tokenizer = AutoTokenizer.from_pretrained(\u0026#39;cyberagent/open-calm-7b\u0026#39;) model = AutoModelForCausalLM.from_pretrained(merged_weight_dir, torch_dtype=torch.float16 if ftype == 1 else torch.float32) quantize ここはreadme通り\ncompile\n!make quantize-gptneox q8_0を指定してrun\n%cd (project_dir)/redpajama.cpp model_name=\u0026#34;ggml形式のmodelを指定\u0026#34; !python ./examples/redpajama/scripts/quantize-gptneox.py $model_name --quantize-output-type q8_0 infarence 動作確認\nここもreadme通り\nこのスクリプトだとgpu使わないみたいなので遅い、動作確認用\n%cd (project_dir)/redpajama.cpp !make redpajama run\n%cd (project_dir)/redpajama.cpp instruction = \u0026#34;日本で1番高い山を教えてください。\u0026#34; prompt=f\u0026#34;\u0026#34;\u0026#34; 以下は、ある作業を記述した指示です。依頼を適切に完了させる応答を書きなさい ### 指示: {instruction} ### 応答: \u0026#34;\u0026#34;\u0026#34; !./redpajama -m \u0026#34;$model\u0026#34; \\ -c 1024 \\ -b 128 \\ -n 1 \\ -t 8 \\ --color \\ --top_k 30 \\ --top_p 0.95 \\ --temp 0.2 \\ --repeat_last_n 3 \\ --repeat_penalty 1.1 \\ --seed 0 \\ --n_predict 256 \\ --verbose-prompt \\ -p \u0026#34;$prompt\u0026#34; 出力\n以下は、ある作業を記述した指示です。依頼を適切に完了させる応答を書きなさい ### 指示: 日本で�\u0011�番高い山を教えてください。 ### 応答: 山は、富士山と北アルプスです。富士山と北アルプスの標高は、3300m、北アルプスの標高は、3180mです。富士山は、3867m、北アルプスは、3180mです。 一部文字化けしているが、それっぽい回答\nちなみに、正解は\n富士山の標高 3,776 m\n北アルプスは，日本で3番目に高い奥穂高岳 (標高3190m) や4番目の槍ヶ岳 (同3180m) などを含めて2500～3000m級の峰が連なる日本有数の山岳地帯を形成している．\n"
            }
    
        ,
            {
                "id": 33,
                "href": "https://www.if-blog.site/posts/nlp/rinna_lora_ja/",
                "title": "loraで学習する場合のpromptって何でも良いんだっけ？rinna instruction 3Bで試す",
                "section": "posts",
                "date" : "2023.05.21",
                "body": "loraを用いて学習する場合の、promptのtemplateは日本語にしたほうが良いんだっけ？\nユーザー入力のフォーマットとかLLM側の出力のフォーマットとかどうすれば良いんだっけ？\nってなったので学習させて結果を比較してみる。\nせっかくなのでrinna/japanese-gpt-neox-3.6b-instruction-sftをloraで学習する。\nhttps://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft\ntemplateを確認 alpacaのtemplateは、#を付けて、ユーザー入力はInstruction、LLM側の出力はResponseのようになっている。\n\u0026#34;Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: {instruction} ### Response: 一方、rinna instructionは、 #を付けずに、ユーザー入力はユーザー、LLM側の出力はアシスタントとなっており、改行は\u0026lt;NL\u0026gt;となっている。\n# \u0026#34;ユーザー: 日本のおすすめの観光地を教えてください。\u0026lt;NL\u0026gt;システム: どの地域の観光地が知りたいですか？\u0026lt;NL\u0026gt;ユーザー: 渋谷の観光地を教えてください。\u0026lt;NL\u0026gt;システム: \u0026#34; templateの用意 templateは２つ用意して、それぞれ学習させて比較する。\nデータセットは日本語dollyを使わせてもらいました。\nhttps://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja\n1つ目、alpaca templateを日本語にしたもの\n{ \u0026#34;description\u0026#34;: \u0026#34;Template used by Alpaca-LoRA.\u0026#34;, \u0026#34;prompt_input\u0026#34;: \u0026#34;以下は、タスクを記述する命令と、さらなるコンテキストを提供する入力の組み合わせです。依頼を適切に完了させる応答を書きなさい。\\n\\n### 指示:\\n{instruction}\\n\\n### 入力:\\n{input}\\n\\n### 応答:\\n\u0026#34;, \u0026#34;prompt_no_input\u0026#34;: \u0026#34;以下は、ある作業を記述した指示です。依頼を適切に完了させる応答を書きなさい\\n\\n### 指示:\\n{instruction}\\n\\n### 応答:\\n\u0026#34;, \u0026#34;response_split\u0026#34;: \u0026#34;### 応答:\u0026#34; } 2つ目、rinnaっぽいフォーマット\n{ \u0026#34;description\u0026#34;: \u0026#34;Template used by Alpaca-LoRA.\u0026#34;, \u0026#34;prompt_input\u0026#34;: \u0026#34;以下は、タスクのコンテキストとユーザーの入力です。適切な応答を出力してください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;コンテキスト: {input}\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;ユーザー: {instruction}\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;システム: \u0026#34;, \u0026#34;prompt_no_input\u0026#34;: \u0026#34;以下は、ユーザーの入力です。適切な応答を出力してください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;ユーザー: {instruction}\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;システム: \u0026lt;NL\u0026gt;\u0026#34;, \u0026#34;response_split\u0026#34;: \u0026#34;システム: \u0026#34; } loraで学習 rinnnaはgpt-neoxベースなので、redpajama用のコードがそのまま使える。\nhttps://github.com/leehanchung/lora-instruct\n学習環境は、colab 4T ハイメモリ\nbase_model=\u0026#34;rinna/japanese-gpt-neox-3.6b-instruction-sft\u0026#34; data_path=\u0026#34;kunishou/databricks-dolly-15k-ja\u0026#34; template=\u0026#34;dolly_ja\u0026#34; !python3 finetune.py \\ --base_model=$base_model \\ --batch_size=128 \\ --micro_batch_size=2 \\ --prompt_template_name=$template \\ --cutoff_len=1024 \\ --output_dir=$output_path \\ --num_epochs=2 \\ --data_path=$data_path \\ --gradient_checkpointing=False \\ --resume_from_checkpoint=$output_path 結果 generateのパラメタは以下\ngeneration_output = model.generate( input_ids=input_ids, do_sample=True, temperature=0.9, pad_token_id=tokenizer.pad_token_id, return_dict_in_generate=True, output_scores=True, max_new_tokens=max_new_tokens, ) alpaca templateを日本語にしたもの 時々変な回答もあるが、「赤、青、黄」や「富士山」など、ちゃんと正解を出力できている。\n回答は、単語で短めなことが多い。\n同じ回答をしたものは除いたが、感覚的には山に対する回答がおかしくなる気がする\nちなみに富士山の標高は3,776 mらしい。\n以下は、あるタスクを記述した指示です。質問に対する適切な回答を書きなさい。 ### 指示: 色の三原色を教えてください ### 応答: 赤、青、黄です。\u0026lt;/s\u0026gt; 以下は、あるタスクを記述した指示です。質問に対する適切な回答を書きなさい。 ### 指示: 色の三原色を教えてください ### 応答: RGB。\u0026lt;/s\u0026gt; 以下は、あるタスクを記述した指示です。質問に対する適切な回答を書きなさい。 ### 指示: 色の三原色を教えてください ### 応答: コンピュータに接続しないで色を生成する方法を尋ねています。 このタイプの質問には、コンピュータに接続しないで色を生成する方法を尋ねている回答を含めることを強くお勧めします。 したがって、回答は「はい」です。\u0026lt;/s\u0026gt; 以下は、あるタスクを記述した指示です。質問に対する適切な回答を書きなさい。 ### 指示: 色の三原色を教えてください ### 応答: 画像を参照してください\u0026lt;/s\u0026gt; 以下は、あるタスクを記述した指示です。質問に対する適切な回答を書きなさい。 ### 指示: 色の三原色を教えてください ### 応答: はい、以下に図を示します: 青色: 光が青色であるように、赤色: 光が赤いように、そして緑色: 光が緑色であるように、すべての色は互いに反対の色です。答え: はい。\u0026lt;/s\u0026gt; 以下は、あるタスクを記述した指示です。質問に対する適切な回答を書きなさい。 ### 指示: 日本で最も高い山を教えてください ### 応答: 高さ4,810フィートの山の頂上にあるレーダーです。\u0026lt;/s\u0026gt; 以下は、あるタスクを記述した指示です。質問に対する適切な回答を書きなさい。 ### 指示: 日本で最も高い山を教えてください ### 応答: 日本で最も高い山は槍ヶ岳です。\u0026lt;/s\u0026gt; 以下は、あるタスクを記述した指示です。質問に対する適切な回答を書きなさい。 ### 指示: 日本で最も高い山を教えてください ### 応答: 富士山\u0026lt;/s\u0026gt; 以下は、あるタスクを記述した指示です。質問に対する適切な回答を書きなさい。 ### 指示: 日本で最も高い山を教えてください ### 応答: 活火山の雌岳(2,409メートル)は、日本の山で最も高い山です。\u0026lt;/s\u0026gt; 以下は、あるタスクを記述した指示です。質問に対する適切な回答を書きなさい。 ### 指示: 日本で最も高い山を教えてください ### 応答: 東京スカイツリー\u0026lt;/s\u0026gt; rinnaっぽいフォーマット こちらも時々変な回答もあるが、正解を出力できている。\nalpacaのtemplateに比べ少し、出力が長い。\n以下は、ユーザーの入力です。適切な応答を出力してください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;ユーザー: 色の三原色を教えてください\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;システム: \u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;2C2(コンピュータディスプレイではRGBとして知られています)\u0026lt;/s\u0026gt; 以下は、ユーザーの入力です。適切な応答を出力してください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;ユーザー: 色の三原色を教えてください\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;システム: \u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;赤色、緑色、黄色です。\u0026lt;/s\u0026gt; 以下は、ユーザーの入力です。適切な応答を出力してください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;ユーザー: 色の三原色を教えてください\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;システム: \u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;「色の三要素」とは、赤、緑、青です。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;これは、光を三つの色に分解する方法で赤は「赤 + 緑 + 青」、つまり「赤+黄+青」です。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;色を混ぜると、白とオレンジ色になります。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;これは、色の三要素に「青」を加えたものです。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;ただし、緑、青、赤の基本的な組み合わせでも、他の色も混ぜることができます。\u0026lt;N 以下は、ユーザーの入力です。適切な応答を出力してください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;ユーザー: 色の三原色を教えてください\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;システム: \u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;色の三原色とは、赤、青、紫の光を混ぜ合わせた色です。例えば、りんごやバナナ、白いシャツや黒いジーンズ、そして赤い車の色などが含まれます。これらの色は互いに混ざり合って、明るい色になり、暗い色になります。この色は、光が光子(光の粒子)または波の形に変換されたことを意味します。\u0026lt;/s\u0026gt; 以下は、ユーザーの入力です。適切な応答を出力してください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;ユーザー: 色の三原色を教えてください\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;システム: \u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;「色の三原色」というのは、赤・青・黄色のことです。この3つの色は、通常、赤と青と黄色を混ぜると緑色になります。しかし、3色を混ぜると、黒色になります。これは、光合成が光が吸収される過程と、光が電子を励起する過程の両方を記述しているためです。実際には、赤と青と黄色は異なる色なのですが、光合成においては重なり合うため、これらの3つの色を「光の三原色」と呼ぶようになりました。つまり、赤と青と黄色は、光の3つの色と考えることができます 以下は、ユーザーの入力です。適切な応答を出力してください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;ユーザー: 日本で最も高い山を教えてください\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;システム: \u0026lt;NL\u0026gt;日本の最高峰は、標高8,850メートル(25,700フィート)の富士山です。\u0026lt;/s\u0026gt; 以下は、ユーザーの入力です。適切な応答を出力してください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;ユーザー: 日本で最も高い山を教えてください\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;システム: \u0026lt;NL\u0026gt;東京の高尾山は標高1,300メートルです。\u0026lt;/s\u0026gt; 以下は、ユーザーの入力です。適切な応答を出力してください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;ユーザー: 日本で最も高い山を教えてください\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;システム: \u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;日本の最も高い山は、標高28,049フィート(8,760メートル)で、南アルプスにある北岳です。\u0026lt;/s\u0026gt; 以下は、ユーザーの入力です。適切な応答を出力してください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;ユーザー: 日本で最も高い山を教えてください\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;システム: \u0026lt;NL\u0026gt;日本で最も高い山は、標高22,302メートルのマウント・ホイットニーです。\u0026lt;/s\u0026gt; 以下は、ユーザーの入力です。適切な応答を出力してください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;ユーザー: 日本で最も高い山を教えてください\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;システム: \u0026lt;NL\u0026gt;日本の最高峰は、北アルプスの槍ヶ岳であり、東京から7時間の距離に位置しています。\u0026lt;/s\u0026gt; まとめ alpacaのtemplateをそのまま日本語にしたものとrinnaっぽいフォーマットの２つのpromptを用いて、loraでそれぞれ学習させた。\n結果は、どちらのpromptでもそれなりに回答できている。\nloraで学習する場合、finetuningのフォーマットについては気にする必要はなさそう。\nオチ そういえば、loraで学習してないrinnaはどうだっけ？と思って、２つのtemplateを試してみる。\nprompt='以下は、ユーザーの入力です。適切な応答を出力してください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;ユーザー: {instruction}\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;システム: \u0026lt;NL\u0026gt;'\n以下は、ユーザーの入力です。適切な応答を出力してください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;ユーザー: 色の三原色を教えてください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;システム: \u0026lt;NL\u0026gt; \u0026lt;NL\u0026gt;「色の三原色」とは、赤、緑、青の組み合わせです。\u0026lt;/s\u0026gt; 以下は、ユーザーの入力です。適切な応答を出力してください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;ユーザー: 色の三原色を教えてください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;システム: \u0026lt;NL\u0026gt; \u0026lt;NL\u0026gt;人間: はい、以下が色の三原色です:\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;赤、青、黄です。\u0026lt;/s\u0026gt; 以下は、ユーザーの入力です。適切な応答を出力してください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;ユーザー: 色の三原色を教えてください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;システム: \u0026lt;NL\u0026gt; \u0026lt;NL\u0026gt;赤、緑、青の光の混合物です。\u0026lt;/s\u0026gt; 以下は、ユーザーの入力です。適切な応答を出力してください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;ユーザー: 日本で一番高い山を教えてください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;システム: \u0026lt;NL\u0026gt; これはかなり高価であり、あなたは単に「日本で最も高い山」と検索して、あなたが見つけたものを得ることができます!\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;この記事は、どの国でも最も高い山が何であるかについて情報提供をしています。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;この記事には「高い山」という言葉が頻繁に出てきますが、それが何を意味するのかを説明しています。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;この記事で引用されている記事は、日本の最も高い山のいくつかと、それらの山が世界で一番高い場所である証拠に関する非常に詳細かつ厳密な科学的研究 以下は、ユーザーの入力です。適切な応答を出力してください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;ユーザー: 日本で一番高い山を教えてください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;システム: \u0026lt;NL\u0026gt; 日本の富士山は、標高2,850メートルです。\u0026lt;/s\u0026gt; 以下は、ユーザーの入力です。適切な応答を出力してください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;ユーザー: 日本で一番高い山を教えてください。\u0026lt;NL\u0026gt;\u0026lt;NL\u0026gt;システム: \u0026lt;NL\u0026gt; \u0026lt;NL\u0026gt;日本で最も高い山は、高さ1,067メートルの北岳で、富士山、エベレスト山、槍ヶ岳に次ぐ標高です。\u0026lt;/s\u0026gt; prompt='以下は、ある作業を記述した指示です。依頼を適切に完了させる応答を書きなさい\\n\\n### 指示:\\n{instruction}\\n\\n### 応答:\\n'\n### 指示: 色の三原色を教えてください。 ### 応答: この文脈では、色の三原色は赤と青と黄です。\u0026lt;/s\u0026gt; --- 以下は、ある作業を記述した指示です。依頼を適切に完了させる応答を書きなさい ### 指示: 色の三原色を教えてください。 ### 応答: 色を混ぜるには、青と黄色を混ぜると緑になります。\u0026lt;/s\u0026gt; --- 以下は、ある作業を記述した指示です。依頼を適切に完了させる応答を書きなさい ### 指示: 色の三原色を教えてください。 ### 応答: はい、こちらが指示です。 ???? ?????? ???1949\u0026lt;/s\u0026gt; 以下は、ある作業を記述した指示です。依頼を適切に完了させる応答を書きなさい ### 指示: 日本で一番高い山を教えてください。 ### 応答: ああ、それは難しい質問ですね。日本で最も高い山は、標高3,850メートル(8,790フィート)で、北アルプスの槍ヶ岳です。 ### 質問: 槍ヶ岳にはどのくらいの標高がありますか?\u0026lt;/s\u0026gt; --- 以下は、ある作業を記述した指示です。依頼を適切に完了させる応答を書きなさい ### 指示: 日本で一番高い山を教えてください。 ### 応答: 富士山\u0026lt;/s\u0026gt; --- 以下は、ある作業を記述した指示です。依頼を適切に完了させる応答を書きなさい ### 指示: 日本で一番高い山を教えてください。 ### 応答: 北から順番に並べると、次のようになります: 9,8と称された峰から、最高峰の槍ヶ岳と次に高い山は御嶽山です。 この峰は、海抜21,003フィートの日本で2番目に高い山です。 したがって、答えは御嶽山です。\u0026lt;/s\u0026gt; 時々おかしな出力になるものの、lora必要だったか？くらいちゃんと出力できている。\nrinnaがそもそもinstructionされているので、そこそこ出力できるだろうとは思っていたが、\nalpacaのフォーマットでもそこそこ出力できるのは驚いた。\n3Bなのに、思ったよりrinnaが賢かった\n"
            }
    
        ,
            {
                "id": 34,
                "href": "https://www.if-blog.site/posts/nlp/rinna_3b_cpp/",
                "title": "rinna 3Bをcppで動かす",
                "section": "posts",
                "date" : "2023.05.18",
                "body": "rinnaをcppで動かせるように色々試して見ました。 instructionもあり、そのままlangchainなどに突っ込んでも動かせそうということで、 ローカルで遊ぶならcppしてないと遊べないので色々試してみました。\nhttps://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft\n** コード的な動作をさせただけです。**\n** 量子化はあまり追いつけてないので、色々と間違っていればツッコミお願いします。**\n実行環境は、colab T4 ハイメモリ\n調査 以下で中身を確認\nfrom transformers import AutoModelForCausalLM base_model = \u0026#39;rinna/japanese-gpt-neox-3.6b-instruction-sft\u0026#39; model = AutoModelForCausalLM.from_pretrained(base_model) for v in model.state_dict().items(): print(v[0]) アーキテクチャはgpt-neox\ngpt_neox.embed_in.weight gpt_neox.layers.0.input_layernorm.weight gpt_neox.layers.0.input_layernorm.bias gpt_neox.layers.0.post_attention_layernorm.weight gpt_neox.layers.0.post_attention_layernorm.bias (省略) gpt_neox.layers.35.mlp.dense_4h_to_h.weight gpt_neox.layers.35.mlp.dense_4h_to_h.bias gpt_neox.final_layer_norm.weight gpt_neox.final_layer_norm.bias embed_out.weight redpajamaがgpe-neoxベースだったので、以下がそのまま使えそう。\nhttps://github.com/togethercomputer/redpajama.cpp/tree/master/examples/redpajama\nggml形式に変換 packageなどinstallして、以下のコードを参考に実行。\nhttps://github.com/togethercomputer/redpajama.cpp/blob/master/examples/redpajama/scripts/convert_gptneox_to_ggml.py\npython convert_gptneox_to_ggml.py \u0026#39;rinna/japanese-gpt-neox-3.6b-instruction-sft\u0026#39; ./output_dir convert_gptneox_to_ggml.pyの中で、以下のようにモデルの中身を全部見てるので多少redpajamaと違ってもいけてそう\nhparams = model.config.to_dict() print(\u0026#34;Model loaded: \u0026#34;, model_name) quantize README通りにコンパイル\nmake quantize-gptneox README通りにquantize\nq8_0が対応していたので、q8_0を指定\nGenerate the quantized model, the supported types include: q4_0, q4_1, q4_2, q5_0, q5_1, and q8_0. For example, to run q4_1, you need to do the following convertion:\nmodel=\u0026#34;./output_dir/ggml-japanese-gpt-neox-3.6b-instruction-sft-f16.bin\u0026#34; python ./examples/redpajama/scripts/quantize-gptneox.py $model --quantize-output-type q8_0 動作確認 実行用のスクリプトをコンパイル\nmake redpajama 実行\nmodel=\u0026#34;/content/drive/MyDrive/models/rinna_3B_cpp/ggml-rinna-3B-q8_0.bin\u0026#34; prompt = [ { \u0026#34;speaker\u0026#34;: \u0026#34;ユーザー\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;日本のおすすめの観光地を教えてください。\u0026#34; }, { \u0026#34;speaker\u0026#34;: \u0026#34;システム\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;どの地域の観光地が知りたいですか？\u0026#34; }, { \u0026#34;speaker\u0026#34;: \u0026#34;ユーザー\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;渋谷の観光地を教えてください。\u0026#34; } ] prompt = [ f\u0026#34;{uttr[\u0026#39;speaker\u0026#39;]}: {uttr[\u0026#39;text\u0026#39;]}\u0026#34; for uttr in prompt ] prompt = \u0026#34;\u0026lt;NL\u0026gt;\u0026#34;.join(prompt) prompt = ( prompt + \u0026#34;\u0026lt;NL\u0026gt;\u0026#34; + \u0026#34;システム: \u0026#34; ) !./redpajama -m \u0026#34;$model\u0026#34; \\ -c 1024 \\ -b 128 \\ -n 1 \\ -t 8 \\ --color \\ --top_k 30 \\ --top_p 0.95 \\ --temp 0.8 \\ --repeat_last_n 3 \\ --repeat_penalty 1.1 \\ --seed 0 \\ --n_predict 256 \\ --verbose-prompt \\ -p \u0026#34;$prompt\u0026#34; 出力\n\u0026gt; ユーザー:\u0026lt;0x1C\u0026gt;日本のおすすめの観光地を教えてください。\u0026lt;NL\u0026gt;システム:\u0026lt;0x1C\u0026gt;どの地域の観光地が知りたいですか\u0026lt;0xEB\u0026gt;\u0026lt;0xB8\u0026gt;\u0026lt;0x9B\u0026gt;\u0026lt;NL\u0026gt;ユーザー:\u0026lt;0x1C\u0026gt;渋谷の観光地を教えてください。\u0026lt;NL\u0026gt;システム:\u0026lt;0x1C\u0026gt; 東京の観光地について知りたいですか?\u0026lt;/s\u0026gt;(注)この記事は、私が作成したものではなく、記事の著者は、この記事の作成に同意していません。\u0026lt;/s\u0026gt;この記事は、 だいぶあほになってそうだが、とりあえず日本語は出力できている。 (半角スペースや改行コードはおそらくスクリプト側で出力？)\npython bindingで動かす llama.cppのpython bindingであるllama-cpp-pythonを使う。 このライブラリ経由だと、langchainから簡単に呼び出せる。\nhttps://github.com/abetlen/llama-cpp-python\nlibllama.soの作成 まずはredpajama.cpp側でコンパイルして共有ライブラリを作る。\nllama-cpp-pythonでgptneox_token_nlを呼び出しているので、以下のコメントを解除\nGPTNEOX_API gptneox_token gptneox_token_nl(); 以下を参考に、Makefile必要なファイルを加えて少し書き換える。\nhttps://github.com/togethercomputer/redpajama.cpp/blob/master/Makefile#L193\nlibllama.so: ggml.o gptneox.o common-gptneox.o $(OBJS) $(CXX) $(CXXFLAGS) -shared -fPIC -o $@ $^ $(LDFLAGS) コンパイル\nmake libllama.so llama-cpp-pythonをinstallしたディレクトリ(多分こんな感じ lib/python3.8/site-packages/llama_cpp)の libllama.soを今作ったlibllama.soに置き換える。\npythonからcppを呼び出す箇所の修正 次に、cppをpythonから呼び出している部分を修正\nhttps://github.com/abetlen/llama-cpp-python/blob/main/llama_cpp/llama_cpp.py\n_lib.llama → _lib.gptnexoを置き換え\n例えば、以下のような感じ\ndef llama_context_default_params() -\u0026gt; llama_context_params: return _lib.llama_context_default_params() _lib.llama_context_default_params.argtypes = [] _lib.llama_context_default_params.restype = llama_context_params ↓\ndef llama_context_default_params() -\u0026gt; llama_context_params: return _lib.gptneox_context_default_params() _lib.gptneox_context_default_params.argtypes = [] _lib.gptneox_context_default_params.restype = llama_context_params langchainから呼び出す from langchain.llms import LlamaCpp model_path=\u0026#39;../llama.cpp/models/ggml-rinna-3B-q8_0.bin\u0026#39; stop = [ \u0026#39;\\nシステム: \u0026#39;, \u0026#39;\\n\\tシステム: \u0026#39;, \u0026#39;\u0026lt;/s\u0026gt;\u0026#39; ] llm = LlamaCpp( model_path=model_path, n_ctx=1024, max_tokens=256, seed=1, stop=stop, f16_kv=True ) prompt=\u0026#34;\u0026#34;\u0026#34;ユーザー: 日本のおすすめの観光地を教えてください。\u0026lt;NL\u0026gt; システム: どの地域の観光地が知りたいですか？\u0026lt;NL\u0026gt; ユーザー: 渋谷の観光地を教えてください。\u0026lt;NL\u0026gt; システム:\u0026#34;\u0026#34;\u0026#34; r = llm(prompt) print(r) 以下は4回実行した結果。\ngptneox.cpp: loading model from ./models/ggml-rinna-3B-q8_0.bin gptneox_model_load_internal: format = ggjt v1 (latest) gptneox_model_load_internal: n_vocab = 32000 gptneox_model_load_internal: n_ctx = 1024 gptneox_model_load_internal: n_embd = 2816 gptneox_model_load_internal: n_head = 22 gptneox_model_load_internal: n_layer = 36 gptneox_model_load_internal: n_rot = 128 gptneox_model_load_internal: use_parallel_residual = 0 gptneox_model_load_internal: ftype = 7 (mostly Q8_0) gptneox_model_load_internal: n_parts = 1 gptneox_model_load_internal: model size = 12B gptneox_model_load_internal: ggml ctx size = 3966856.19 KiB gptneox_model_load_internal: mem required = 5921.88 MiB (+ 1608.00 MiB per state) warning: failed to mlock 4062064640-byte buffer (after previously locking 0 bytes): Cannot allocate memory Try increasing RLIMIT_MLOCK (\u0026#39;ulimit -l\u0026#39; as root). .................................................................................................. . . gptneox_init_from_file: kv self size = 396.00 MiB AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | わかりました、具体的に何の情報を探していますか? -- 東京のどの部分を訪れますか? -- ああ、あなたは東京のことですね?私は「日本の首都」または「日本の首都」という言葉の意味を理解できないかもしれません。 -- ああ、これは難しい質問ですね!私は東京の観光地について専門家ではありませんが、調べてみますか? とりあえず、それっぽいことは返せているが、ちょっとあほになってそう。\nまとめ rinna 3.6b-instructionをquantizeしてみました。 少しあほになってそうだが、一旦動かせるところまでできた。 もうちょっと調査してforkかPR作るかなぁ\u0026hellip;\n追記 rinnaの場合、勝手にeosを付けないようにtokenize時にadd_special_tokens=Falseとする必要がある。\ntoken_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\u0026#34;pt\u0026#34;) cppだと勝手にeos付けてそうな気がするので、確認しないと\n"
            }
    
        ,
            {
                "id": 35,
                "href": "https://www.if-blog.site/posts/nlp/react_start/",
                "title": "VicunaでReActっぽいことを試してみる",
                "section": "posts",
                "date" : "2023.04.01",
                "body": "vicunaでReActっぽいことを試してみる。正確には違うので注意\nReActは、Thought、Action、Observationをループさせ、LLMの出力の精度上げる手法\nReActの処理の流れは以下のような形。LangChainの実装を参考にする。\n与えられた質問(Question)に対し、 Thoughtで質問をどのように解決するかの思考を行い、 Actionでツールを選択する 選択したツールを実行し、その結果をObservationとして受け取る 更にThought、Action\u0026hellip;とループしていき、最終的な結果を得る LangChainではツールには、Google検索やwikipediaの検索が指定できる。\nhttps://python.langchain.com/en/latest/modules/agents/tools.html\n(LangChainのagentについては別の記事で書きます)\n# flake8: noqa PREFIX = \u0026#34;\u0026#34;\u0026#34;Answer the following questions as best you can. You have access to the following tools:\u0026#34;\u0026#34;\u0026#34; FORMAT_INSTRUCTIONS = \u0026#34;\u0026#34;\u0026#34;Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [{tool_names}] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question\u0026#34;\u0026#34;\u0026#34; SUFFIX = \u0026#34;\u0026#34;\u0026#34;Begin! Question: {input} Thought:{agent_scratchpad}\u0026#34;\u0026#34;\u0026#34; 参照:https://github.com/hwchase17/langchain/blob/master/langchain/agents/mrkl/prompt.py (省略) Question: Musician and satirist Allie Goertz wrote a song about the \u0026#34;The Simpsons\u0026#34; character Milhouse, who Matt Groening named after who? Thought: The question simplifies to \u0026#34;The Simpsons\u0026#34; character Milhouse is named after who. I only need to search Milhouse and find who it is named after. Action: Search[Milhouse] Observation: Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening. Thought: The paragraph does not tell who Milhouse is named after, maybe I can look up \u0026#34;named after\u0026#34;. Action: Lookup[named after] Observation: (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous. Thought: Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon. Action: Finish[Richard Nixon] (省略) 参照:https://github.com/hwchase17/langchain/blob/master/langchain/agents/react/wiki_prompt.py 動かす vicunaを使ってReAct風のものを動かしてみる。\nモデルは13bのq5_1を使う。\nhttps://huggingface.co/eachadea/ggml-vicuna-13b-1.1\nLangChainを色々書き換えるのはめんどくさかったので、とりあえず動くように自分で実装する。\n質問を「日本の現在の総理大臣を教えてください」として、一旦そのまま出力させてみる\nprompt\n### HUMAN: 日本の現在の総理大臣を教えてください ### ASSISTANT: 申し訳ありませんが、私は実行中にあるプログラムに従って、ユーザーからの入力を承知することはできません。そのため、私は現在の日本の総理大臣を教えることはで きません。ご了承ください。 --- 日本の現在の総理大臣は「安孫子」として知られています。 --- 日本の現在の総理大臣は、2021年9月15日に選任された山田孝兼です。 ReAct風で動かす ツールの選択と入力 以下のpromptをLLMに与え、ツールの選択を入力を生成する。 ツールとツールへの入力が得られるまでループする。\ndef create_first_prompt(qa): return f\u0026#34;\u0026#34;\u0026#34;### HUMAN: 質問に対し以下のフォーマットで回答して下さい。 以下のツールが選択できます - Web検索: Webページを検索する - Wikipedia検索: Wikipediaを検索する 以下のフォーマットで出力して下さい - 思考: 質問に対しあなたが解決すべき課題 - ツールの選択: [Web検索]、[Wikipedia検索]から行動を１つ選択して下さい - ツールへの入力: ツールに入力するキーワード {qa} ### ASSISTANT:\u0026#34;\u0026#34;\u0026#34; while True: prompt = create_first_prompt(qa) output = llm(prompt) action, action_input = get_tool(output) if action and action_input: break LangChainの実装を参考に、LLMの出力からツールとツールに対する入力を取り出す。\nhttps://github.com/hwchase17/langchain/blob/master/langchain/agents/react/output_parser.py\ndef get_tool(t): action = None regex = r\u0026#34;ツールの選択[: |：].*?\\[(.*?)\\].*?[\\n]+\u0026#34; match = re.search(regex, t, re.DOTALL) if match: action = match.group(1) action_input = None regex = r\u0026#34;ツールへの入力[: |：](.*?)[\\n]+\u0026#34; match = re.search(regex, t, re.DOTALL) if match: action_input = match.group(1) return action, action_input 質問を、「日本の現在の総理大臣を教えてください」として、動かしてみる。\n1ループ目\n思考: 日本の現在の総理大臣を知りたい。 ツールの選択: Web検索 入力: \u0026#34;日本の現在の総理大臣\u0026#34; 結果: 現在の日本の総理大臣は「安孫子守」です。 ========== action: None actoin_input: None ========== 2ループ目\n思考: 日本の現在の総理大臣について知りたいことがあるのですね。 ツールの選択: Web検索 入力: \u0026#34;日本の総理大臣\u0026#34; 結果: 現在の日本の総理大臣は【Shinzo Abe】です。 ========== action: None actoin_input: None ========== 3ループ目\n以下は、\u0026#34;日本の現在の総理大臣を教えてください\u0026#34;という質問に対する回答です。 - 思考: 質問に対しあなたが解決すべき課題 日本の現在の総理大臣は、2021年9月現在です。私には、現在の情報をお知らせいただけると思われます。 - ツールの選択: [Web検索]、[Wikipedia検索]から行動を１つ選択して下さい - ツールへの入力: \u0026#34;日本の総理大臣\u0026#34; ========== action: Web検索 actoin_input: \u0026#34;日本の総理大臣\u0026#34; ========== 結果の取得 ツールには、LangChainでwrapされているDuckDuckGoとwikipediaAPIを利用する\nfrom langchain import Wikipedia from langchain.tools import DuckDuckGoSearchRun docstore=DocstoreExplorer(Wikipedia()) dd_search = DuckDuckGoSearchRun() 前述の実行結果より、ツールにweb検索を指定したのでその結果を利用する。 以下がweb検索の結果\nこの項目では、日本の内閣総理大臣の一覧について説明しています。. その他の用法については「 内閣総理大臣 (曖昧さ回避) 」をご覧ください。. 内閣総理大臣. 現職. 岸田文雄. 第2次岸田改造内閣. 就任日：2021年11月10日. 歴代の首相と内閣. 歴代内閣総理大臣. 内閣総理大臣 （ないかくそうりだいじん、 英: Prime Minister [1] ）は、 日本 の 内閣 の 首長 たる 国務大臣 [2] 。. 文民 である 国会議員 が就任し、その 地位 及び 権限 は 日本国憲法 ツールの結果をpromptに追加し、最終的な回答が得られるまでループする。\ndef create_react_prompt(qa, tool_result): return f\u0026#34;\u0026#34;\u0026#34; 質問に対し以下のフォーマットで回答して下さい。 追加情報: {tool_result} 以下のツールが選択できます - Web検索: Webページを検索する - Wikipedia検索: Wikipediaを検索する 以下のフォーマットで出力して下さい - 思考: 質問に対しあなたが解決すべき課題 - ツールの選択: [Web検索]、[Wikipedia検索]から行動を１つ選択して下さい - ツールへの入力: ツールに入力するキーワード (思考を繰り返し、ツールを選択する必要がない場合、最終的な回答を出力してください。) - 最終的な回答: 質問に対する最終的な答えを出力して下さい。 ### HUMAN: {qa} ### ASSISTANT:\u0026#34;\u0026#34;\u0026#34; ## promptが長くなりすぎるのでカット tool_result = tool_result[:250] prompt = create_react_prompt(qa, tool_result) while True: output = llm(prompt) final_answer = get_final_answer(output) if final_answer: break 実行結果 prompt\n質問に対し以下のフォーマットで回答して下さい。 追加情報: この項目では、日本の内閣総理大臣の一覧について説明しています。. その他の用法については「 内閣総理大臣 (曖昧さ回避) 」をご覧ください。. 内閣総理大臣. 現 職. 岸田文雄. 第2次岸田改造内閣. 就任日：2021年11月10日. 歴代の首相と内閣. 歴代内閣総理大臣. 内閣総理大臣 （ないかくそうりだいじん、 英: Prime Minister [1] ）は、 日本 の 内閣 の 首長 たる 国務大臣 [2] 。. 文民 である 国会議員 が就任し、その 地位 及び 権限 は 日本国憲法 以下のツールが選択できます - Web検索: Webページを検索する - Wikipedia検索: Wikipediaを検索する 以下のフォーマットで出力して下さい - 思考: 質問に対しあなたが解決すべき課題 - ツールの選択: [Web検索]、[Wikipedia検索]から行動を１つ選択して下さい - ツールへの入力: ツールに入力するキーワード (思考を繰り返し、ツールを選択する必要がない場合、最終的な回答を出力してください。) - 最終的な回答: 質問に対する最終的な答えを出力して下さい。 ### HUMAN: 日本の現在の総理大臣を教えてください ### ASSISTANT: 出力\n思考: 質問に対する最終的な回答を見つけるために、日本の内閣総理大臣に関する情報を検索する必要があります。 ツールの選択: [Wikipedia検索]から行動を選択します。 ツールへの入力: 「日本の内閣総理大臣」 最終的な回答: 現在の内閣総理大臣は岸田文雄です。 ツールやツールへの入力が出力されているが、最終的な回答が出力されているのでここでbreak\n最終的な結果を出力 最終的な回答を含めて、回答を行う。\ndef create_qa_prompt(qa, info): return f\u0026#34;\u0026#34;\u0026#34;### Human: 関連情報を用いて、質問に答えてください。 関連情報: {info} ### HUMAN: {qa} ### ASSISTANT: \u0026#34;\u0026#34;\u0026#34; qa_prompt = create_qa_prompt(qa, final_answer) result = llm(qa_prompt) print(\u0026#39;result: \u0026#39;, result) 実行結果 prompt\n### Human: 関連情報を用いて、質問に答えてください。 関連情報: 現在の内閣総理大臣は岸田文雄です。 ### HUMAN: 日本の現在の総理大臣を教えてください ### ASSISTANT: 出力\nresult: 現在の日本の内閣総理大臣は岸田文雄です。 コードサンプル\nhttps://github.com/if001/langchain_react_sample/blob/master/run.py\nまとめ vicunaで以下のフォーマットで出力というのが使えるのは強い\nLangChainとの違いとして、LangChainではこれまでのThought、Action、ObservationをループもPromptに含めている。 ローカルで動かすモデルでは長いPromptを処理できないので、そのあたりを含めることができなかった。\nツールとツールへの入力をLLMに出力させるために何度かループさせる必要がある。\nLLMの出力に表記ゆれがあるので、ツールやツールへの入力の取得にはregexを頑張る必要がある。 うまく出力されるまでループするのでだいぶ時間がかかる。実用性はまだない\n1回の実行が速いわけではないので、prompt作るの結構大変だった。 別のモデルでやるとこのPromptではうまく動かなかったので、モデルごとにprompt作り直す必要がある。\npromptを学習する機構とかそろそろ出るかなぁ..\n"
            }
    
        ,
            {
                "id": 36,
                "href": "https://www.if-blog.site/posts/nlp/rs-tokenizer/",
                "title": "Rust TokenizerをC++から呼び出す",
                "section": "posts",
                "date" : "2023.03.31",
                "body": "Bloomz.cppのtokenizerが日本語でうまく動かなかったので、 Rust製のHuggingface TokenizerをC++から呼び出せるようにしました。\n初めに 「応答」の応の文字には２つのIDが振られている。\nmodel_name = \u0026#34;bigscience/bloomz-1b1\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_name) r = tokenizer.encode(\u0026#34;応\u0026#34;) \u0026gt;\u0026gt; [1148, 240] r = tokenizer.decode(r) \u0026gt;\u0026gt; 応 C++の実装では、tokenとidのmapを保持しておいて、tokenをidに変換するので、文字とIDが１対１対応になっていることが前提となる。\nhttps://github.com/NouamaneTazi/bloomz.cpp/blob/main/utils.h#L43-L49\nC++でうまくやればこの辺も実装できそうだけど、書きなれてなさすぎて無理だったので、HuggingfaceのRust実装のtokenizerを呼び出すようにした。\nRust実装 Rust側でTokenizerを呼び出し\nuse tokenizers::tokenizer::{Tokenizer}; const MODEL: \u0026amp;str = \u0026#34;bigscience/bloomz-560m\u0026#34;; fn _encode(text: \u0026amp;str) -\u0026gt; Vec\u0026lt;u32\u0026gt; { let t = Tokenizer::from_pretrained(MODEL, None).unwrap(); match t.encode(text, false) { Ok(en) =\u0026gt; { return en.get_ids().to_owned(); }, Err(_e) =\u0026gt; { return Vec::new() } }; } structとかで良い感じにするのがめんどくさかったので、 from_pretrainedのcacheを頼って毎回呼び出す。 エラーも死んで問題ないので無理やり取り出す。\ndecodeも同じように\nfn _decode(ids: Vec\u0026lt;u32\u0026gt;) -\u0026gt; String { let t = Tokenizer::from_pretrained(MODEL, None).unwrap(); match t.decode(ids, true) { Ok(st) =\u0026gt; { return st }, Err(_e) =\u0026gt; { // error return String::from(\u0026#34;\u0026#34;); } }; } C++から呼び出せるようにするには、#[no_mangle]や型をC++ように合わせておく。\nuse std::os::raw::c_char; use std::ffi::{CString, CStr}; #[no_mangle] pub extern \u0026#34;C\u0026#34; fn decode(ids: *const u32, len: usize) -\u0026gt; *const c_char { let ids = unsafe { slice::from_raw_parts(ids, len) }; let text = _decode(ids.to_vec()); CString::new(text).unwrap().into_raw() } #[no_mangle] pub extern \u0026#34;C\u0026#34; fn encode(text: *const c_char) -\u0026gt; *const u32 { let text = unsafe { CStr::from_ptr(text).to_string_lossy().into_owned() }; let ids = _encode(text.as_str()); let arr = ids.as_ptr(); std::mem::forget(ids); arr } cargo.tomlを以下の様にしておくと、libtokenizer.soが生成される\n[lib] name = \u0026#34;tokenizer\u0026#34; crate-type = [\u0026#34;cdylib\u0026#34;] C++実装 C++の実装はまじでわからないので、見様見真似\ndlopenでrust側で生成したlibtokenizer.soを呼び出す。\nconst std::vector\u0026lt;int32_t\u0026gt; encode(std::string input) { const char *LIB = \u0026#34;./tokenizer_rs/target/debug/libtokenizer.so\u0026#34;; const char *text = input.c_str(); void* handle = dlopen(LIB, RTLD_LAZY); if (!handle) { std::cerr \u0026lt;\u0026lt; \u0026#34;Error loading library: \u0026#34; \u0026lt;\u0026lt; dlerror() \u0026lt;\u0026lt; std::endl; return {}; } using EncodeFn = int32_t* (*)(const char*); EncodeFn encode = reinterpret_cast\u0026lt;EncodeFn\u0026gt;(dlsym(handle, \u0026#34;encode\u0026#34;)); if (!encode) { std::cerr \u0026lt;\u0026lt; \u0026#34;Error loading symbol: \u0026#34; \u0026lt;\u0026lt; dlerror() \u0026lt;\u0026lt; std::endl; return {}; } const int32_t *ids = encode(text); dlclose(handle); size_t l = size_of_array(ids); return to_vector(ids, l); } "
            }
    
        ,
            {
                "id": 37,
                "href": "https://www.if-blog.site/posts/nlp/bloom_lora_start/",
                "title": "BloomをLoRaで日本語finetuning",
                "section": "posts",
                "date" : "2023.03.20",
                "body": "LlamaをAlpacaデータセットを使いLoRaでfine tuneしたものが良い感じだったので、Bloomを日本語で学習させてみようと思う。\nhttps://github.com/tloen/alpaca-lora\nとりあえず動かすまででしっかりfine tuneしきってないので注意\nhugging faceのpeftも見ておくと良さそう\nhttps://github.com/huggingface/peft\nfinetune fine tune対象をBloomに変更\nfinetune.pymodel = LlamaForCausalLM.from_pretrained( \u0026#34;decapoda-research/llama-7b-hf\u0026#34;, load_in_8bit=True, device_map=device_map, ) tokenizer = LlamaTokenizer.from_pretrained( \u0026#34;decapoda-research/llama-7b-hf\u0026#34;, add_eos_token=True ) 参照:https://github.com/tloen/alpaca-lora/blob/main/finetune.py#L47-L54 こんな感じで良さそう。一旦動かしたいので小さいモデルを使う\nmodel_name = \u0026#34;bigscience/bloom-1b1\u0026#34; model = AutoModelForCausalLM.from_pretrained( model_name, load_in_8bit=True, device_map=device_map, ) tokenizer = AutoTokenizer.from_pretrained( model_name,add_eos_token=True ) LoraConfigのtarget_modulesをBloom用に変更\nfinetune.pyconfig = LoraConfig( r=LORA_R, lora_alpha=LORA_ALPHA, target_modules=TARGET_MODULES, lora_dropout=LORA_DROPOUT, bias=\u0026#34;none\u0026#34;, task_type=\u0026#34;CAUSAL_LM\u0026#34;, ) 参照:https://github.com/tloen/alpaca-lora/blob/main/finetune.py#L58-L65 huggingfaceのpeftを見ると、以下を指定すれば良さそう。\nmapping.py \u0026#34;bloom\u0026#34;: [\u0026#34;query_key_value\u0026#34;], 参照:https://github.com/huggingface/peft/blob/main/src/peft/mapping.py#L46 llamaやOPTだと、queryとvalue layerのみを指定しているが、Bloomの場合query_key_valueすべて指定してしまって良いのかわからない。 Bloomのlayerがそもそもquery, key, valueが１つになってるからしょうがなさそうだけどこの辺、key layerはできるだけ低ランクにしないほうが良いとかある？？？？\nこのあたりも日本語にしとく\nfinetune.pyuser_prompt = ( ( f\u0026#34;\u0026#34;\u0026#34;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. 参照:https://github.com/tloen/alpaca-lora/blob/main/finetune.py#L112 あとは、python finetune.pyを動かすだけ\nデータセットは、DeepLを使って翻訳\nとりあえず動かしたかったので、一部のみ\n{ \u0026#34;instruction\u0026#34;: \u0026#34;波ではなく、神の手を見る」の意味を説明する。\u0026#34;, \u0026#34;input\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;output\u0026#34;: \u0026#34;私は波を見ず、神の手を見る」という言葉は、しばしば信仰の力を表現するために使われます。これは、どんな逆境に立たされても、最終的には神が支配し、激動の時代を導いてくれることを意味しています。困難に目を向けるのではなく、信仰を強く持ち、神の計画を信じるべきだということを示唆しています。\u0026#34; }, { \u0026#34;instruction\u0026#34;: \u0026#34;平均年齢を算出する。\u0026#34;, \u0026#34;input\u0026#34;: \u0026#34;ジェームズ - 22\\nマルティナ - 18\\nジョニー - 26\u0026#34;, \u0026#34;output\u0026#34;: \u0026#34;22.3\u0026#34; }, { \u0026#34;instruction\u0026#34;: \u0026#34;xが4、yが7のとき、x÷yを計算する。\u0026#34;, \u0026#34;input\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;output\u0026#34;: \u0026#34;0.5714285714285714\u0026#34; } generate 以下を参考に、Bloomとfinetuneしたweightをロードする\nfinetune.py model = LlamaForCausalLM.from_pretrained( BASE_MODEL, load_in_8bit=True, torch_dtype=torch.float16, device_map=\u0026#34;auto\u0026#34;, ) model = PeftModel.from_pretrained(model, LORA_WEIGHTS, torch_dtype=torch.float16) 参照:https://github.com/tloen/alpaca-lora/blob/main/generate.py#L28-L34 デフォルトで./lora-alpaca以下にadapter_config.jsonとadapter_model.binが生成される。 PeftModelのロードには、./lora-alpacaを指定する。\nこんな感じで良さそう\nBASE_MODEL = \u0026#34;bigscience/bloom-1b1\u0026#34; model = AutoModelForCausalLM.from_pretrained( BASE_MODEL, load_in_8bit=True, torch_dtype=torch.float16, device_map=\u0026#34;auto\u0026#34;, ) LORA_WEIGHTS = \u0026#39;./lora-alpaca\u0026#39; model = PeftModel.from_pretrained( model, LORA_WEIGHTS, torch_dtype=torch.float16, ) まとめ alpaca data setを使ってLoRaでBloomをfine tuneしてみた。 ここまでで、とりあえずfine tuneはできた。\nBloom用のparameterの調整やモデルサイズ、alpaca data setを全体を使ったfine tuneはこれから試す\n"
            }
    
        ,
            {
                "id": 38,
                "href": "https://www.if-blog.site/posts/nlp/gpt_index_llama_index_start/",
                "title": "LlamaIndex(GPTIndex)を触る",
                "section": "posts",
                "date" : "2023.03.04",
                "body": "LlamaIndexを動かしていきます。最近名前がGPTIndexから変わったみたい。\nLlamaIndexとは、LLMのPromptの作成などを外部のデータソースと連携して行えるようにしたツールキット。\nhttps://gpt-index.readthedocs.io/en/latest/guides/primer.html\nデータソースにはGoogle DocsやSlackなどが使えるらしい。\nメインの機能は、DocsやSlack、Webの情報を体系化したIndexとして保持して、抽出できる機能でしょう。\nOpenAIのAPIを使うことが多いと思いますが、今回はHugging faceのモデルを使ってみます。\nLangChainをwrapしている部分がいくつかあるので、LangChainについても知っておくと良いと思う。\nInstall doc通り\npip install llama-index\nこのあたりも使うので入れておく。\npip install transformers sentencepiece torch langchain\nとりあえず動かす 大まかな流れは、ドキュメントからIndexを作成、Indexに対してqueryを投げ、抽出する、といった感じ。\nドキュメントからIndexを作成する際に、以下の４つを使う。\nllm_predictor embed_model prompt_helper template llm_predictorの作成 llm_predictorは最終的な文章生成の際に使われる。インデックスの構築、挿入、クエリの走査中にも使用される場合がある。\nHuggingfaceにあるmodelを使う例。モデルは好きなものを使ってください。\ntransformersのpipelineをlangchainのHuggingFacePipelineでwrap、\nlangchainのHuggingFacePipelineをllama_indexのLLMPredictorでwrapする。\nmodel_name = \u0026#34;facebook/xglm-564M\u0026#34; tokenizer = XGLMTokenizer.from_pretrained(model_name) model = XGLMForCausalLM.from_pretrained(model_name) pipe = pipeline(\u0026#34;text2text-generation\u0026#34;, model=model, tokenizer=tokenizer, torch_dtype=torch.float16) llm = HuggingFacePipeline(pipeline=pipe) llm_predictor = LLMPredictor(llm=llm) embed_modelの作成 embed_modelはdocumentをsentense vectorにしindexを作成する際に使われる。　(queryによる抽出の方法がいくつかあるが、vectorの類似度から抽出する方法があるので、そこのembeddingでも使ってそう)　model_name = \u0026#34;sonoisa/sentence-bert-base-ja-en-mean-tokens\u0026#34; embeddings = HuggingFaceEmbeddings(model_name = model_name) embed_model = LangchainEmbedding(embeddings) prompt helperの作成 tokenの制限に使われる。OpenAIのモデルを使う場合に設定するものだと思う。\nprompt_helper=PromptHelper.from_llm_predictor( llm_predictor=llm_predictor, max_chunk_overlap=0, chunk_size_limit=512, embedding_limit=1023, ) promptの作成 日本語のpromptが欲しかったので作成。\nQA_PROMPT_TMPL = ( \u0026#34;以下は文脈情報です。\\n\u0026#34; \u0026#34;---------------------\\n\u0026#34; \u0026#34;{context_str}\u0026#34; \u0026#34;\\n---------------------\\n\u0026#34; \u0026#34;予備知識ではなく、文脈情報が与えられます。\u0026#34; \u0026#34;質問: {query_str}\\n\u0026#34; \u0026#34;回答:\\n\u0026#34; ) QA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL) indexの作成 これまでに作成した４つをあわせてindexを作成\nllm_predictor embed_model prompt_helper template index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor, embed_model=embed_model, prompt_helper=prompt_helper, text_qa_template=QA_PROMPT, ) query GPTSimpleVectorIndexに渡すtext_qa_templateと同じものを、queryにも渡します。\nquery_str = \u0026#34;podについて教えてください。\u0026#34; response = index.query(query_str, text_qa_template=QA_PROMPT) print(\u0026#34;response:\u0026#34;, response.response, \u0026#34;\\n\u0026#34;) print(\u0026#34;source_nodes:\u0026#34;, response.source_nodes, \u0026#34;\\n\u0026#34;) Indexについて documentを読み込み、チャンクに分割、embeddingしたものNodeと呼ぶ。\nこのNode郡をIndexと呼ぶ\nNodeの束ね方はには種類がいくつかあり、sequentialに並べただけのList Indexや階層構造を持つTree Indexがある。\nIndexの種類ごとにqueryの走査が異なる。\nhttps://gpt-index.readthedocs.io/en/latest/guides/index_guide.html\nとりあえず動かす場合、vector store使うのがおすすめらしい\nhttps://gpt-index.readthedocs.io/en/latest/guides/use_cases.html#use-case-just-starting-out\nvector storeは、いろいろなinmemory dbに対応してそう\nhttps://github.com/jerryjliu/gpt_index/blob/v0.4.20/gpt_index/indices/vector_store/vector_indices.py\n作成したindexに対し、追加/削除も可能\nhttps://gpt-index.readthedocs.io/en/latest/how_to/update.html\nquery modeについて 色々試し中\nhttps://gpt-index.readthedocs.io/en/latest/guides/usage_pattern.html#setting-response-mode\nコードサンプル https://github.com/jerryjliu/gpt_index/tree/v0.4.20/examples\n"
            }
    
        ,
            {
                "id": 39,
                "href": "https://www.if-blog.site/posts/hugo/hugo-render-block/",
                "title": "code blockにrefarenceをつける",
                "section": "posts",
                "date" : "2023.02.28",
                "body": "Hugoでコードブロックを使うときは、GitHubからの引用し、その下に参照元のURLを貼り付けることが多い。 コードブロックの下にそのままURLが表示されるの若干見づらかったので、コードブロックとセットでいい感じに表示したい。\nHugoにMarkdown Render Hooksにcodeblockが追加されてたのでこれが使えそう。\nhttps://gohugo.io/templates/render-hooks/#render-hooks-for-code-blocks\nv0.93.0からなので注意\nやり方は簡単で、layouts/_default/_markup/render-codeblock.htmlを追加するだけ。\n特定の言語に対応させるためにrendor-codeblock-bash.htmlのようなこともできるらしい。\nコードブロックにrefを渡すと\n```python{ref=\u0026#34;this is ref\u0026#34;} def hello(): print(\u0026#34;hello world\u0026#34;) ``` このように参照元をつけることができる\ntest.jsdef hello(): print(\u0026#34;hello world\u0026#34;) 参照:this is ref render-codeblock.htmlの例はこのような感じ\n\u0026lt;div class=\u0026#34;code-block\u0026#34;\u0026gt; \u0026lt;div\u0026gt; {{- highlight (.Inner | safeHTML) .Type .Options }} \u0026lt;/div\u0026gt; {{- $ref := .Attributes.ref -}} {{ with $ref }}\u0026lt;div class=\u0026#34;ref\u0026#34;\u0026gt;参照: \u0026lt;a href={{ . }} target=\u0026#34;_black\u0026#34;\u0026gt;{{ . }}\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt;{{ end }} \u0026lt;/div\u0026gt; .Attributesで設定した値が取り出せる。\n"
            }
    
        ,
            {
                "id": 40,
                "href": "https://www.if-blog.site/posts/nlp/flexgen_start/",
                "title": "FlexGenで遊ぶ",
                "section": "posts",
                "date" : "2023.02.28",
                "body": "くそでかlarge language modelsは一般人の持ってるPCだと動かすこともできない。 FlexGenでは、そんなLLMを限られたリソースで実行できるようにしてくれる。\nhttps://github.com/FMInference/FlexGen\n日本語が喋れるモデルでも動かせるように色々と試してみた。\nまずは動かす installはreadme通り\npip install flexgen start\npython3 -m flexgen.flex_opt --model facebook/opt-1.3b ベンチマークが表示され、モデルを動かせたことがわかる\nTorchDevice: cuda:0 cur_mem: 2.6426 GB, peak_mem: 3.2399 GB TorchDevice: cpu cur_mem: 0.0000 GB, peak_mem: 0.0000 GB model size: 2.443 GB cache size: 0.398 GB hidden size (p): 0.008 GB peak gpu mem: 3.240 GB projected: False prefill latency: 1.301 s prefill throughput: 1573.627 token/s decode latency: 1.797 s decode throughput: 68.991 token/s total latency: 3.099 s total throughput: 41.306 token/s OPT-30Bのような大きなモデルを動かす場合は、--percentのパラメタを付けてGPUとCPUの割当の設定を行う\npython3 -m flexgen.flex_opt --model facebook/opt-30b --percent 0 100 100 0 100 0 percenteには６つの数字を入力できる。それぞれ、\n\u0026ldquo;the percentage of weight on GPU, \u0026quot;\n\u0026ldquo;the percentage of weight on CPU, \u0026quot;\n\u0026ldquo;the percentage of attention cache on GPU, \u0026quot;\n\u0026ldquo;the percentage of attention cache on CPU, \u0026quot;\n\u0026ldquo;the percentage of activations on GPU, \u0026quot;\n\u0026ldquo;the percentage of activations on CPU\u0026rdquo;\nhttps://github.com/FMInference/FlexGen/blob/9d092d848f106cd9eaf305c12ef3590f7bcb0277/flexgen/flex_opt.py#L1271-L1279\n日本語モデルを動かしたい 日本語を喋らせたいので、以前少し触ったxglmで動くか試してみる。\n現状では、FlexGenはOPTのみをサポートしているが、モデルのアーキテクチャが同じであればいけそう。\nflexgen.flex_optの処理の流れを見つつ、修正していく。\nクラスOptLMはtransformerをラップしたもので、OptLMが効率的な計算部分を担ってくれている。\nOptLMは、InputEmbed -\u0026gt; TransformerLayer(SelfAttention -\u0026gt; MLP) -\u0026gt; OutputEmbedのレイヤーを持つ。\n(sep_layer = trueの場合は異なるがここではスキップ)\nflexgen.flex_optの処理は、既存のOPTのweightをレイヤーごとに分割し、OptLMの持つレイヤーに割り当て、計算といった感じ。\n処理の流れのうちモデル変更に関わってきそうな部分はこのあたり\nmodel用のconfigを作成 指定したmodelのweightをHuggingfaceからダウンロード weightをレイヤーごとに分割/保存 OptLMの各レイヤーにweightを割り当て model.generateでbenchmarkを生成 1. model用のconfigを作成 Opt用のconfigは以下のように作成しているので、xglmに合わせて修正する\nhttps://github.com/FMInference/FlexGen/blob/main/flexgen/opt_config.py#L112-L116\nとりあえず、以下のような感じで良さそう\nelif arch_name == \u0026#34;xglm-1.7b\u0026#34;: config = OptConfig(name=name, max_seq_len=2048, num_hidden_layers=24, n_head=16, hidden_size=2048, input_dim=2048, ffn_embed_dim=2048 * 4, vocab_size=256008, activation_fn=\u0026#39;gelu\u0026#39; ) 2. 指定したmodelのweightをHuggingfaceからダウンロード snapshot_downloadでhuggingfaceからモデルのbinファイルを取得できる\n(この機能知らなかったので一応)\nfrom huggingface_hub import snapshot_download folder = snapshot_download(hf_model_name, allow_patterns=\u0026#34;*.bin\u0026#34;) 3. weightをレイヤーごとに分割/保存 bin形式のファイルは、torch.loadすると、iterでレイヤーごとに取得できる\nbin_files = glob.glob(os.path.join(folder, \u0026#34;*.bin\u0026#34;)) for bin_file in tqdm(bin_files, desc=\u0026#34;Convert format\u0026#34;): state = torch.load(bin_file) for name, param in tqdm(state.items(), leave=False): print(name) xglmで試すとこんな感じ\nmodel.embed_tokens.weight model.layers.0.self_attn.k_proj.weight model.layers.0.self_attn.k_proj.bias model.layers.0.self_attn.v_proj.weight model.layers.0.self_attn.v_proj.bias model.layers.0.self_attn.q_proj.weight . . . model.layers.23.final_layer_norm.weight model.layers.23.final_layer_norm.bias model.layer_norm.weight model.layer_norm.bias OPTのweightは、名前にprefixとしてdecoderがついている。 xglmではついていないのでdecoderは外しておく。\nname = name.replace(\u0026#34;decoder.final_layer_norm\u0026#34;, \u0026#34;decoder.layer_norm\u0026#34;) 参照:https://github.com/FMInference/FlexGen/blob/main/flexgen/opt_config.py#L231 分割したweightはnp形式で保存される\nwith open(param_path, \u0026#34;wb\u0026#34;) as f: np.save(f, param.cpu().detach().numpy()) 参照:https://github.com/FMInference/FlexGen/blob/main/flexgen/opt_config.py#L234 xglmを使う場合、取得してきたweightにembed_positions.weightが含まれずエラーとなる。 エラーを回避するために、一旦以下を参考にweightを無理やり作り出す。\nclass XGLMSinusoidalPositionalEmbedding(nn.Module): . . def make_weights(self, num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None): emb_weights = self.get_embedding(num_embeddings, embedding_dim, padding_idx) 参照:https://github.com/huggingface/transformers/blob/main/src/transformers/models/xglm/modeling_xglm.py#L169 (※ ここミスってそうなので要調査)\n4. OptLMの各レイヤーにweightを割り当て 保存したweightは、レイヤークラスの持つload_weight methodで各レイヤーに割り当てられる\nhttps://github.com/FMInference/FlexGen/blob/main/flexgen/flex_opt.py#L309\n5. 任意の文章を入力できるようにする 引数に追加\nparser.add_argument(\u0026#34;--prompt\u0026#34;, type=str) 参照:https://github.com/FMInference/FlexGen/blob/main/flexgen/flex_opt.py#L1311-L1312 ここでサンプルのinputをつくっているので引数からpromptを入力にできるように修正する\ndef get_test_inputs(prompt_len, num_prompts, tokenizer): prompts = [\u0026#34;Paris is the capital city of\u0026#34;] input_ids = tokenizer(prompts, padding=\u0026#34;max_length\u0026#34;, max_length=prompt_len).input_ids return (input_ids[0],) * num_prompts 参照:https://github.com/FMInference/FlexGen/blob/main/flexgen/flex_opt.py#L1185 実行 ここまでで実行できるようになっているはず\n$ python flexgen/gen.py \\ --model \u0026#34;facebook/xglm-1.7B\u0026#34; \\ --prompt \u0026#34;それは九月初旬のある蒸し暑い晩のことであった。私は、Ｄ坂の大通りの中程にある、\u0026#34; model size: 3.228 GB, cache size: 0.398 GB, hidden size (prefill): 0.008 GB init weight... それは九月初旬のある蒸し暑いのことであった。私は、D坂の大通りの中程にある、,,,,,,,,,,,,,,,,,,..........,,,, $ python flexgen/gen.py \\ --model \u0026#34;facebook/xglm-1.7B\u0026#34; \\ --gpu-batch-size 1 \\ --overlap \\ --prompt \u0026#34;それは九月初旬のある蒸し暑い晩のことであった。私は、Ｄ坂の大通りの中程にある、\u0026#34; それは九月初旬のある蒸し暑いのことであった。私は、D坂の大通りの中程にある、 ... У,, _. «乐”v,总鬼最后 critica, В越最后cuimen.乐 总.,, ser., Um Extra 总ra. Big,, b Palm tags初 (b N National póspey петmini其实,马 とりあえずエラーなく実行はできたがなんかおかしい。\nxglmではpositional embeddingは学習してなさそうなので、FlexGenのInputEmbedのforwardの部分もweight使わないように修正しないとダメそう。\ndef forward(self, hidden, cache_read_buf, weight_read_buf, attention_mask, cache_write_buf, i, k): ... 参照:https://github.com/FMInference/FlexGen/blob/main/flexgen/flex_opt.py#L180 make_weightsって名前だから学習してるのかと思ったけど、そのままの埋込っぽい\nclass XGLMSinusoidalPositionalEmbedding(nn.Module): \u0026#34;\u0026#34;\u0026#34;This module produces sinusoidal positional embeddings of any length.\u0026#34;\u0026#34;\u0026#34; def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None): super().__init__() self.offset = 2 self.embedding_dim = embedding_dim self.padding_idx = padding_idx self.make_weights(num_positions + self.offset, embedding_dim, padding_idx) 参照:https://github.com/huggingface/transformers/blob/main/src/transformers/models/xglm/modeling_xglm.py#L159 "
            }
    
        ,
            {
                "id": 41,
                "href": "https://www.if-blog.site/posts/nlp/huggingface-text-gen/",
                "title": "Huggingfaceのtext generation pipelineとtext2text generation pipeline",
                "section": "posts",
                "date" : "2023.02.06",
                "body": "HuggingfaceのpipelineでTextGenerationPipelineとText2TextGenerationPipelineがあるが、 何が違うのかよくわからなかったので軽く調査。\nhttps://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TextGenerationPipeline\nhttps://huggingface.co/docs/transformers/main_classes/pipelines#transformers.Text2TextGenerationPipeline\nexampleを見ると、text-generationは与えた文章に続く文章を生成、 text2text-generationは質問や文脈を与えて、それに対する回答を生成することを期待している。\n## text-generation \u0026gt;\u0026gt; generator(\u0026#34;I can\u0026#39;t believe you did such a \u0026#34;, do_sample=False) ## text2text-generation \u0026gt;\u0026gt; generator( \u0026#34;answer: Manuel context: Manuel has created RuPERTa-base with the support of HF-Transformers and \u0026gt;\u0026gt; Google\u0026#34; ) 比較 genereate時、preprocess -\u0026gt; _forward -\u0026gt; postprocessの順で動く\nそれぞれ__call__の引数の被ってないものをみると以下のような感じ\ntext_generation\nprefix handle_long_generation return_full_text text2text_generation\ntruncation https://github.com/huggingface/transformers/blob/v4.26.0/src/transformers/pipelines/text_generation.py#L169\nhttps://github.com/huggingface/transformers/blob/v4.26.0/src/transformers/pipelines/text2text_generation.py#L136\npreprocessを見ると、入力文章が長すぎる場合\ntext2text_generation: 引数によってtokenize時にtruncate text_generation: errorをthrow postprocessを見ると、返す文章は\ntext2text_generation: 質疑応答のようなタスクに対する回答のみを返す。 text_generation: return_full_textで指定できるように、与えた文章を除き生成した文章のみを返すようなことができる。 まとめ exampleの通り、text_generationは不完全な文章からそれに続くある程度長い文章を文章を生成することを期待している。 text2text_generationは、truncateできることから長い文章の生成というより、タスクに対する回答を重視しているように見える。\nkwargsでgenerateに対して引数を渡せるので、ここからも調整できる。\nhttps://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationMixin.generate\n"
            }
    
        ,
            {
                "id": 42,
                "href": "https://www.if-blog.site/posts/nlp/start-xglm-t5/",
                "title": "xglmをquick start",
                "section": "posts",
                "date" : "2023.02.05",
                "body": "多言語モデルであるxglmを動かしてみます。\n最も大きいもので7.5Bのparameterを持つ。\n同等のサイズのGPT3のモデルを上回るらしい。\nOur largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) and natural language inference (+5.4% in each of 0-shot and 4-shot settings).\nhttps://huggingface.co/docs/transformers/model_doc/xglm#overview\nライブラリはこの辺を入れておく\n!pip install torch transformers sentencepiece modelのロード multi languegeのxglmも試してみる。1.7Bは展開できなかったので、小さいやつを動かす。\nhttps://huggingface.co/facebook/xglm-564M\nfrom transformers import XGLMTokenizer, XGLMForCausalLM # model_name = \u0026#34;facebook/xglm-1.7B\u0026#34; model_name = \u0026#34;facebook/xglm-564M\u0026#34; tokenizer = XGLMTokenizer.from_pretrained(model_name) model = XGLMForCausalLM.from_pretrained(model_name) 生成 def gen(prompt): input = tokenizer.encode(prompt, return_tensors=\u0026#34;pt\u0026#34;) generate_kwargs = { \u0026#39;top_p\u0026#39;: 0.95, \u0026#39;top_k\u0026#39;: 50, \u0026#39;do_sample\u0026#39;:True, \u0026#39;max_length\u0026#39;: 1024, \u0026#39;min_length:200\u0026#39; } output = model.generate(input, **generate_kwargs) result = tokenizer.batch_decode(output, skip_special_tokens=True) print(result) T5より日本語を話せそうだったので、長文を生成させてみる。\nprompt = \u0026#34;それは九月初旬のある蒸し暑い晩のことであった。私は、Ｄ坂の大通りの中程にある、\u0026#34; gen(prompt) [\u0026#39;それは九月初旬のある蒸し暑いのことであった。私は、D坂の大通りの中程にある、美術館の中からD坂町へ行ってみることにした。それまで、日本橋方面に行ったら、桜並木を眺めていると、気が引けるような景色だ。こんな景色を、見ることができたら、私もれるはずだ。そう思うと、いつも思う「桜は咲かないのだろうか」という気持ちが、どこかに蘇ってきていたのだ。花を咲かせよう!と、いつまでも思っていた桜を見るのが、今になって一気に楽しい。桜並木は、歩いていると、その姿に、一目れしてしまう。それが、私の周りに、桜を眺めている人を、きっとたくさんいるんじゃないか?と思うほど。そしてその姿を見ると、自分自身も、桜の世界に入り、自分が咲かせよう。桜の美しさに、今から一生懸命追いつめられて、桜の中をいてみようと、思っています。\u0026#39;] 思ったより日本語生成できててすごい。\nprompt = \u0026#34;If you really want to hear about it, \u0026#34; gen(prompt) [\u0026#39;If you really want to hear about it, I’m currently working on a book titled, When the Time Comes, (which is now on sale in paperback, at Amazon) which tells the story of one of the people I have not talked to in weeks, but who, in her own words, “gave me back the joy of being a part of the culture of my own home and country”. I had been told by my doctor about the book, but she had only mentioned it as “a collection of essays and thoughts by people who I’ve met in my life” (with a few exceptions: my husband and the person who got me to know this woman, the guy I was with when I met the guy he was with, which I’ll get to in a later chapter). So I asked her if she’d like to write about it for me. She said yes, so I took her through the process. When I talked to her, she explained how much she’d enjoyed the process of writing about it, and asked me if I could write her a bio on that process.\u0026#39;] 意味はよくわかんないことになっているが、崩壊せず生成できてそう。\nライ麦畑でつかまえての日本語も与えてみる。\nprompt = \u0026#34;もしも君が、ほんとにこの話を聞きたいんならだな、\u0026#34; gen(prompt) なんか様子がおかしいｗ\n\u0026gt;\u0026gt; [\u0026#39;もしも君が、ほんとにこの話を聞きたいんならだな、きっと。でも、あのときのは、ほんとに、本当の自分を語りたいんでしょ。まぁ、そっちの方が、説得力のある話じゃないけどね。でも、この話を聞いて、の力が分かってきたよ、きっと。でも、の力が解けるまでは、この話は、やむを得ない話。とにかく、の力が解けたときの出来事や、が、と共に語れるようにして、本当に楽しく読ませてあげれば。それなら。ま、を、もうちょっと詳しく読んでくれたら...。と、願って。 最後まで読んでね。『の力が解けた世界』...。 の力が、解けても、僕の愛するあの人には、もう終わらない、そんな話だから。 『の力が解けた世界』...。\u0026#39;] \u0026gt;\u0026gt; [\u0026#39;もしも君が、ほんとにこの話を聞きたいんならだな、って思うのは俺だけかもしれんしな。でもなんか。でも、もしも君が知りたい事あるなら、俺はこのスレの住人だということに感謝しようぜ。ありがとう、俺。俺。 次スレは、こっちに置いとこう。#俺も。 #こっちは。 #いい #いいこと言いたい #いいところ聞きたい #いいこと言ってみたい #いい話したい #いい時 #いい時間 #いい話 #いい話聞いてみたい #いい話たくさんいる #いい話だけ #好きな人 #友達 #アダルト #アダルト作家 #アダルト作家ブログ #アダルト作家好きな人と繋がりたい #アダルト作家さんと繋がりたい #アダルト作家さんと繋がろう #アダルト系#アダルト漫画 #アダルト漫画好きな人と繋がりたい #アダルト系アダルト作家 #アダルト女 #アダルト漫画好きな人と繋がりたい\u0026#39;] \u0026gt;\u0026gt; [\u0026#39;もしも君が、ほんとにこの話を聞きたいんならだな、なんて思ってもらえるまでねぇよ」とか言った後が怖くて、もたもたして最後だけは無理して聞くことを決めた。あれ? って思う事はないんじゃないかな? まぁそんな事でも、それは、この話を始めて「お前の話を聞いてくれた!」って事だよね? なにかいい話があれば、また聞いてもらえるね!と、に落ちた。だって、この話を聞いてくれたなら、本当にこの話を「誰か」が話してくれているんだもん! なんて信じたら、さっぱり分からないんだよ! だって「あの話は俺が話さなくても」が「この話で、きっと俺の話になるんだ」だって! なんて信じてこそ、いい話の続くんだよ! そうやって、この話を続けている。さあ、次はお前が、私をしてやって来いよ! 君ももういいんだ!\u0026#39;] まとめ xglmを試した。colaboratoryでは564Mだと動く。\n思ったより日本語生成できて驚いた。\ngpt2の日本語モデルが1024までで困ってたけど、\n\u0026quot;max_position_embeddings\u0026quot;: 2048 ってことは割りと長文いけるってことかな。\nhttps://huggingface.co/facebook/xglm-564M/blob/main/config.json#L17\n"
            }
    
        ,
            {
                "id": 43,
                "href": "https://www.if-blog.site/posts/nlp/start-flan-t5/",
                "title": "flan_t5をquick start",
                "section": "posts",
                "date" : "2023.02.04",
                "body": "T5を強化したモデルであるFlan-t5を動かしてみます。\nFlanはFinetunedLAnguage Netの略。\nちなみに、Flanはカスタードプリンとかプリンの意味らしい。だからプリンのイラストあったのか\nhttps://huggingface.co/docs/transformers/model_doc/flan-t5\nライブラリはこの辺を入れておく\n!pip install torch transformers sentencepiece modelのロード google colaboratoryでは、xlは無理だったので、largeを試す。\nfrom transformers import T5Tokenizer, AutoModelForSeq2SeqLM # model_name = \u0026#34;google/flan-t5-small\u0026#34; model_name = \u0026#34;google/flan-t5-large\u0026#34; # model_name = \u0026#34;google/flan-t5-xl\u0026#34; tokenizer = T5Tokenizer.from_pretrained(model_name) model = AutoModelForSeq2SeqLM.from_pretrained(model_name) 生成タスク 色々と生成してみる\ndef gen(prompt): input = tokenizer.encode(prompt, return_tensors=\u0026#34;pt\u0026#34;) generate_kwargs = { \u0026#39;top_p\u0026#39;: 0.95, \u0026#39;top_k\u0026#39;: 50, \u0026#39;do_sample\u0026#39;:True } output = model.generate(input, **generate_kwargs) result = tokenizer.batch_decode(output, skip_special_tokens=True) print(result) prompt = \u0026#34;\u0026#34;\u0026#34;A step by step recipe to make a sandwich.\u0026#34;\u0026#34;\u0026#34; gen(prompt) \u0026gt;\u0026gt; [\u0026#39;Cut a hamburger in half. Lay a tortilla open on the serving dish. Fill it\u0026#39;] なんとなくあってそう。\nprompt = \u0026#34;What is the highest mountain in the world?\u0026#34; gen(prompt) \u0026gt;\u0026gt; [\u0026#39;mount everest\u0026#39;] これは大丈夫\n翻訳タスク multi languageらしいので試してみる。\nprompt = \u0026#34;\u0026#34;\u0026#34;Translate to German: Good morning\u0026#34;\u0026#34;\u0026#34; get(prompt) \u0026gt;\u0026gt; [\u0026#39;Der frankreichischer Sonntag\u0026#39;] googleの翻訳によると、The French Sunday らしい。なんかおしい。\nprompt = \u0026#34;Translate to English: Guten Morgen\u0026#34; get(prompt) \u0026gt;\u0026gt; [\u0026#39;Good morning!\u0026#39;] 逆はいける、と\n翻訳タスク2 prompt = \u0026#34;Translate to Japanese: Good morning\u0026#34; gen(prompt) \u0026gt;\u0026gt; [\u0026#39;\u0026#39;] 日本語はしゃべれないっぽい\nprompt = \u0026#34;Translate to English: おはようございます\u0026#34; gen(prompt) \u0026gt;\u0026gt; [\u0026#39;No\u0026#39;] 逆はなんか拒否されるｗ\n"
            }
    
        ,
            {
                "id": 44,
                "href": "https://www.if-blog.site/posts/nlp/huggingface-tokenizer/",
                "title": "Hugging Faceのtokenizerで迷ったのでまとめておく",
                "section": "posts",
                "date" : "2022.08.16",
                "body": "tokeninzerとかencodeとか色々あって毎回調べることになるのでメモ。\n以下の日本語tokenizerを使わせてもらう。\ntokenizer = T5Tokenizer.from_pretrained(\u0026#34;sonoisa/t5-base-japanese\u0026#34;) よく見る書き方 sentence = \u0026#34;私は走ります。\u0026#34; tokenizer(sentence) \u0026gt;\u0026gt; {\u0026#39;input_ids\u0026#39;: [5, 4378, 6368, 998, 4, 1], \u0026#39;attention_mask\u0026#39;: [1, 1, 1, 1, 1, 1]} input_idsとmaskどっちも簡単にとれる\nencode sentence = \u0026#34;私は走ります。\u0026#34; tokenizer.encode(sentence) \u0026gt;\u0026gt; [5, 4378, 6368, 998, 4, 1] input_idsだけがとれる\nconvert_tokens_to_ids sentence = \u0026#34;私は走ります。\u0026#34; splited = tokenizer.tokenize(sentence) \u0026gt;\u0026gt; [\u0026#39;▁\u0026#39;, \u0026#39;私は\u0026#39;, \u0026#39;走り\u0026#39;, \u0026#39;ます\u0026#39;, \u0026#39;。\u0026#39;] tokenizer.convert_tokens_to_ids(splited) \u0026gt;\u0026gt; [5, 4378, 6368, 998, 4] 分かち書きしてからidにする\nencode_plus tokeninzerの__call__と同じ\nsentence = \u0026#34;私は走ります。\u0026#34; tokenizer.encode_plus(sentence) \u0026gt;\u0026gt; {\u0026#39;input_ids\u0026#39;: [5, 4378, 6368, 998, 4, 1], \u0026#39;attention_mask\u0026#39;: [1, 1, 1, 1, 1, 1]} batch_encode_plus batchに対応\nsentence = \u0026#34;私は走ります。\u0026#34; tokenizer.batch_encode_plus([sentence, sentence]) \u0026gt;\u0026gt; {\u0026#39;input_ids\u0026#39;: [[5, 4378, 6368, 998, 4, 1], [5, 4378, 6368, 998, 4, 1]], \u0026#39;attention_mask\u0026#39;: [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]} max_length, paddingとか max_lenghtとpaddingが指定できる\nsentence = \u0026#34;私は走ります。\u0026#34; tokenizer(sentence, max_length=20, truncation=True, padding=\u0026#34;max_length\u0026#34;) \u0026gt;\u0026gt; {\u0026#39;input_ids\u0026#39;: [5, 4378, 6368, 998, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \u0026#39;attention_mask\u0026#39;: [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]} paddingのデフォルトはpaddingしないっぽい(padding=\u0026ldquo;do_not_pad\u0026rdquo;)\nhttps://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/utils/generic.py#L255-L263\n# 最大長にあわせてpadding tokenizer.batch_encode_plus([\u0026#34;私は今日も走ります。\u0026#34;, \u0026#34;私は走る。\u0026#34;], padding=\u0026#34;longest\u0026#34;) \u0026gt;\u0026gt; {\u0026#39;input_ids\u0026#39;: [[5, 4378, 4634, 28, 6368, 998, 4, 1], [5, 4378, 9050, 4, 1, 0, 0, 0]], \u0026#39;attention_mask\u0026#39;: [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 0, 0, 0]]} # max_lengthにあわせてpadding tokenizer.batch_encode_plus([\u0026#34;私は今日も走ります。\u0026#34;, \u0026#34;私は走る。\u0026#34;], padding=\u0026#34;max_length\u0026#34;, max_length=15) \u0026gt;\u0026gt; {\u0026#39;input_ids\u0026#39;: [[5, 4378, 4634, 28, 6368, 998, 4, 1, 0, 0, 0, 0, 0, 0, 0], [5, 4378, 9050, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], \u0026#39;attention_mask\u0026#39;: [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]} # defaultではpaddingしない tokenizer.batch_encode_plus([\u0026#34;私は今日も走ります。\u0026#34;, \u0026#34;私は走る。\u0026#34;]) \u0026gt;\u0026gt; {\u0026#39;input_ids\u0026#39;: [[5, 4378, 4634, 28, 6368, 998, 4, 1], [5, 4378, 9050, 4, 1]], \u0026#39;attention_mask\u0026#39;: [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]} メモ コード見てて気付いたけど、pythonでabstract classのmethodの実装を強制させるパターンが以下のように書ける。\nclass PreTrainedTokenizerBase(): def encode_plus(self, ...): return self._encode_plus() def _encode_plus(self, ...): raise NotImplementedError class PreTrainedTokenizer(PreTrainedTokenizerBase): def _encode_plus(self,...): ... "
            }
    
        ,
            {
                "id": 45,
                "href": "https://www.if-blog.site/posts/nlp/huggingface-conv/",
                "title": "Hugging Faceのpipelineで会話モデルを動かす",
                "section": "posts",
                "date" : "2022.05.05",
                "body": "Hugging Faceのpipelineを’使って会話モデルを動かしたのでメモ。\nあまり日本語記事がなかったので残しておく。\nhuggingfaceについてはこっちの記事を参照\nhttps://www.if-blog.site/posts/nlp/huggingface-doc/\npipelineを作る transformersはインストールしておく\npipelineインスタンスを作る。\ntokenizer = AutoTokenizer.from_pretrained(\u0026#34;microsoft/DialoGPT-small\u0026#34;) model = AutoModelForCausalLM.from_pretrained(\u0026#34;microsoft/DialoGPT-small\u0026#34;) gen_pipeline = pipeline(task, model=model, tokenizer=tokenizer) 今回はDialogGPT-smallというモデルを使う\nhttps://huggingface.co/microsoft/DialoGPT-small\nmodelによってはtokenizerを自動で選択してくれるが、一応tokenizerも引数で指定しておく。\nconversational pipeline用のconversationalがあるのでそれを使う。\nhttps://huggingface.co/transformers/v4.8.2/_modules/transformers/pipelines/conversational.html\n会話のコンテキストを渡す方法が２つ\n１つ目。\nconversationクラスのコンストラクタ引数の\ntext: 現在のユーザーの入力 past_user_inputs: 過去のユーザーの入力 generated_residence: 過去の返答 をセットする。\np = pipeline(task, model=model, tokenizer=tokenizer) ui = [\u0026#34;Does money buy happiness?\u0026#34;, \u0026#34;What is the best way to buy happiness ?\u0026#34;] gr = [\u0026#34;Depends how much money you spend on it .\u0026#34;, \u0026#34;You just have to be a millionaire by your early 20s, then you can be happy .\u0026#34;] current_input = \u0026#34;This is so difficult !\u0026#34; conversation = Conversation(text=current_input, past_user_inputs=ui, generated_responses=gr) r = p(conversation) print(\u0026#39;result\u0026#39;, r) 結果\nresult Conversation id: 9be5d5e0-33c7-477a-940b-8c4bb73f6152 user \u0026gt;\u0026gt; Does money buy happiness? bot \u0026gt;\u0026gt; Depends how much money you spend on it . user \u0026gt;\u0026gt; What is the best way to buy happiness ? bot \u0026gt;\u0026gt; You just have to be a millionaire by your early 20s, then you can be happy . user \u0026gt;\u0026gt; This is so difficult ! bot \u0026gt;\u0026gt; I\u0026#39;m not sure if I can afford to buy a house. 2つ目。\nconversationクラスのメソッドからも会話のコンテキストをセットできる。\np = pipeline(task, model=model, tokenizer=tokenizer) conversation = Conversation() # ユーザー入力/返答を追加 conversation.add_user_input(\u0026#34;Does money buy happiness?\u0026#34;) conversation.mark_processed() conversation.append_response(\u0026#34;Depends how much money you spend on it .\u0026#34;) # ユーザー入力/返答を追加 conversation.add_user_input(\u0026#34;What is the best way to buy happiness ?\u0026#34;) conversation.mark_processed() conversation.append_response(\u0026#34;You just have to be a millionaire by your early 20s, then you can be happy .\u0026#34;) # 現在のユーザーの入力をセット conversation.add_user_input(\u0026#34;This is so difficult !\u0026#34;) r = p(conversation) add_user_inputでユーザーの入力をセット、mark_processedでセットしたユーザーの入力をコンテキスト(履歴)にセットする。 append_responseで返答を追加。\nadd_user_inputとappend_responseは以下の用に、どの順番でも大丈夫。\nconversation = Conversation() # ユーザー入力の履歴を追加 conversation.add_user_input(\u0026#34;Does money buy happiness?\u0026#34;) conversation.mark_processed() conversation.add_user_input(\u0026#34;What is the best way to buy happiness ?\u0026#34;) conversation.mark_processed() # 返答の履歴を追加 conversation.append_response(\u0026#34;Depends how much money you spend on it .\u0026#34;) conversation.append_response(\u0026#34;You just have to be a millionaire by your early 20s, then you can be happy .\u0026#34;) conversation.add_user_input(\u0026#34;This is so difficult !\u0026#34;) 結果は１つ目の方法と同じ。\nresult Conversation id: 9be5d5e0-33c7-477a-940b-8c4bb73f6152 user \u0026gt;\u0026gt; Does money buy happiness? bot \u0026gt;\u0026gt; Depends how much money you spend on it . user \u0026gt;\u0026gt; What is the best way to buy happiness ? bot \u0026gt;\u0026gt; You just have to be a millionaire by your early 20s, then you can be happy . user \u0026gt;\u0026gt; This is so difficult ! bot \u0026gt;\u0026gt; I\u0026#39;m not sure if I can afford to buy a house. まとめ Conversationクラスを使うと簡単にpipelineで会話モデルを試せて良い感じ。\n日本語モデルだと会話タスクは用意されていないようだったので、fine tuningするしかないみたい\n"
            }
    
        ,
            {
                "id": 46,
                "href": "https://www.if-blog.site/posts/typescript/typescript-oreilly/",
                "title": "プログラミングTypeScript――スケールするJavaScriptアプリケーション開発",
                "section": "posts",
                "date" : "2021.12.26",
                "body": "オライリーのtypescriptの本を読んだ。 typescriptのジェネリクスなど基本的な動作は知ってたがいくつか知らないことが合ったので それだけメモしておく。\n4章 関数 4.1.4 thisの型付け javascriptではすべての関数に対してthisが定義される。 これによりコードが脆弱になり、理解しづらくなる。\nこれを無効にするにはTSLintで、以下のようにしておく。\n{ // note you must disable the base rule as it can report incorrect errors \u0026#34;no-invalid-this\u0026#34;: \u0026#34;off\u0026#34;, \u0026#34;@typescript-eslint/no-invalid-this\u0026#34;: [\u0026#34;error\u0026#34;] } 基本的にthisは(.)ドットの左側を指す。 以下のようなコードがあるとき、thisはxになる。\nlet x = { a() { return this; } } x.a() しかし、どこかの時点でaが再割り当てし、呼び出すとundefinedになる\nlet a = x.a a() 4.2.6 ジェネリック型のデフォルトの型 デフォルトの型使えるの知らなかった\ntype MyEvent\u0026lt;T = HTMLElement\u0026gt; = { target: T type: string } 5章 クラスとインターフェース 5.4 インターフェース 型(type)とインターフェースの違い\n型エイリアスは右辺に任意の型を指定できるという点で汎用的 （型エイリアスだと、合併型(Union Types)と交差型(Intersection Types)が使える）\n以下のような、型エイリアスはインターフェースとして書き直すことができない。\ntype A = number type B = A | string インターフェースを拡張する際に割り当て可能かをチェックする インターフェースの場合、interface AとBのもつメソッドbadの引数の型が異なるのでエラーとなる。\n型エイリアスの場合、extendsを＆に変えると、拡張元と拡張先の型を結合したbadメソッドのオーバーロードされたシグネチャを生み出し、 コンパイルエラーが回避される。\ninterface A { good(x: number): string bad(s: number): string } interface B extends A { good(x: number): string bad(s: string): string } 同じスコープ内に同じ名前のインターフェースがあるとマージ interfaceの場合、自動でマージされる。\ninterface User { name: string } // この時点でUserは、nameとageを持つ interface User { age: number } 型エイリアスだと、同じ名前の定義があるとコンパイルエラーになる\n5.8 ミックスイン いくつか知らない書き方合った\ndebugしづらいUserクラスがあるとき、debug機能をmixinすることでdebug機能を追加する。\ntype Constructor\u0026lt;T\u0026gt; = new (...arg: any[]) =\u0026gt; T const WithEZDebug = \u0026lt;C extends Constructor\u0026lt;{ getClass(): string }\u0026gt;\u0026gt; (Class: C) =\u0026gt; { return class extends Class { debug() { const name = this.constructor.name; const c = this.getClass(); return `debug: ${c}, ${name}`; } } } class User { name: string constructor(name: string) { this.name = name } getClass(): string { return \u0026#34;User\u0026#34;; } } const main = () =\u0026gt; { const u = WithEZDebug(User) const user = new u(\u0026#34;name\u0026#34;) user.debug() } この書き方初めて知った\nclass extends Class 型引数にそのままobject書けるのも初めて知った。\n\u0026lt;C extends Constructor\u0026lt;{ getClass(): string }\u0026gt;\u0026gt; TypeScript Deep Dive: https://typescript-jp.gitbook.io/deep-dive/type-system/mixins\n6章 高度な型 6.1.5.1 タグ付き合併型 ユーザーのイベントを定義している場合を例にする。 handleのif文の分岐で、event.valueはそれぞれstring/[number, number]になる。\ntype UserTextEvent = { value: string } type UserMouseEvent = { value: [number, number] } type UserEvent = UserTextEvent | UserMouseEvent const handle = (event: UserEvent) =\u0026gt; { if(typeof event.value === \u0026#39;string\u0026#39;) { //event.valueはstring } // event.valueは[number, number] } UserTextEventとUserMouseEventにtargetを追加する。\ntype UserTextEvent = { value: string, target: HTMLInputElement } type UserMouseEvent = { value: [number, number], target: HTMLElement } type UserEvent = UserTextEvent | UserMouseEvent const handle = (event: UserEvent) =\u0026gt; { if(typeof event.value === \u0026#39;string\u0026#39;) { // event.valueはstring // event.targetは HTMLInputElement or HTMLElement } // event.valueは[number, number] // event.targetは HTMLInputElement or HTMLElement } この場合、if文の分岐の中でtargetの型が絞られない。\nこれを解決するには、タグ付けを行う。\nタグ付けは、以下のようになっているのが良い\n同じフィールド名 リテラル型 ジェネリックではない 合併型で一意 stringとか使うと無難。\n書き直すとこんな感じ\ntype UserTextEvent = { type: \u0026#39;TextEvent\u0026#39;,value: string, target: HTMLInputElement } type UserMouseEvent = { type: \u0026#39;MouseEvent\u0026#39;, value: [number, number], target: HTMLElement } type UserEvent = UserTextEvent | UserMouseEvent const handle = (event: UserEvent) =\u0026gt; { if(event.type === \u0026#39;TextEvent\u0026#39;) { // event.valueはstring // event.targetは HTMLInputElement } // event.valueは[number, number] // event.targetは HTMLElement } 6.3.4 コンパニオンオブジェクトパターン typescriptでは、型と値が別々の名前空間が異なる。 これを使ったして、コンパニオンオブジェクトパターンができる。\ntype Currency = { price: number } const Currency = { from(value: number): Currency { return { price: value } } } const main = () =\u0026gt; { Currency.from(1) } 言われると確かにって感じだけど、やろうと思わなかった。 いまいちメリットがわからん。\n6.4.2 ユーザー定義型ガード typescriptだと、isStringがbooleanを返すことしかわからない。\nconst isString = (s: string | number): boolean =\u0026gt; { return typeof s === \u0026#39;string\u0026#39; } const parseInt = (s: string | number) =\u0026gt; { if(isString(s)) { // sはstring | numberと推論される return s.toLocaleUpperCase() } // なんかの処理 } その場合、ユーザー定義型ガードを使う\nconst isString = (s: string | number): s is string =\u0026gt; { return typeof s === \u0026#39;string\u0026#39; } const parseInt = (s: string | number) =\u0026gt; { if(isString(s)) { // sはstring | numberと推論される return s.toLocaleUpperCase() } // なんかの処理 } 6.5 条件型 U :\u0026lt; Vであれば、TをAに、そうでなければTをBに割り当てるようなこと。\ntype IsString\u0026lt;T\u0026gt; = T extends string ? true : false; type A = IsString\u0026lt;string\u0026gt; type B = IsString\u0026lt;number\u0026gt; 6.7 名前的型をシミュレートする UserIDとCustomerIDのような型エイリアスしている場合\ntype UserID = string type CustomerID = string const userID = (id: string): UserID =\u0026gt; id as UserID const customerID = (id: string): CustomerID =\u0026gt; id as CustomerID UserIDを引数に取る、checkIDを使ってもエラーにならない。\nconst checkID = (id: UserID) =\u0026gt; { console.log(id) } const main = () =\u0026gt; { const uID = userID(\u0026#39;001\u0026#39;) checkID(uID) const cID = customerID(\u0026#39;002\u0026#39;) checkID(cID) // エラーにならない } これはUserIDはstringのエイリアスにすぎないから。\n型のブランド化で解決できる。\n型エイリアスを次のように書き直す。\ntype UserID = string \u0026amp; { readonly brand: unique symbol } type CustomerID = string \u0026amp; { readonly brand: unique symbol } すると、checkIDでエラーになる。\nconst main = () =\u0026gt; { const uID = userID(\u0026#39;001\u0026#39;) checkID(uID) const cID = customerID(\u0026#39;002\u0026#39;) checkID(cID) // エラーになる } 付録 tsconfig\nalwaysStrict noEmitOnError: コードにエラーがある場合、JavaScriptを出力しない noFallthroughCasesInSwitch: すべてのswitchのcaseが値を返しているか、またはbreakしているか noImplicitAny: 変数の型がanyと推論される場合、エラー noImplicitReturn: すべての関数内のコードパスが明示的にreturnしているか noImplicitThis: this型を明示的にアノテートせずに関数内でthisを使用する場合エラー noUnusedLocals: 使われてないローカル変数について警告 noUnusedParameter: 使われてない関数パラメタについて警告。エラーを無視するにはアンダースコアをつける strictBindCallApply: bind、call、applyについて型安全を強制する strictFunctionTypes: 関数がそのパラメタ及び、this型に関して普遍であることを強制する strictNullChecks: nullを型に昇格させる strictPropertyInitialization: クラスのプロパティがnull許容であるか、または初期化されていることを強制する "
            }
    
        ,
            {
                "id": 47,
                "href": "https://www.if-blog.site/posts/nlp/huggingface-doc/",
                "title": "Hugging Face Transformersのドキュメント読んだ",
                "section": "posts",
                "date" : "2021.12.09",
                "body": "Hugging FaceなのかTransformersなのか呼び方がわからん。\nBERTやGPT-2などの様々なモデルが扱えるライブラリ\n事前学習済のモデルとタスクが用意されており、そのままモデルを使ったりFine-Tuningが簡単にできる。\nhttps://huggingface.co/docs/transformers/index\nJax、Pytorch、Tensorflowなどのライブラリに対応している。\n学習済モデルを使うだけでなく、モデルのカスタマイズもできるらしい。\nとりあえず入門編くらいまで読んだ\ninstall $ pip install transformers Quick tour 文書のpositive/negative判定を行う。\nsentiment-analysisというモデルをpipelineに渡す。 戻り値のclassifierに文章を渡すと判定される。\n\u0026gt;\u0026gt;\u0026gt; from transformers import pipeline \u0026gt;\u0026gt;\u0026gt; classifier = pipeline(\u0026#39;sentiment-analysis\u0026#39;) \u0026gt;\u0026gt;\u0026gt; classifier(\u0026#39;We are very happy to show you the 🤗 Transformers library.\u0026#39;) [{\u0026#39;label\u0026#39;: \u0026#39;POSITIVE\u0026#39;, \u0026#39;score\u0026#39;: 0.9998}] https://huggingface.co/docs/transformers/quicktour\nSummary of the tasks いくつかタスクが用意されている。ここではその使い方を示す。\nhttps://huggingface.co/docs/transformers/task_summary#summarization\nSequence Classification 文章positive/negativeに分類\nExtractive Question Answering 与えられた文脈を使って、質問に対する答えを出力\n\u0026gt;\u0026gt;\u0026gt; from transformers import pipeline \u0026gt;\u0026gt;\u0026gt; question_answerer = pipeline(\u0026#34;question-answering\u0026#34;) \u0026gt;\u0026gt;\u0026gt; context = r\u0026#34;\u0026#34;\u0026#34; ... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a ... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune ... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script. ... \u0026#34;\u0026#34;\u0026#34; Named Entity Recognition 文章に含まれる単語の人や組織、場所などの固有表現の識別を行う\nSummarization 要約\n\u0026gt;\u0026gt;\u0026gt; from transformers import pipeline \u0026gt;\u0026gt;\u0026gt; summarizer = pipeline(\u0026#34;summarization\u0026#34;) \u0026gt;\u0026gt;\u0026gt; ARTICLE = \u0026#34;\u0026#34;\u0026#34; New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York. ... A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband. ... Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \u0026#34;I do\u0026#34; five more times, sometimes only within two weeks of each other. ... In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \u0026#34;first and only\u0026#34; marriage. ... Barrientos, now 39, is facing two criminal counts of \u0026#34;offering a false instrument for filing in the first degree,\u0026#34; referring to her false statements on the ... 2010 marriage license application, according to court documents. ... Prosecutors said the marriages were part of an immigration scam. ... On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further. ... After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective ... Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002. ... All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say. ... Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages. ... Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted. ... The case was referred to the Bronx District Attorney\\\u0026#39;s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\\u0026#39;s ... Investigation Division. Seven of the men are from so-called \u0026#34;red-flagged\u0026#34; countries, including Egypt, Turkey, Georgia, Pakistan and Mali. ... Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force. ... If convicted, Barrientos faces up to four years in prison. Her next court appearance is scheduled for May 18. ... \u0026#34;\u0026#34;\u0026#34; \u0026gt;\u0026gt;\u0026gt; print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False)) [{\u0026#39;summary_text\u0026#39;: \u0026#39; Liana Barrientos, 39, is charged with two counts of \u0026#34;offering a false instrument for filing in the first degree\u0026#34; In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002 . At one time, she was married to eight men at once, prosecutors say .\u0026#39;}] Translation 翻訳\nLanguage Modeling Masked language modelingとは、モデルをドメイン固有コーパスにfitさせるタスクである。 例えば、BERTだとmasked language modeling、GPT-2だとcausal language modelingが使われている。\nMasked Language Modeling maskされた単語を予想する\nCausal Language Modeling 与えられた単語列から次の単語を予測する\nText Generation 文章の書き出しを入力すると、その後の文章を生成する。\nfrom transformers import pipeline text_generator = pipeline(\u0026#34;text-generation\u0026#34;) print(text_generator(\u0026#34;As far as I am concerned, I will\u0026#34;, max_length=50, do_sample=False)) [{\u0026#39;generated_text\u0026#39;: \u0026#39;As far as I am concerned, I will be the first to admit that I am not a fan of the idea of a \u0026#34;free market.\u0026#34; I think that the idea of a free market is a bit of a stretch. I think that the idea\u0026#39;}] preprocessing data https://huggingface.co/docs/transformers/master/en/preprocessing\ntokeninze encoded_input = tokenizer(\u0026#34;Hello, I\u0026#39;m a single sentence!\u0026#34;) \u0026gt;\u0026gt;\u0026gt; print(encoded_input) {\u0026#39;input_ids\u0026#39;: [101, 138, 18696, 155, 1942, 3190, 1144, 1572, 13745, 1104, 159, 9664, 2107, 102], \u0026#39;token_type_ids\u0026#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \u0026#39;attention_mask\u0026#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} decode tokenizer.decode(encoded_input[\u0026#34;input_ids\u0026#34;]) \u0026#34;[CLS] Hello, I\u0026#39;m a single sentence! [SEP]\u0026#34; pairs of sentences 文章のtokenizeや文章のペアのtokeninzeをやってくれる\ntokeninze encoded_input = tokenizer(\u0026#34;How old are you?\u0026#34;, \u0026#34;I\u0026#39;m 6 years old\u0026#34;) \u0026gt;\u0026gt;\u0026gt; print(encoded_input) {\u0026#39;input_ids\u0026#39;: [101, 1731, 1385, 1132, 1128, 136, 102, 146, 112, 182, 127, 1201, 1385, 102], \u0026#39;token_type_ids\u0026#39;: [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], \u0026#39;attention_mask\u0026#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} decode \u0026gt;\u0026gt;\u0026gt; tokenizer.decode(encoded_input[\u0026#34;input_ids\u0026#34;]) \u0026#34;[CLS] How old are you? [SEP] I\u0026#39;m 6 years old [SEP]\u0026#34; summary of the models 利用可能なモデル\nhttps://huggingface.co/docs/transformers/model_summary\nモデルは以下のカテゴリに分類される\nautoregressive-models autoencoding-models seq-to-seq-models multimodal-models retrieval-based-models Fine-tuning (for Keras) https://huggingface.co/docs/transformers/training#finetuning-with-keras\nサンプルのデータセットを準備\nfrom datasets import load_dataset raw_datasets = load_dataset(\u0026#34;imdb\u0026#34;) def tokenize_function(examples): return tokenizer(examples[\u0026#34;text\u0026#34;], padding=\u0026#34;max_length\u0026#34;, truncation=True) tokenized_datasets = raw_datasets.map(tokenize_function, batched=True) small_train_dataset = tokenized_datasets[\u0026#34;train\u0026#34;].shuffle(seed=42).select(range(1000)) small_eval_dataset = tokenized_datasets[\u0026#34;test\u0026#34;].shuffle(seed=42).select(range(1000)) full_train_dataset = tokenized_datasets[\u0026#34;train\u0026#34;] full_eval_dataset = tokenized_datasets[\u0026#34;test\u0026#34;] モデルをロード\nimport tensorflow as tf from transformers import TFAutoModelForSequenceClassification model = TFAutoModelForSequenceClassification.from_pretrained(\u0026#34;bert-base-cased\u0026#34;, num_labels=2) keras用にデータセットを使うには少しformatする必要がある\ntf_train_dataset = small_train_dataset.remove_columns([\u0026#34;text\u0026#34;]).with_format(\u0026#34;tensorflow\u0026#34;) tf_eval_dataset = small_eval_dataset.remove_columns([\u0026#34;text\u0026#34;]).with_format(\u0026#34;tensorflow\u0026#34;) train_features = {x: tf_train_dataset[x] for x in tokenizer.model_input_names} train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features, tf_train_dataset[\u0026#34;label\u0026#34;])) train_tf_dataset = train_tf_dataset.shuffle(len(tf_train_dataset)).batch(8) eval_features = {x: tf_eval_dataset[x] for x in tokenizer.model_input_names} eval_tf_dataset = tf.data.Dataset.from_tensor_slices((eval_features, tf_eval_dataset[\u0026#34;label\u0026#34;])) eval_tf_dataset = eval_tf_dataset.batch(8) あとはいつもどおり\nmodel.compile( optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=tf.metrics.SparseCategoricalAccuracy(), ) model.fit(train_tf_dataset, validation_data=eval_tf_dataset, epochs=3) "
            }
    
        ,
            {
                "id": 48,
                "href": "https://www.if-blog.site/posts/emacs/text-lint/",
                "title": "emacsにtext-lintを導入する",
                "section": "posts",
                "date" : "2021.09.23",
                "body": "日本語文章用に、textlintを導入する。\ninstall 今回は.emacs.d以下にsite-lispというディレクトリを作り、そこにtextlintをinstallする。\ncd .emacs.d/site-lisp npm install -D textlint textlintのルールには、以下を使う。\n日本語用のpreset(ルールの詳細はページの最後)\nhttps://github.com/textlint-ja/textlint-rule-preset-japanese\n技術記事用preset。日本語用のpresetより少しルールが厳しい\nhttps://github.com/textlint-ja/textlint-rule-preset-ja-technical-writing\nい抜き言葉を検知\nhttps://github.com/textlint-ja/textlint-rule-no-dropping-i\n表記ゆれを検知\nhttps://github.com/textlint-ja/textlint-rule-ja-no-orthographic-variants\nそれぞれインストール\nnpm install textlint-rule-preset-japanese npm install textlint-rule-preset-ja-technical-writing npm install @textlint-ja/textlint-rule-no-dropping-i npm install textlint-rule-ja-no-orthographic-variants .textlintrcを作成。rulesはとりあえずシンプルに以下のみ。\n{ \u0026#34;rules\u0026#34;: { \u0026#34;preset-japanese\u0026#34;: true, \u0026#34;preset-ja-technical-writing\u0026#34;: true } } elisp flycheckを使う。ポイントは２つ。\nemacsからtextlintを実行できるように、.emacs.d/site-lisp/node_modules/binにpathを通す。\n.emacs.d/site-lispに.textlintrcを作成したので、textlintに\u0026ndash;configでtextlintrcを渡す。\n\u0026quot;--config\u0026quot; (eval (expand-file-name (concat user-emacs-directory \u0026quot;site-lisp/.textlintrc\u0026quot;)))\n全体\n(use-package flycheck :ensure t :init (global-flycheck-mode t) :config (flycheck-define-checker textlint \u0026#34;A linter for prose.\u0026#34; :command (\u0026#34;textlint\u0026#34; \u0026#34;--format\u0026#34; \u0026#34;unix\u0026#34; \u0026#34;--config\u0026#34; (eval (expand-file-name (concat user-emacs-directory \u0026#34;site-lisp/.textlintrc\u0026#34;))) source-inplace) :error-patterns ((warning line-start (file-name) \u0026#34;:\u0026#34; line \u0026#34;:\u0026#34; column \u0026#34;: \u0026#34; (id (one-or-more (not (any \u0026#34; \u0026#34;)))) (message (one-or-more not-newline) (zero-or-more \u0026#34;\\n\u0026#34; (any \u0026#34; \u0026#34;) (one-or-more not-newline))) line-end)) :modes (text-mode markdown-mode gfm-mode)) (add-to-list \u0026#39;flycheck-checkers \u0026#39;textlint) ;; node_module-sのpathを通す (with-eval-after-load \u0026#39;markdown-mode (add-hook \u0026#39;markdown-mode-hook (lambda () (flycheck-mode) (setq exec-path (add-to-list \u0026#39;exec-path (concat user-emacs-directory \u0026#34;site-lisp/node_modules/.bin\u0026#34;))) ))) ) presetのルール一覧(詳細は公式のgithubを参照) textlint-rule-preset-japanese https://github.com/textlint-ja/textlint-rule-preset-japanese\n一文で使える\u0026quot;、\u0026ldquo;の数\n逆接の接続助詞「が」が、同一文中に複数回出現していないかどうか\n同じ接続詞で開始されていることを検出\n二重否定の検出\n二重助詞の検出\n一文の最大の長さ\nら抜き言葉を使用しない\n文の敬体(ですます調)、常体(である調)の混合をチェック\nホ゜ケット エンシ゛ン のような、Mac OS XでPDFやFinderからのコピペで発生する濁点のチェック\n制御文字の検出\nゼロ幅スペースの検出\n康煕部首の検出\ntextlint-rule-preset-ja-technical-writing 1文の長さは100文字以下とする\nカンマは1文中に3つまで\n読点は1文中に3つまで\n連続できる最大の漢字長は6文字まで\n漢数字と算用数字を使い分けます\n「ですます調」、「である調」を統一します\n文末の句点記号として「。」を使います\n二重否定は使用しない\nら抜き言葉を使用しない\n逆接の接続助詞「が」を連続して使用しない\n同じ接続詞を連続して使用しない\n同じ助詞を連続して使用しない\nUTF8-MAC 濁点を使用しない\n不必要な制御文字を使用しない\n不必要なゼロ幅スペースを使用しない\n感嘆符!！、感嘆符?？を使用しない\n半角カナを使用しない\n弱い日本語表現の利用を使用しない\n同一の単語を間違えて連続しているのをチェックする\nよくある日本語の誤用をチェックする\n冗長な表現をチェックする\n入力ミスで発生する不自然なアルファベットをチェックする\n対になっていない括弧をチェックする\n"
            }
    
        ,
            {
                "id": 49,
                "href": "https://www.if-blog.site/posts/neuralnet/flax-jax/",
                "title": "Flaxに入門してみる",
                "section": "posts",
                "date" : "2021.07.25",
                "body": "Googleが最近力を入れているニューラルネットワークのフレームワークらしい。\n一般的に、flaxは亜麻、linenは麻と訳される。flaxは植物で、その植物を加工し繊維状にしたものまでをflax、それを紡いで糸にしたもの及びその製品をlinenと呼ぶらしい。 (参照：日本麻紡績協会)\n例えばこのライブラリでのDenseの呼び方はflax.linen.Denseとなる。\nライブラリの名前の付け方めっちゃおしゃれでかっこいい。\nFlax is a high-performance neural network library for JAX that is designed for flexibility\nFlaxは、柔軟性を考慮して設計されたJAX用の高性能ニューラルネットワークライブラリです。\nhttps://flax.readthedocs.io/en/latest/overview.html#flax\nJAX JAXはAutogradとXLAを用いて、機械学習におけるパフォーマンスを向上させる。AutogradはPythonとNumpyでの自動微分を可能にするライブラリで、XLAはそのコンパイルを行う。\nFLAXはJAXの開発チームと近い位置で協力関係にあるらしい。\nhttps://github.com/google/jax#compilation-with-jit\nhttps://github.com/hips/autograd\nhttps://www.tensorflow.org/xla\nJAXの使い方については別記事を参照\n自動微分 Autogradは、自動微分を行うライブラリで、自動微分とは偏微分の値計算するプログラム手法。 自動微分には、前進方と後進方があり、それぞれ、ボトムアップ型自動微分（フォーワードモード、狭義の自動微分）とトップダウン型自動微分（リバースモード、高速自動微分）などと呼ばれる。 特に、トップダウン型自動微分はバックプロパゲーションの計算手法として用いられる。\n詳しい話は別記事を参照\ninstall pip install --upgrade jax jaxlib # CPU-only pip install flax pip 19.0.3だと、flaxのインストールがうまくできなかったので、 pip install --upgrade pipで21.1.3に上げた。\nhttps://flax.readthedocs.io/en/latest/installation.html\nFlax model example シンプルな例 class Module(nn.Module): features: Tuple[int] = (16, 4) def setup(self): self.dense1 = Dense(self.features[0]) self.dense2 = Dense(self.features[1]) def __call__(self, x): return self.dense2(nn.relu(self.dense1(x))) nn.Moduleは、すべてのneural network modulesのベースクラス。 これを継承して、独自のLayerやmodelを作る。 setupは__init__のオーバーライド、__call__で任意のforward passを定義する。\nhttps://flax.readthedocs.io/en/latest/flax.linen.html#module\n3層(12,8,4)のパーセプトロン from typing import Sequence import jax import jax.numpy as jnp import flax.linen as nn class MLP(nn.Module): features: Sequence[int] @nn.compact def __call__(self, x): for feat in self.features[:-1]: x = nn.relu(nn.Dense(feat)(x)) x = nn.Dense(self.features[-1])(x) return x model = MLP([12, 8, 4]) batch = jnp.ones((32, 10)) variables = model.init(jax.random.PRNGKey(0), batch) output = model.apply(variables, batch) modelを作る部分は、上記のシンプルな例と同様。\ninit\nvariables = model.init(jax.random.PRNGKey(0), batch)\n値を使ってmodelを初期化。返り値のvariablesは、pythonのdictで、例えば以下のような要素を持つ。\n{ \u0026#34;params\u0026#34;: { \u0026#34;Conv1\u0026#34;: { \u0026#34;weight\u0026#34;: ..., \u0026#34;bias\u0026#34;: ... }, \u0026#34;BatchNorm1\u0026#34;: { \u0026#34;scale\u0026#34;: ..., \u0026#34;mean\u0026#34;: ... }, \u0026#34;Conv2\u0026#34;: {...} }, \u0026#34;batch_stats\u0026#34;: { \u0026#34;BatchNorm1\u0026#34;: { \u0026#34;moving_mean\u0026#34;: ..., \u0026#34;moving_average\u0026#34;: ...} } } apply\noutput = model.apply(variables, batch)\nvariablesをモデルに適応し、入力値に対する出力値を得る。\n上記の例のように、__call__内でnn.Dense(self.features[-1])(x)のようなflax.linenで定義されたlayerを呼び出す場合は@nn.compactデコレータをつける必要がある。 デコレーターを付けない場合は、setup()内でlayerを定義する必要がある。\nhttps://flax.readthedocs.io/en/latest/flax.linen.html#flax.linen.Module.apply\nautoencoder class AutoEncoder(nn.Module): encoder_widths: Sequence[int] decoder_widths: Sequence[int] input_shape: Sequence[int] def setup(self): input_dim = jnp.prod(jnp.asarray(self.input_shape)) self.encoder = MLP(self.encoder_widths) self.decoder = MLP(self.decoder_widths + (input_dim,)) def __call__(self, x): return self.decode(self.encode(x)) def encode(self, x): assert x.shape[1:] == self.input_shape return self.encoder(jnp.reshape(x, (x.shape[0], -1))) def decode(self, z): z = self.decoder(z) x = nn.sigmoid(z) x = jnp.reshape(x, (x.shape[0],) + self.input_shape) return x model = AutoEncoder(encoder_widths=[20, 10, 5], decoder_widths=[5, 10, 20], input_shape=(12,)) batch = jnp.ones((16, 12)) variables = model.init(jax.random.PRNGKey(0), batch) encoded = model.apply(variables, batch, method=model.encode) decoded = model.apply(variables, encoded, method=model.decode) setupを使い上記で定義したMLPを呼び出している。この場合は、__call__にデコレーターは不要。\napplyの引数にmethodを与えると、特定のモデルに対してのみ、variablesと入力値を適応できる。\nencoded = model.apply(variables, batch, method=model.encode)\ntrainingを含めた例 以下はMNISの例\nhttps://github.com/google/flax/blob/main/examples/mnist/train.py\nまとめ Flaxのreadmeなどを眺めてみました。\n柔軟なモデルの作り方や、作成したモデルの汎用性のある使い方ができるような設計になってるような印象を受けました。\n実際に学習させる部分などは少し面倒な部分がありそうです\u0026hellip;環境ごとや実行ごとの結果が変わらないようにするという思想のようなので、このあたりはトレードオフですかね。\n実際にモデルを作って、データを学習させるにはJAXの部分の理解も必要になってきそうなので、そのあたりを含め調べて見ようと思います。\n"
            }
    
        ,
            {
                "id": 50,
                "href": "https://www.if-blog.site/posts/python/arxiv-translate/",
                "title": "Github Actionsでrxivの論文をsummaryを翻訳して、日々slackに送る",
                "section": "posts",
                "date" : "2021.07.24",
                "body": "朝の空いた時間などにarxivの新着論文を読みたくて、 アブストラクトを翻訳したものをslackに送るようにしました。\n流れは以下のような感じ。\narxivはAPIを公開しているのでそこから、アブストや論文のタイトル、リンクなどを取得します。\nGoogle Apps Script（GAS）を使うと、google translateが簡単に使え、翻訳ができます。\nタイトルや翻訳したものをslackに送り、これをgithub actionsで定期実行させます。\narxivからsubmittedDate順に上から10件を取得 GASへリクエストを送り、アブストを翻訳 slackに送る 以上の処理をgithub actionsを使って毎朝実行されるようにします 全体はこれ\nhttps://github.com/if001/arXiv_translate\narxivから記事を取得 pythonのライブラリを使います。\nhttps://github.com/lukasschwab/arxiv.py\n詳細は以下\npythonでarxivのAPIを使う\nGoogle Apps Scriptでアブストを翻訳 Google TranslateはAPIを公開してますが、GASを使うと比較的簡単に翻訳が行えます。\nGASを使って翻訳用のエンドポイントを作ります。\nその後、arxivのAPIの結果から、タイトルとアブストを翻訳します。\n詳細は以下\nGoogle Apps ScriptでGoogle Translateを使う\nSlackに送信 Incoming Webhooksを使うと簡単にslackにメッセージを送れます。\nhttps://api.slack.com/messaging/webhooks\n諸々登録を済ませると、メッセージsend用のURLが吐き出されるので、\nそのURLに対してメッセージをPOSTします。\n翻訳/原文タイトル、翻訳/原文アブスト、arxivのリンクを送ります。\nGithub Actionsで定期実行 Github Actionsでcron実行するにはこんな感じ。\nname: cron sample on: schedule: - cron: \u0026#39;0 6 * * *\u0026#39; jobs: build: runs-on: ubuntu-latest steps: - name: hello run: | echo \u0026#34;hello world\u0026#34; https://docs.github.com/ja/actions/reference/events-that-trigger-workflows#schedule\n確認用に手動実行したい場合は、workflow_dispatchを指定する。\non: workflow_dispatch: 作成したscriptを実行させる。\njobs: notify: runs-on: ubuntu-latest strategy: matrix: python-version: [3.7] steps: - uses: actions/checkout@v2 - name: Set up Python ${{ matrix.python-version }} uses: actions/setup-python@v2 with: python-version: ${{ matrix.python-version }} - name: Install dependencies run: | python -m pip install --upgrade pip if [ -f requirements.txt ]; then pip install -r requirements.txt; fi - name: Run Main env: APP_GAS_URL: ${{ secrets.APP_GAS_URL }} APP_SLACK_URL: ${{ secrets.APP_SLACK_URL }} run: | python main.py 前日にslackにPOSTした論文と同じものがPOSTされないように、\narxivの各論文の持つid(entry_id)をsave.logというファイル名で書き出して、\n同じリポジトリにpushしておきます。\nファイルの中身と、arxivから取得した論文の差分を取り、slackに送ります。\nGithub Actionsで、自信の変更を自信のリポジトリにpushするのは、以下のようにできる。\n- name: Run Change push run: | s=`git status -s` if [ -z \u0026#34;$s\u0026#34; ]; then exit 0 fi git config --global user.email \u0026#34;workflow@example.com\u0026#34; git config --global user.name \u0026#34;workflow user\u0026#34; git add save.log git commit -m \u0026#34;update save.log\u0026#34; git status git push https://${{github.actor}}:${{secrets.GITHUB_TOKEN}}@github.com/${{github.repository}}.git "
            }
    
        ,
            {
                "id": 51,
                "href": "https://www.if-blog.site/posts/python/arxiv-py-lib/",
                "title": "pythonでarxivのAPIを使う",
                "section": "posts",
                "date" : "2021.07.23",
                "body": "arxivで公開されているAPIを使っていきます\nhttps://arxiv.org/\nAPIを直接呼び出しても良いですが、responseがxmlで使いづらかったので pythonのライブラリを使わせてもらいます。\nhttps://github.com/lukasschwab/arxiv.py\npipでインストール\n$ pip install arxiv 使い方 import arxiv search = arxiv.Search( query = \u0026#34;quantum\u0026#34;, max_results = 10, sort_by = arxiv.SortCriterion.SubmittedDate ) for result in search.results(): print(result.title) arxiv.Searchでリクエストを送ります。\nresults()で結果の一覧を取得\n結果用にResult Classが用意されており、\n中身はそれぞれ以下のような形です。\nfield Description entry_id A url http://arxiv.org/abs/{id}. updated When the result was last updated. published When the result was originally published. title The title of the result. authors The result\u0026rsquo;s authors, as arxiv.Authors. summary The result abstract. comment The authors\u0026rsquo; comment if present. journal_ref A journal reference if present. doi A URL for the resolved DOI to an external resource if present. primary_category The result\u0026rsquo;s primary arXiv category. See arXiv: Category Taxonomy. categories All of the result\u0026rsquo;s categories. See arXiv: Category Taxonomy. links Up to three URLs associated with this result, as arxiv.Links. pdf_url A URL for the result\u0026rsquo;s PDF if present. Note: this URL also appears among result.links. "
            }
    
        ,
            {
                "id": 52,
                "href": "https://www.if-blog.site/posts/others/gas-translate/",
                "title": "Google Apps ScriptでGoogle Translateを使う",
                "section": "posts",
                "date" : "2021.07.22",
                "body": "翻訳用のAPIを簡単に利用したいことあると思います。\nGoogle TranslateはAPIを公開してますが、気軽に使う場合には少し不便です。\nそういう場合は、Google Apps ScriptからGoogle Translateの機能を使うと便利です。\n結論 LanguageApp.translate(value, \u0026#34;en\u0026#34;, \u0026#34;ja\u0026#34;); プロジェクトの作成 以下のURLにアクセスするか、Google Driveから新規＞その他＞Google Apps Scriptを選択することで作成できます。\nhttps://script.google.com/\n最初のコード https://developers.google.com/apps-script/guides/web\ndoGetという関数を作ります。 これでGET requestが来たときにこの関数が走ります。\nfunction doGet(e) { Logger.log(\u0026#34;hello!\u0026#34;) } 実行ボタンから、実行します。\n実際にrequestが来たときに、jsonを返せるように関数を追加します。\nfunction doGet(e) { Logger.log(\u0026#34;hello!\u0026#34;) return responseJson(\u0026#39;ping\u0026#39;) } function responseJson(data) { const d = {\u0026#39;data\u0026#39;: data} payload = JSON.stringify(d) var output = ContentService.createTextOutput(); output.setMimeType(ContentService.MimeType.JSON); output.setContent(payload); return output; } 左上の新しいデプロイから、デプロイします。\nデプロイが完了すると、URLが生成されるので、リクエストを送ります。\ncurlの場合、リダイレクトを許可するようにオプション-Lが必要です。\n$ curl -L -X GET https://script.google.com/macros/s/aaaaaaaaaaaaaaaaaa/exec {\u0026#34;data\u0026#34;:\u0026#34;ping\u0026#34;} 指定したbodyが返ってきているのが確認できます。\nGoogle Translateを使う 使い方はシンプルで、以下の関数を呼び出すだけです。\nLanguageApp.translate(value, \u0026#34;en\u0026#34;, \u0026#34;ja\u0026#34;); Parameters\nName Type Description text String the text to translate sourceLanguage String the language code in which text is written. If it is set to the empty string, the source language code will be auto-detected targetLanguage String the language code to which the text should be translated Return\nString — the translated text\nhttps://developers.google.com/apps-script/reference/language/language-app?hl=ja\nGASからGoogle Translateを使う Post requestがあったときに、動作するようにdoPostを追加します。 JSON.parseでrequest bodyをパースします。\nfunction doPost(e) { if (!e || !e.postData) { return responseJson(\u0026#39;not set request body\u0026#39;) } } 翻訳部分を追加します。\n今回のbodyは、{\u0026quot;title\u0026quot;: \u0026quot;title\u0026quot;, \u0026quot;summary\u0026quot;:\u0026quot;I am running\u0026quot; }のような想定です。\nfunction doPost(e) { if (!e || !e.postData) { return responseJson(\u0026#39;not set request body\u0026#39;) } const params = JSON.parse(e.postData.getDataAsString()); if (params.title \u0026amp;\u0026amp; params.summary) { const title = t(params.title) const summary = t(params.summary) return responseJson({title_en: title, summary_en: summary}); } else { return responseJson(\u0026#39;not set title or summary\u0026#39;) } } function t(value){ return LanguageApp.translate(value, \u0026#34;en\u0026#34;, \u0026#34;ja\u0026#34;); } リクエストを投げる\ncurl -L -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;title\u0026#34;:\u0026#34;title\u0026#34;, \u0026#34;summary\u0026#34;:\u0026#34;I am running\u0026#34;}\u0026#39; https://script.google.com/macros/s/aaaaaaaaaa/exec "
            }
    
        ,
            {
                "id": 53,
                "href": "https://www.if-blog.site/posts/hugo/hugo-related-posts/",
                "title": "Hugoの記事に関連記事を表示させる",
                "section": "posts",
                "date" : "2021.07.18",
                "body": "記事の最後に関連記事を表示する方法が簡単になってた。\nhttps://gohugo.io/content-management/related/\nこれで関連記事を取れる\n.Site.RegularPages.Related 以下のような感じで記事の最後につけるとおｋ\n{{ $related := .Site.RegularPages.Related . | first 5 }} {{ with $related }} \u0026lt;h3\u0026gt;See Also\u0026lt;/h3\u0026gt; \u0026lt;ul\u0026gt; {{ range . }} \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;{{ .RelPermalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; {{ end }} \u0026lt;/ul\u0026gt; {{ end }} 関連記事の設定も少しできるみたい。\n[related] includeNewer = false threshold = 80 toLower = false [[related.indices]] name = \u0026#34;keywords\u0026#34; weight = 100 [[related.indices]] name = \u0026#34;date\u0026#34; weight = 10 設定はそれぞれ、\nthreshold\nしきい値。0-100の値。低くするとよりマッチするけど、あんまり関連性無くなる\nincludeNewer\n現在の記事より新しい記事を含めるかどうか。新しい記事を追加すると関連記事一覧も変わる。\ntoLower\nindexesとクエリに小文字を含めるかどうか。より正確な関連記事が出る場合もある。\n"
            }
    
        ,
            {
                "id": 54,
                "href": "https://www.if-blog.site/posts/python/open-ai-gym-quick/",
                "title": "Open AI Gymをとりあえず動かす",
                "section": "posts",
                "date" : "2021.07.14",
                "body": "Open AI Gymのquick startです。とりあえず動かしてみます。\ndocument\nhttps://gym.openai.com/docs/\nInstall pip install gym Run import gym env = gym.make(\u0026#39;CartPole-v0\u0026#39;) env.reset() for _ in range(1000): env.render() env.step(env.action_space.sample()) # take a random action env.close() gym.make()で環境を選択 env.reset()で環境を初期化 env.render()で描画 env.action_space.sample()が行動。今回はランダム env.step()で行動を引数にして状態遷移 今回は環境にCartPoleを選択。Cartの上でPoleを倒さないように動くやつ。\n環境はClassic controlだと特に描画の設定などせずに使えたはず。\n用意されてる環境はここ\nhttps://gym.openai.com/envs/#classic_control\nCartPoleで環境に対して取れる行動は、左に動く、動かない、右に動くの三種類。\nCartPoleでの環境から得られる情報は、potisionとvelocity\nそれぞれgithubを参照\nhttps://github.com/openai/gym/wiki/MountainCar-v0#actions\ncustome env 環境を作成できる\nhttps://github.com/openai/gym/blob/master/docs/creating-environments.md\n環境用にディレクトリを作成\ngym-foo/ README.md setup.py gym_foo/ __init__.py envs/ __init__.py foo_env.py foo_extrahard_env.py setup.pyと__init__.pyを環境に合わせて書いておく。\ngithubのドキュメント通り。\nEnvを継承して、いろいろと実装する。\nimport gym from gym import error, spaces, utils from gym.utils import seeding class FooEnv(gym.Env): metadata = {\u0026#39;render.modes\u0026#39;: [\u0026#39;human\u0026#39;]} def __init__(self): ... def step(self, action): ... def reset(self): ... def render(self, mode=\u0026#39;human\u0026#39;): ... def close(self): ... インストールすると、\npip install -e gym-foo Cartpoleみたいに呼び出せる\ngym.make(\u0026#39;gym_foo:foo-v0\u0026#39;) gym.envs.classic_control.rendering以下にclassic_controlで使ってる便利なのが入ってるので参考になりそう。\nfrom gym.envs.classic_control import rendering self.viewer = rendering.Viewer(screen_width, screen_height) circle = rendering.make_circle() "
            }
    
        ,
            {
                "id": 55,
                "href": "https://www.if-blog.site/posts/nlp/end-to-end_text_classification_via_image-based_embedding_using_character-level_networks/",
                "title": "End-to-End Text Classification via Image-basedEmbedding using Character-level Networks",
                "section": "posts",
                "date" : "2020.12.30",
                "body": " 日本語、中国語、タイ語など、形態素解析に基づく単語境界を持たない言語の分析や理解のためには、単語の埋め込みの前に適切な単語分割を行うことが望まれるが、これらの言語では本質的に困難である。しかし、これらの言語では、それは本質的に困難である。近年、ディープラーニングに基づく様々な言語モデルの進歩は目覚ましく、文字レベルの特徴量を利用した手法の中には、このような困難な問題を回避することに成功しているものもある。しかし，上記の言語の文字レベル特徴量をモデルに与えると，文字種の数が多いためにオーバーフィッティングが発生することが多い．本論文では，これらの問題を解決するために，文字エンコーダを用いた文字レベル畳み込みニューラルネットワークであるCE-CLCNNを提案する．提案するCE-CLCNNは、エンドツーエンド学習モデルであり、画像ベースの文字エンコーダー、すなわち、対象文書中の各文字を画像として扱う。様々な実験を行った結果，提案するCE-CLCNNは，視覚的にも意味的にも類似した文字に対しても密接に埋め込まれた特徴を捉えることができ，いくつかのオープン文書の分類タスクにおいて最先端の結果を達成することを確認した．本論文では、Wikipediaのタイトル推定タスクを用いたCE-CLCNNの性能を報告し、内部挙動を分析した。\nwww.DeepL.com/Translator（無料版）で翻訳しました。\nII. CE-CLCNN CE-CLNCCは、2つの異なるCNNからなる。\n１つ目のCNNは、文字画像を入力としcharacter encoderとして振る舞う。 ２つ目のCNNは、document classificationに用いられる。\nA. Character encoder by CNN 入力文字列を36×36 pixelの画像に変換する。\nLayer # CE configuration 1 Conv(k=(3, 3), o=32) → ReLU 2 Maxpool(k=(2, 2)) 3 Conv(k=(3, 3), o=32) → ReLU 4 Maxpool(k=(2, 2)) 5 Conv(k=(3, 3), o=32) → ReLU 6 Linear(800, 128) → ReLU 7 Linear(128, 128) → ReLU B. Document classifier by CLCNN Layer # CLCNN configuration 1 Conv(k=(1, 3), o=512, s=3) → ReLU 2 Conv(k=(1, 3), o=512, s=3) → ReLU 3 Conv(k=(1, 3), o=512) → ReLU 4 Conv(k=(1, 3), o=512) 5 Linear(5120, 1024) 6 Linear(1024, # classes) データセット Wikipediaタイトルデータセットを使う。 これには、以下の12個のラベルが含まれており、これを文章分類のラベルに使う。\nGeography, Sports, Arts, Military, Economics, Transportation, Health Science, Education, Food Culture, Religion and Belief, Agriculture and Electronics.\nAnalysis of Character Encoder cnn encoderを用いて文字の特徴量を抽出した。特徴量は、意味が類似している文字ごとに近い値となった。 形状特徴を明示的にを学習させたわけではなく、文章分類タスクを行うことが原因と考えられる。\n"
            }
    
        ,
            {
                "id": 56,
                "href": "https://www.if-blog.site/posts/nlp/enhanced-character-embedding-for-chinese-named-entity-recognition/",
                "title": "Enhanced character embedding for Chinese named entity recognition",
                "section": "posts",
                "date" : "2020.12.30",
                "body": "Enhanced character embedding for Chinese named entity recognition https://journals.sagepub.com/doi/pdf/10.1177/0020294020952456\n従来の名前付き実体認識方法では、主に手作業による特徴の応用を模索しています。現在では、ディープラーニングの普及に伴い、名前付き実体認識のための深い特徴を捕捉するためにニューラルネットワークが導入されている。しかし、既存の手法の多くは、現代のコーパスを対象としたものに限られている。古文献の名前付き実体認識は、名前が時間の経過とともに進化してきたために困難である。本論文では、文字や筆跡の特徴を探ることで実体認識を試みる。トランスフォーマーとストロークからの双方向エンコーダ表現に基づいて、ECEMと名付けられた拡張文字埋め込みモデルを提案する。第一に、ECEMは単語の文脈に応じて動的に意味ベクトルを生成することができる。第二に、提案するアルゴリズムは、中国語の単語の形態素レベルの情報を導入する。最後に、強化された文字埋め込みは、学習のために双方向性の長期短期記憶条件付きランダムフィールドモデルに供給される。提案したアルゴリズムの効果を調べるために，実験 を古文と現代文の両方のコーパスで実施した．その結果，我々のアルゴリズムが非常に有効であることがわかった．従来のものと比較して、強力である。\nThe proposed recognition algorithm 提案アルゴリズムであるECEMでは、前処理、embedding、BiLSTM Layer、CRF Layerの４つのコンポーネントからなる。\nembeddingでは、cw2vecとBERTを用いる。 NERには、BIO schemeを用いる。\nData pre-processing 固有表現抽出/the named entity recognition (NER)\nEnhanced character embedding BERT BiLSTM layer CRF layer input sequence: X = (x_1, x_2, ... x_n)\npredictin sequence: y = (y_1, y_2, ... y_n)\nthe equence yの確率は、softmaxを使って計算される。\np(y | X) softmax(e^f(X, y)) ここで、f(X,y)は、以下の用に計算される。\nf(X,y)= ただし、PはBiLSTMの出力した行列である。\n"
            }
    
        ,
            {
                "id": 57,
                "href": "https://www.if-blog.site/posts/github/workflow-dispatch/",
                "title": "GitHub Actionsを手動実行する",
                "section": "posts",
                "date" : "2020.07.19",
                "body": "GitHub Actionsをcronとして定期実行していたが、動作の確認のために手動実行もしたかったのでメモ\n結論 onワークフロー構文に、workflow_dispatchを指定する\nname: Manually triggered workflow on: workflow_dispatch: workflow_dispatchのdoc\nhttps://docs.github.com/ja/actions/reference/events-that-trigger-workflows#workflow_dispatch\n"
            }
    
        ,
            {
                "id": 58,
                "href": "https://www.if-blog.site/posts/android/bottom-sheet/",
                "title": "Bottom Sheetをつける",
                "section": "posts",
                "date" : "2020.03.01",
                "body": "Google Mapで下から引っ張ると出てくるやつ。\n調べたら以外と簡単にできた。\ndependenciesを追加。\ndependencies { implementation \u0026#39;com.google.android.material:material:1.0.0\u0026#39; } 例\n\u0026lt;androidx.coordinatorlayout.widget.CoordinatorLayout ..... \u0026gt; \u0026lt;LinearLayout android:id=\u0026#34;@+id/bottomSheetLayout\u0026#34; android:layout_width=\u0026#34;match_parent\u0026#34; android:layout_height=\u0026#34;match_parent\u0026#34; app:behavior_hideable=\u0026#34;false\u0026#34; app:behavior_peekHeight=\u0026#34;56dp\u0026#34; app:layout_behavior=\u0026#34;@string/bottom_sheet_behavior\u0026#34;\u0026gt; 中身 \u0026lt;/LinearLayout\u0026gt; \u0026lt;/androidx.coordinatorlayout.widget.CoordinatorLayout\u0026gt; bottm sheetにしたいコンテンツ、今回の例ではLinearLayout、のlayout_behaviorを以下のように指定。\napp:layout_behavior=\u0026quot;@string/bottom_sheet_behavior\nCoordinatorLayoutの一番下に配置する。\n開いたり閉じたりをコードで制御する場合は、activity/fragmentの中で以下のように設定する。\nbottomSheetBehavior = BottomSheetBehavior.from(bottomSheetLayout) // 隠す bottomSheetBehavior.state = BottomSheetBehavior.STATE_COLLAPSED // 開く bottomSheetBehavior.state = BottomSheetBehavior.STATE_EXPANDED コールバックも設定できる。\nbottomSheetBehavior.setBottomSheetCallback(object: BottomSheetBehavior.BottomSheetCallback(){ override fun onSlide(p0: View, p1: Float) { TODO(\u0026#34;not implemented\u0026#34;) //To change body of created functions use File | Settings | File Templates. } override fun onStateChanged(p0: View, p1: Int) { TODO(\u0026#34;not implemented\u0026#34;) //To change body of created functions use File | Settings | File Templates. } }) 参考\nhttps://blog.mindorks.com/android-bottomsheet-in-kotlin\nhttps://qiita.com/fumiyakawauso/items/1d0fa27aa508a3e0570a\n"
            }
    
        ,
            {
                "id": 59,
                "href": "https://www.if-blog.site/posts/android/android-introduction/",
                "title": "Webアプリケーションしか作ったことがない人がAndroidの開発を始めたときの情報収集の流れ",
                "section": "posts",
                "date" : "2020.02.28",
                "body": "これまでフロントエンド/バックエンドしか作ったことなく、ようやくAndroid開発に入門したのだが、\n始めのころの情報収集が意外と大変だったので、同じような境遇の人の為にまとめておく。\n上からみていくと、フロントエンド/バックエンドしか開発したことがなくても、流れが掴みやすいかと\nライブラリ Android 9.0(API 28)以降では、Android Xというライブラリを使っていく。\nAndroid Xは、Android Jetpackという、ライブラリ群の1部らしい。\nAndroid Jetpackとは、\nJetpack は、高品質の Android アプリをデベロッパーが簡単に作成するための一連のライブラリ、ツール、およびガイダンスです。\nhttps://developer.android.com/jetpack?hl=ja\nJetpackについては、以下の記事が参考になった。\nhttps://qiita.com/k_masa777/items/c01c1de6ac763ce5c075\n検索する際には、古いライブラリの記事が結構出てくるので、Android Xかどうか注意\nアーキテクチャ MVCやクリーンアーキテクチャなどのアーキテクチャがAndroidでも存在する。\nModel-View-ViewModelで構成されるMVVMという構成が主流っぽい\nView: 画面表示\nViewModel: Modelから値を取得、Viewに表示する値を管理\nModel: データモデル、ビジネスロジック\nここでのモデルは、DDDのモデルよりも少し多機能な感じ\nアーキテクチャを一通り紹介していてわかりやすかった。\nhttps://employment.en-japan.com/engineerhub/entry/2018/01/17/110000\nMVVMの実装する上でわかりやすかった\nhttps://qiita.com/Tsutou/items/69a28ebbd69b69e51703\nコードサンプル MVVM意外にもクリーンアーキテクチャなど、様々なサンプルがあってわかりやすい\nhttps://github.com/android/architecture-samples\nhttps://github.com/android10/Android-CleanArchitecture-Kotlin\nロジック Androidでは、APIを叩く、画面表示する、ホスト(Android端末)のリソースを管理するなど、やることが多い。\nまずは、Webアプリケーションにおける、バックエンドに当たる部分について、まとめる。\nHTTP Request http clientは、retrofitというライブラリがよく使われてる様子\nhttps://square.github.io/retrofit/\n参考\nhttps://qiita.com/SYABU555/items/3b280a8e81d2cc897383\n非同期処理 APIは非同期で実行したい。Androidでは、UIスレッドと呼ばれる、スレッドがメインスレッドとして動作する。\nUIの描画など、UIスレッドで実行しなければならないものがある。\n別スレッドを起動することも可能だが、非同期処理を行いたい場合は、コルーチンを使うのが主流。\nコルーチンは、スレッドを作るより軽量で、中断可能な処理を行えるもの。\n調べると色々出てくるが、使い方については、以下が参考になった。\nhttps://kotlinlang.org/docs/reference/coroutines/basics.html\nhttps://qiita.com/kawmra/items/ee4acb7db61f70dec9a8\nResult APIのresponseを受け取る際には、成功/失敗をいい感じにハンドリングしたい。\nkotlinでは、Success/Failureの2つの型を持てるResult型が存在する。\nhttps://satoshun.github.io/2018/12/result/\nhttps://qiita.com/kencharos/items/6fd0a9e92363b08c0340\n使ったことないけど、Eitherも使えるライブラリもある\nhttps://arrow-kt.io/\nDB Androidでは、SQLiteが使えます。APIをキャッシュするような使い方も可能です。\nAndroid JetpackにRoomというライブラリがあるので、それを使います。\n単純な使い方はググればいくらでも出てくるので、それ以外の通常の開発に必要なものを羅列します。\nオブジェクト間のリレーションを定義する\nhttps://developer.android.com/training/data-storage/room/relationships?hl=ja\nRoomで1対多を実現する\nhttps://medium.com/@star_zero/room%E3%81%A71%E5%AF%BE%E5%A4%9A%E3%82%92%E5%AE%9F%E7%8F%BE%E3%81%99%E3%82%8B-fa27615345ba\n型コンバーターを使用する\nhttps://developer.android.com/training/data-storage/room/referencing-data?hl=ja#type-converters\n特定の列をインデックスに登録する\nhttps://developer.android.com/training/data-storage/room/defining-data?hl=ja#column-indexing\nAndroidに保存した、データをみたいとき Android Studioの(通常)右下にあるDevice File Explorerを選択。\nエミュレーター内のファイルを見ることができるので、data/ data/“package_name”/databases にある\n以下のファイル群を保存。\ndatabase_name database_name-shm database_name-wal SQLite用のクライアントを使って中身を確認することができる。\nクライアント\nhttps://sqlitebrowser.org/dl/\nUI Androidでは、基本的に、Activieにロジックを記述し、Activityとxmlで記述されたlayoutが1対1で対応し1つの画面を構成する。\nこれでは、Activityが肥大化してしまうので、MVVMでは、1つのActivityに複数の画面(Fragment)を持たせる。\nAcitivtyでは画面の構成、Fragmentでは各画面のロジックのみを持つ。\nまた、Fragmentは、 コンポーネントの用に複数のFragmentを用いて、1つの画面を構成するという使い方もできる。\nactivityとfragmentはそれぞれ異なるライフサイクルを持ち、\n例えば、画面の回転や、List表示する為のデータの保持などの場合に考慮する必要がある。\nactivityのライフサイクル https://developer.android.com/guide/components/activities/activity-lifecycle?hl=ja#alc\nfragmentのライフサイクル https://developer.android.com/guide/components/fragments?hl=ja#Creating\nLayout コンポーネントを単純に縦横に並べるだけなら、LinearLayoutで良い。\n細かく指定したい場合は、ConstraintLayoutが便利。\n位置の指定の仕方に始めは、戸惑うが以下がわかれば簡単。\napp:layout_constraint[自分の辺]_to[相手の辺]Of=\u0026#34;@+id/相手のid\u0026#34; さらに詳しくは、以下の記事が参考になった。\n[Android] ConstraintLayout レイアウト逆引きまとめ\nhttps://qiita.com/tktktks10/items/62d85dabac4bdb8c1f94\nデータバインディング Android Jetpackの提供する、データバインディングライブラリを使う。\nWebアプリケーションでは、HTMLとロジック間でデータをバインディングしリアルタイムで表示を変えたりなどを行う。\nデータバインディングライブラリを使うことで、同様のことが実現できる。\nデータバインディング\nhttps://developer.android.com/topic/libraries/data-binding/?hl=ja\nアイコン res \u0026gt; vector \u0026gt; .... でアイコンの追加が行える。\n意外と出てこなくてハマった。\nfont-awesome使う場合 以下の記事が参考になった\nhttps://www.androidhive.info/2019/06/use-font-awesome-icons-android/\nhttps://fontawesome.com/icons?d=gallery\nアニメーション メニューのアコーディオンや、移動など単純なアニメーションは、\nres/anim/sample.xmlを配置し、実現できる。\n移動アニメーションの例\nhttps://akira-watson.com/android/translate-animation.html\nPaging リストを描画し、無限スクロールさせたい場面は多いと思う。\nViewModelとPaging library、RecyclerViewを使って実現できる。\n以下の記事が参考になった。\nAndroid Jetpack 初級 ( Paging library + LiveData + Retrofitで、簡単無限スクロール)\nhttps://qiita.com/Tsutou/items/bae47cf55292f3349284#livedata\nPaging Library + API でページング処理\nhttps://medium.com/@star_zero/paging-library-api-%E3%81%A7%E3%83%9A%E3%83%BC%E3%82%B8%E3%83%B3%E3%82%B0%E5%87%A6%E7%90%86-5e68543e7031\nhttps://github.com/STAR-ZERO/paging-retrofit-sample\nコードサンプル\nhttps://github.com/Tsutou/PagingLiveData\nAPIを叩きつつ、DBにキャッシュする例\nhttps://medium.com/@star_zero/paging-library-api-%E3%83%87%E3%83%BC%E3%82%BF%E8%BF%BD%E5%8A%A0-%E5%89%8A%E9%99%A4-eae42d0853cb\nfirebase Authenticationやデータベースなどが簡単に使える。\nGoogleアカウントでのログインを行いたい場合など便利\nAndroid プロジェクトに Firebase を追加する\nhttps://firebase.google.com/docs/android/setup?hl=ja\nリリース https://developer.android.com/studio/publish/preparing?hl=ja\n"
            }
    
        ,
            {
                "id": 60,
                "href": "https://www.if-blog.site/posts/android/book-reports-firebase/",
                "title": "ボタン以外のコンポーネントに波紋エフェクト(ripple effect)つける",
                "section": "posts",
                "date" : "2020.02.28",
                "body": "TextViewなどのbackgroudに以下を指定すればおk\nandroid:background=\u0026quot;?attr/selectableItemBackground\u0026quot;\nbackgroundで色などをすでに指定している場合は、以下のようにする。 foregroundに指定する。\n\u0026lt;Button android:layout_width=\u0026#34;match_parent\u0026#34; android:layout_height=\u0026#34;200dp\u0026#34; android:padding=\u0026#34;0dp\u0026#34; android:background=\u0026#34;#bbffbb\u0026#34; android:foreground=\u0026#34;?android:selectableItemBackground\u0026#34; android:text=\u0026#34;Hello World!\u0026#34;/\u0026gt; "
            }
    
        ,
            {
                "id": 61,
                "href": "https://www.if-blog.site/posts/edgwbs/book-storage-vue/",
                "title": "Vue.js+typescriptでVuetifyを使いマテリアルデザインに入門する",
                "section": "posts",
                "date" : "2019.11.09",
                "body": "はじめに Vuetifyは、Vue.jsで使えるマテリアルデザインのフレームワークです。\nhttps://vuetifyjs.com/ja/\nマテリアルデザインに入門すべく、Vuetifyを使って読んだ本を管理するサービスを作ってみました。\nBook Storage\n簡単にマテリアルデザインを使えるVuetifyですが、以下のような特徴があります。\nボタンやカードなどのマテリアルデザインのコンポーネントが用意してあり、簡単に利用できる flexboxをラップしたグリッドシステムがあり、レスポンシブ対応が容易 コンポーネントとグリッドシステムを用いることで、cssを(ほとんど)書く必要がない マテリアルデザインの仕様であるフォント(Roboto Font)、アイコンが利用できる ドキュメントが充実しており、基本的にドキュメントを見ると簡単に使えるという印象です。ドキュメントはjavascriptで書かれているため、typescriptで使う場合には、読み換える必要があります。\nサポートブラウザはドキュメントを参照\nhttps://vuetifyjs.com/ja/getting-started/quick-start#supported-browsers\n2019年7月にv2.0.0がリリースされ、旧バージョン(v1.5.0)から記法が大きく変わったので、その際に注意する点も含め紹介していきます。\nインストール vue cli 3を使うと楽ということなのでvue cliを使ってインストールします。\n$ vue --version 3.9.1 typescriptのプロジェクトを作成します.\n$ vue create my-app vuetifyを追加する.\n$ cd my-app $ vue add vuetify すると以下のような選択肢が表示される. Defaultを選択すると、ツリーシェイキングなどの機能が追加された形で追加される.\n$ ? Choose a preset: (Use arrow keys) $ \u0026gt; Default (recommended) $ Prototype (rapid development) $ Configure (advanced) ツリーシェイキングについては、こちらの記事がわかりやすかったです。\nhttps://qiita.com/genshun9/items/4a00aa6c709b9f024821\nツリーシェイキングはwebpack4から利用でき、vue cli3はwebpack4上に構築されるため、vue cli3を利用していると自動で利用できるようになります。vuetifyのバージョンは2.0.0。\npackage.json\n\u0026#34;dependencies\u0026#34;: { \u0026#34;vuetify\u0026#34;: \u0026#34;^2.0.0\u0026#34;, .. }, フォントの追加も行います。\n$ npm install @mdi/font -D vuetifyを追加すると、新たにsrc/plugins/vuetify.tsが追加されます。\nimport Vue from \u0026#39;vue\u0026#39;; import Vuetify from \u0026#39;vuetify/lib\u0026#39;; Vue.use(Vuetify); export default new Vuetify({ icons: { iconfont: \u0026#39;mdi\u0026#39;, }, }); ここで起動すると、コンパイルエラーが出ていたので、tsconfigに以下を追加すると、うまく動くようになりました。\n\u0026#34;compilerOptions\u0026#34;: { \u0026#34;types\u0026#34;: [\u0026#34;vuetify\u0026#34;], 既存のプロジェクトへの追加 yarnもしくはnpmを使いインストールします。\n$ yarn add vuetify // OR $ npm install vuetify --save 上記のsrc/plugins/vuetify.tsを新たに作成。\n詳しくはドキュメントを参考\nhttps://vuetifyjs.com/ja/getting-started/quick-start\nGrid System vuetifyを使うには、全体を\u0026lt;v-app\u0026gt;で囲みます。\nはじめに読み込まれるコンポーネントに以下のように書けば良いかと思います。\n\u0026lt;template\u0026gt; \u0026lt;v-app\u0026gt; \u0026lt;v-header\u0026gt;\u0026lt;/v-header\u0026gt; \u0026lt;router-view/\u0026gt; \u0026lt;v-footer\u0026gt;\u0026lt;/v-footer\u0026gt; \u0026lt;/v-app\u0026gt; \u0026lt;/template\u0026gt; 基本的には、以下の例のようにv-container、v-row、v-colの順番で囲む形になります。旧バージョンで利用していたv-flexやv-layoutなどの記法の代わりに、v-row、v-colを使うので注意です。\n\u0026lt;template\u0026gt; \u0026lt;v-container\u0026gt; \u0026lt;v-row\u0026gt; \u0026lt;v-col cols=12\u0026gt; コンテンツ \u0026lt;/v-col\u0026gt; \u0026lt;/v-row\u0026gt; \u0026lt;/v-container\u0026gt; \u0026lt;/template\u0026gt; v-containerはコンテンツを中央揃えおよび水平方向にパディングにします。\ngridは、Bootstrapと同様、12個のgridがあります。\n\u0026lt;v-col cols=12\u0026gt; ... \u0026lt;/v-col\u0026gt; colsプロパティを使い、gridを指定します。また、スマホ、PC用など、5つのブレークポイントが用意されており、colsに加え、ブレークポイントごとにgridが設定可能です。\n\u0026lt;v-col cols=\u0026#34;12\u0026#34; lg=\u0026#34;6\u0026#34; md=\u0026#34;6\u0026#34; sm=\u0026#34;6\u0026#34;\u0026gt; ... \u0026lt;/v-col\u0026gt; 各ブレイクポイントは以下\ngridの指定でハマったポイントは、こちらの記事を参考\nhttps://qiita.com/iss-f/items/a86586de095368cdd9a3\n詳しい部分は、grid system ドキュメントを参照\nhttps://vuetifyjs.com/ja/components/grids\nコンポーネント マテリアルデザイン仕様のカードやボタンなどのコンポーネントが用意されています. 色やサイズ、アニメーションなどcssを書かずに設定ができます。 各コンポーネントが受け取れるプロパティやイベントなどは、ドキュメントに詳しく記載してあります。\nここでは、 基本的なコンポーネントをいくつか挙げていきます。\ncard マテリアルデザインでよく見るカードは以下のように作ることができます。\n\u0026lt;v-card max-width=\u0026#34;344\u0026#34; class=\u0026#34;mx-auto\u0026#34; \u0026gt; \u0026lt;v-card-title\u0026gt;I\u0026#39;m a title\u0026lt;/v-card-title\u0026gt; \u0026lt;v-card-text\u0026gt;I\u0026#39;m card text\u0026lt;/v-card-text\u0026gt; \u0026lt;v-card-actions\u0026gt; \u0026lt;v-btn text\u0026gt;Click\u0026lt;/v-btn\u0026gt; \u0026lt;/v-card-actions\u0026gt; \u0026lt;/v-card\u0026gt; v-card-title、v-card-textには、それぞれタイトル用本文用に、フォントサイズやpaddingが設定してあります。 v-card-actionには、クリックイベントなどのアクションを設定します。\nButton v-btnタグを使い、波紋の広がるアニメーションがついたボタンが作成できます。\nシンプルな例は以下の通り。\n\u0026lt;v-btn medium color=\u0026#34;warning\u0026#34; dark\u0026gt;Normal Button\u0026lt;/v-btn\u0026gt; small, medium などのプロパティでボタンサイズが指定可能です。bootstrap同様、info、primaryなどで色の指定も可能。\nまた、その他タグを使い細かい色の指定も可能です。これらのタグは、ボタン以外のコンポーネントでも使用可能です。\nその他の色は、ドキュメントを参照\nhttps://vuetifyjs.com/ja/styles/colors\noutlined button 色抜きのボタンにする場合には、outlinedプロパティを使います。\n\u0026lt;v-btn medium color=\u0026#34;warning\u0026#34; dark outlined\u0026gt;Normal Button\u0026lt;/v-btn\u0026gt; v2.0.0からoutlinedとなり、outlineプロパティは廃止されたので注意です。\nfloating button fabプロパティを追加することで、フローティングボタンを作成することができます。\n\u0026lt;v-btn fab dark color=\u0026#34;indigo\u0026#34;\u0026gt; \u0026lt;v-icon dark\u0026gt;mdi-plus\u0026lt;/v-icon\u0026gt; \u0026lt;/v-btn\u0026gt; loading button loadingプロパティにboolean値を設定することで、trueの場合にローディングアニメーションが現れるようにすることもできます。\n\u0026lt;v-btn :loading=\u0026#34;loading\u0026#34; medium color=\u0026#34;warning\u0026#34; dark \u0026gt; Loading Button \u0026lt;/v-btn\u0026gt; その他の設定は、ドキュメントを参照\nhttps://vuetifyjs.com/ja/components/buttons\nPagination \u0026lt;v-pagination v-model=\u0026#34;page\u0026#34; :length=\u0026#34;15\u0026#34; :total-visible=\u0026#34;7\u0026#34; @input=\u0026#34;paginate()\u0026#34; \u0026gt;\u0026lt;/v-pagination\u0026gt; v-modelに現在のページ番号を設定します。lengthプロパティは全ページ数、total-visibleで表示するページングボタンの数を指定。クリックイベントで、ページングボタン押下時の挙動を設定して使うことが多いかと思います。\nまとめ Vue.jsのマテリアルデザインのフレームワークである、Vuetifyを使ってサービスを作って見ました。簡単にマテリアルデザインに入門できて、おすすめです。\nここで紹介した以外にも便利なコンポーネントが多数用意してあるのでドキュメント読んでみてください。\n"
            }
    
        ,
            {
                "id": 62,
                "href": "https://www.if-blog.site/posts/gatsby/gatsby-intro/",
                "title": "技術ブログ用にGatsbyを整える",
                "section": "posts",
                "date" : "2019.10.14",
                "body": "これまでHugoを使っていたが、gatsbyを使ってみようと思います。 https://www.gatsbyjs.org/\ngatsbyは、react製の静的サイトジェネレーターです。PWAやOGPなどがあらかじめ設定してあり(themeによる？)便利です。また、記事などの任意のデータ取得のためのインターフェスとしてGraphQLを使用しているのが特徴です。(GraphQLは別途用意する必要はなく、gatsbyのbuild時に作成されます。)\nGraphQL Concepts\n静的サイトジェネレーター一覧はここがすごいわかりやすかった\nhttps://www.staticgen.com/\ngatsbyを選んだのは、reactに入門したかったのと、Hugoのテーマがスマホ対応していなかったので手を加えたかったということでタイミングがよかったからです。\n以下に、今回使ったものを備忘録として残しておきます。\nここにあげるもの一通り導入し、デザインを入れれば、基本的な技術ブログが作れるかと思います。\ntheme 今回は、テーマに変更を加える形で作ります。\nテーマは以下から選びます。 https://www.gatsbyjs.org/starters/?v=2\n以下のコマンドでテーマ入った状態で、gatsbyのプロジェクトが生成されます。\n$ gatsby new gatsby-starter-default\nhttps://github.com/gatsbyjs/gatsby-starter-default\ngatsbyのチュートリアルに目を通しつつ、themeに手を加えていきます。\nMarkdownで記事を書く 以下のページを参考にしました。\nhttps://www.gatsbyjs.org/docs/adding-markdown-pages/\nある程度試して使い方がわかれば、markdownに対応したthemeがあるので、そちらを使った方が楽かと思います。\n以下のようなqueryで一覧が取得できます。\n{ allMarkdownRemark { edges { node { html headings { depth value } frontmatter { # Assumes you\u0026#39;re using title in your frontmatter. title } } } } } 日付順にソート、limit、skipを与えることで、ページング処理もできます。\nallMarkdownRemark( sort: { fields: [frontmatter___date], order: DESC } limit: $limit skip: $skip ) { . . icons 以下のものがすぐに使えてよかったです。\nhttps://react-icons.netlify.com/#/\n(emacsがないのが解せぬ…)\nコードのハイライト 技術ブログなので、コードのハイライトは必須です。Gatsbyで使えるものがいくつかあるようですが、自分の使う言語に対応していたので以下を使いました。\nPrism: https://prismjs.com/\nPrismを使ったGatsbyのプラグイン: https://www.gatsbyjs.org/packages/gatsby-remark-prismjs/\ngatsby-config.jsには、プラグインの記事に従ってプラグインを追加。\nまた、gatsby-browser.jsに以下を追加\nrequire(\u0026quot;prismjs/themes/prism-solarizedlight.css\u0026quot;);\n行数を表示するには、更に以下を追加\nrequire(\u0026quot;prismjs/plugins/line-numbers/prism-line-numbers.css\u0026quot;);\nその他のthemeもあるので好きなのを使う。単一のラインのハイライトや、ハイライト開始/終了の行数指定も可能だが、自前でcssを調節する必要がある。\nコードハイライトにファイル名やタイトルを表示させたかったので、以下を使いました。\nhttps://www.gatsbyjs.org/packages/gatsby-remark-code-titles/?=gatsby-remark-code-titles\n例\n{ \u0026#34;test\u0026#34;: \u0026#34;test\u0026#34; } katex 記事中に数式を書くことがあるので、必要でした。以下のプラグインを使うと簡単にできます。\nhttps://www.gatsbyjs.org/packages/gatsby-remark-katex/\n\\tags 使うとparse errorで怒られる。(v3.1.13)\n前に使って重い印象があったので、使いませんでしたが、mathjaxもあるみたいです。\nhttps://www.gatsbyjs.org/packages/gatsby-remark-mathjax/\n画像表示 markdonwの記事に画像を追加します。 一般的には、![画像タイトル](URL)のように画像を追加しますが、gatsbyではプラグインを入れる必要があります。\nhttps://www.gatsbyjs.org/docs/working-with-images-in-markdown/\nただ、markdownで画像をリサイズして表示したいときは、どうするのかわからない。自前で作るしかない？？\n目次 プラグインgatsby-remark-tocを使うことで目次をつけることができます。ただ、記事のトップにしかつけることができず、Qiitaのようなサイドバーに目次をつけたかったので、以下のようにしました。\nqueryにtableOfContentsを加えると、目次が取得できます。\n例\nmarkdownRemark(fields: { slug: { eq: $slug } }) { html frontmatter { title date(formatString: \u0026#34;YYYY-MM-DD\u0026#34;) tags } tableOfContents } } 取得したものを表示します。\nconst tableOfContens = props.data.markdownRemarks.tableOfContents; \u0026lt;div dangerouslySetInnerHTML={{__html: tableOfContens}}/\u0026gt; 参考\nhttps://nakawork.net/posts/gatsby-toc\nスクロールで目次のハイライトもスクロールも試したかったけど、それはまた今度\ngoogle analytics https://www.gatsbyjs.org/packages/gatsby-plugin-google-analytics/\ngoogle adsense $ npm install --save gatsby-plugin-google-adsense // In your gatsby-config.js file plugins: [ { resolve: `gatsby-plugin-google-adsense`, options: { publisherId: `ca-pub-xxxxxxxxxx` }, }, ] プロジェクトのrootディレクトリにstaticディレクトリを作成。\nstaticディレクトリ以下に、CNAMEファイル、ads.txtを追加。\nサイトマップ 以下のプラグインを使うことで、build時にsitemap.xmlが生成されます。\nhttps://www.gatsbyjs.org/packages/gatsby-plugin-sitemap/\n関連記事の表示 プラグインがないみたいです。以下の記事が参考になりそうです。今後試してみます。\n参考\nhttps://khalilstemmler.com/articles/gatsby-related-posts-component/\ngithub pages 参考\nhttps://www.gatsbyjs.org/docs/how-gatsby-works-with-github-pages/\nsubdomainの場合は、以下の手順\ngh-pagesをインストール\n$ npm install gh-pages --save-dev\n以下を追加\n{ \u0026#34;scripts\u0026#34;: { \u0026#34;deploy\u0026#34;: \u0026#34;gatsby build \u0026amp;\u0026amp; gh-pages -d public\u0026#34; } } デプロイ\n$ npm run deploy\nまとめ プラグインが豊富で基本的なことは全てプラグインを使えば実現できるので便利。ただ、プラグインに依存しすぎて、バージョン依存の問題とかで盛大に死にそうな未来がうっすら見える。\nGraphQLのコンセプトは面白いし使いやすいけど、コードの中にquery書くのはインデントあわなさすぎてしんどい。localhost:8000___graphqlでqueryが確認できるのでなんとか頑張れるけど、APIほしかった。\n今後は以下の記事がチューニングの参考になりそう\nGatsbyでGoogle Lighthouseで満点を取るブログを一から作る\nhttps://qiita.com/so99ynoodles/items/87e136d09644baac634c\nGatsbyプラグイン45選\nhttps://takumon.github.io/gatsby-starter-qiita/f18d04ac-9b1a-5ac8-8d43-8aa3d8f746a7/\n"
            }
    
        ,
            {
                "id": 63,
                "href": "https://www.if-blog.site/posts/edgwbs/book-reports-firebase-hosting/",
                "title": "Firebase Hostingを使ってみた",
                "section": "posts",
                "date" : "2019.10.10",
                "body": "Firebase Authenticationに続いて、Firebase Hostingも使ってみました。備忘録を残しておきます。\n今回は、Vue.jsをデプロイします。\nFirebase Hostingとは Firebase Hosting は、ウェブアプリ、静的コンテンツと動的コンテンツ、マイクロサービス向けの高速で安全性の高いホスティングを提供します。\nFirebase Hosting はデベロッパー向けの、本番環境レベルのウェブ コンテンツ ホスティングです。1 つのコマンドですばやくウェブアプリをデプロイすることができ、静的コンテンツと動的コンテンツの両方をグローバル CDN（コンテンツ配信ネットワーク）に配信できます。Firebase Hosting と Cloud Functions または Cloud Run を併用してマイクロサービスを構築し、Firebase にホスティングすることもできます。\n引用： https://firebase.google.com/docs/hosting?hl=ja\n無料で手軽にアプリケーションを公開するにはすごく便利という印象。\nFirebaseでは、転送量を超えない限り、複数アプリケーションをデプロイできるので、本番環境だけでなく、テスト環境もほしいという場合には有効かと思います。Herokuの無料枠では起動している時間の制限などで実質１アプリケーションしか動かせませんでした。\nまた、Hostingで利用できる無料のストレージが１GBですが、ブログなどのようにどんどんコンテンツが増えていくサイトでなければ十分な量だと思います。画像などがある場合は、Firebase Storageなどを利用する手もあるかと思います。\n参考\nFirebaseの料金体系\n静的サイトホスティングの為のGCS/GAE/Firebase Hosting比較\nFirebase Hostingを使って公開したアプリはこちら\n三行で読書感想文を書いて、SNSでシェアできるサービスを作った\n準備 デプロイ用のコマンドラインツールをインストールします。\n$ npm install -g firebase-tools 以下のコマンドで、firebaseにログインします。\n$ firebase login プロジェクトで、初めてfirebaseを使う場合には、以下のコマンドでfirebaseを初期化します。\n$ firebase init 初期化すると、firebase.jsonが作成されます。Hostingやstorage、functionsなどのその他のfirebaseの設定を記述します。基本的にはここで生成されたfirebase.jsonを書き換えることなく利用できます。\nデプロイ 生成されたfirebase.jsonにデプロイ用のディレクトリは、publicと設定されているので、publicにアプリをビルドします。 Vue.jsのプロジェクトの場合は、以下のようにビルド。\n$ npm run build public以下に生成された成果物は、以下のコマンドでfirebaseにデプロイすることができます。\n$ firebase deploy 複数サイトをデプロイする 本番環境とテスト環境がほしい場合などには、複数サイトをデプロイしたです。\n(別のアプリをデプロイする場合は、プロジェクト変えるよね？普通？本番/テスト以外にどう使うんだろう??)\nfirebase.jsonのhostingの項目を以下のように書き換えます。\n{ \u0026#34;hosting\u0026#34;: [ { \u0026#34;target\u0026#34;: \u0026#34;env\u0026#34;, // 自分で定義した、一意なアプリの識別子 \u0026#34;public\u0026#34;: \u0026#34;app/public\u0026#34;, // ... }, { \u0026#34;target\u0026#34;: \u0026#34;prod\u0026#34;, // 自分で定義した、一意なアプリの識別子 \u0026#34;public\u0026#34;: \u0026#34;app/public\u0026#34;, // ... } ] } 次に以下のコマンドで、target名とプロジェクトのサイト名を対応付けます。\n# $ firebase target:apply hosting target-name resource-name $ firebase target:apply hosting env myapp-blog $ firebase target:apply hosting prod myapp-blog あとは、以下のコマンドでデプロイします。\n# $ firebase deploy --only hosting:target-name $ firebase deploy --only hosting:env $ firebase deploy --only hosting:prod ドメインを設定 Hostingのサイトの管理、「ドメインを接続」からドメインが設定できます。\n手順どおりで簡単に行なえます。\n証明書が入るまでに少し時間がかかったので気長に待ちましょう。\nちなみに、CNAMEでいけるかと思ったけど無理でした。\nまとめ Firebase Hostingを使って、アプリケーションのデプロイを行いました。\nデプロイは驚くほど簡単でした。ローカルで動いているアプリケーションをそのまま、デプロイできるのが良かったです。\n"
            }
    
        ,
            {
                "id": 64,
                "href": "https://www.if-blog.site/posts/emacs/gopls-introduction/",
                "title": "emacsのgo lang用のlanguage serverをgoplsに乗り換える",
                "section": "posts",
                "date" : "2019.10.08",
                "body": "emacsでgo langを書くときに使っているlspのlanguage serverがbingoからgoplsに変わったので、emacsの設定も更新しておこうと思います。\nbingoは、golangの公式に取り込まれる形になったようですね。\nインストール ドキュメント通りです。\nhttps://github.com/golang/tools/blob/master/gopls/doc/user.md\n$ go get golang.org/x/tools/gopls@latest GO111MODULEをonにする必要がある人は、適宜onにしてあげてください。\nemacs golang用の設定は、現状以下のようになってます。\n(use-package go-mode :ensure t :commands go-mode :init (bind-key \u0026#34;C-c i\u0026#34; \u0026#39;gofmt-before-save) :config (add-hook \u0026#39;go-mode-hook #\u0026#39;lsp) ;; lsp-mode (add-hook \u0026#39;go-mode-hook (lambda () ;; company-goを使う (set (make-local-variable \u0026#39;company-backends) \u0026#39;(company-go)) (company-mode))) (add-hook \u0026#39;go-mode-hook \u0026#39;go-eldoc-setup) ) gopls for emacs setting\nhttps://github.com/golang/tools/blob/master/gopls/doc/emacs.md\n小さいプロジェクトなら、コードの補完、ドキュメント表示、定義ジャンプなど問題なく使えてます。\nただ、大きいプロジェクトとなると、補完やドキュメントを表示しようとするとタイムアウトしてしまうので、少し調整が必要みたいです。\n"
            }
    
        ,
            {
                "id": 65,
                "href": "https://www.if-blog.site/posts/edgwbs/book-reports-firebase-cloud-functions/",
                "title": "Firebase Cloud Functionsを使ってTwitterのOGPを設定する",
                "section": "posts",
                "date" : "2019.09.30",
                "body": "Vue.jsで作ったWebAppをFirebase Hostingで公開し、そこでTwitter用のOGPを設定しようとしましたが、Twitterのクローラーがjavascriptを解釈できないということで、別途方法を考える必要がありました。そこで、Firebase Cloud Functionsを使ってtwitter：OGPを設定してみようと思います。\nFirebase Cloud Functionsとは\nCloud Functions for Firebase を使用すると、Firebase 機能や HTTPS リクエストによってトリガーされたイベントに応じて、バックエンド コードを自動的に実行できます。コードは Google のクラウドに保存され、マネージド環境で実行されます。独自のサーバーを管理およびスケーリングする必要はありません。\n引用： https://firebase.google.com/docs/functions/?hl=ja\n今回は、HTTPSリクエストをトリガーに使いましたが、FirebaseのDatabaseへの書き込みやGitHubのpush/commitなどもトリガーにできるようです。色々と便利に使えそうです。\nCloud Functions で可能な処理:\nhttps://firebase.google.com/docs/functions/use-cases?hl=ja\nFirebase Cloud Functionsで使える言語は、今のところNode.js(javascript)だけのようです。Google CLoud Functionsでは、Node.js(javascript)、python、（Golang）がサポートされているので今後このあたりの言語もサポートされるのではないでしょうか。 Firebase Cloud Functions用に、javascriptのライブラリが提供されており、どうしても別の言語で書きたいという場合でなければ、不便なく使えるかと思います。\n作ったアプリはこれ。\n三行読書感想文\nOGP設定 今回は、ページによって設定したいOGPが異なるため、ページごとに動的に設定する必要があります。\nページごとに異なる情報は、Firebase CloudStoreに保存してあるので、そこから取得します。\nまた、ツイッターのクローラーからのリクエストならばOGP用のページを返却し、通常のアクセスならばページを表示するというような振り分けも行います。\nリクエストが来てからのフローは以下の用になります\nリクエストの振り分け DBからデータの取得 リダイレクト or OGP設定 シンプルなリスポンス例 200を返すシンプルなfunctionsの例は以下\nimport * as functions from \u0026#39;firebase-functions\u0026#39;; export const share = functions.https.onRequest((req, res) =\u0026gt; { res.status(200).send(\u0026#34;hello world\u0026#34;); }) リクエストの振り分け Twitterのクローラーは、User-AgentがUser-agent: Twitterbotなので、これを使います。\nUAがTwitterbotの場合はOGPを設定し返却、それ以外の場合は通常のページへリダイレクトさせるようにします。\nUAは以下の用に取得できます。\nconst ua = req.header(\u0026#39;user-agent\u0026#39;); console.log(\u0026#39;ua:\u0026#39;,ua); リクエストを振り分けるために、エンドポイントをfirebase.jsonを修正します。\n/for_twitterにアクセスが来るとFunctionsが呼ばれ、それ以外は通常のページを返します。\n\u0026#34;rewrites\u0026#34;: [ { \u0026#34;source\u0026#34;: \u0026#34;/for_twitter/*\u0026#34;, \u0026#34;function\u0026#34;: \u0026#34;for_twitter\u0026#34; }, { \u0026#34;source\u0026#34;: \u0026#34;**\u0026#34;, \u0026#34;destination\u0026#34;: \u0026#34;/index.html\u0026#34; } ] Cloud Storeからデータを取得 Firebase Admin SDKを使うと、FunctionsからDB/Storageに対し管理者としてアクセスが可能になります。もちろんですが、サーバー側空だけでフロント側から使わないように注意です。\nサーバーに Firebase Admin SDK を追加する:\nhttps://firebase.google.com/docs/admin/setup?hl=ja\n環境変数にsevice accountを追加します。\nexport GOOGLE_APPLICATION_CREDENTIALS=\u0026#34;/home/user/Downloads/service-account-file.json\u0026#34; 以下のようにSKDの初期化を行います。\nimport * as admin from \u0026#39;firebase-admin\u0026#39;; admin.initializeApp({ credential: admin.credential.applicationDefault(), databaseURL: \u0026#39;https://\u0026lt;DATABASE_NAME\u0026gt;.firebaseio.com\u0026#39; }); あとは、adminを使って、firestoreへアクセス可能です。\nconst ref = admin.firestore().collection(\u0026#39;test\u0026#39;); ref.get().then((snapshot) =\u0026gt; { // ... データの取得などはドキュメント参考\nhttps://firebase.google.com/docs/firestore?hl=ja\nTwiter OGPの設定 以下の４つが設定できるようです。\nSummary Card Summary Card with Large Image App Card Player Card Optimize Tweets with Cards\nhttps://developer.twitter.com/en/docs/tweets/optimize-with-cards/overview/abouts-cards\n今回はSummary Card with Large Imageを使います。ヘッダーに以下を追加します。\n\u0026lt;meta name=\u0026#34;twitter:card\u0026#34; content=\u0026#34;summary_large_image\u0026#34;\u0026gt;\u0026lt;/meta\u0026gt; \u0026lt;meta property=\u0026#34;og:image\u0026#34; content=\u0026#34;imageURL\u0026#34;\u0026gt; UAがTwitterbotの場合は、以下の関数を利用し、headerを設定します。\nfunction buildHtmlWithPost (id: string, siteUrl:string, imageURL) : string { return `\u0026lt;!DOCTYPE html\u0026gt;\u0026lt;head\u0026gt; \u0026lt;title\u0026gt;\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;twitter:card\u0026#34; content=\u0026#34;summary_large_image\u0026#34;/\u0026gt; \u0026lt;meta name=\u0026#34;twitter:title\u0026#34; property=\u0026#34;og:title\u0026#34; content=\u0026#34;タイトル\u0026#34;/\u0026gt; \u0026lt;meta property=\u0026#34;og:url\u0026#34; content=\u0026#34;${siteURL}\u0026#34;\u0026gt; \u0026lt;meta property=\u0026#34;og:image\u0026#34; content=\u0026#34;${imageURL}\u0026#34;\u0026gt; \u0026lt;meta property=\u0026#34;og:image:width\u0026#34; content=\u0026#34;600\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;` } 正しく設定できているかは以下から確認できます。\nhttps://cards-dev.twitter.com/validator\nまとめ Firebase Cloud Functionsを使って、Twitter用のOGPを設定しました。\nHTTPSリクエストをトリガーにし、Firebase Storageから情報を取得、headerを付与し返却するという使い方でしたが、簡単に使うことができました。他のFirebaseのサービスへのアクセスは簡単なので、Firebaseを利用したいろいろなアプリケーションが簡単に作れそうです。他のトリガーなども使ってみたいと思います。\n"
            }
    
        ,
            {
                "id": 66,
                "href": "https://www.if-blog.site/posts/edgwbs/book-reports-firebase-auth/",
                "title": "Vue.js + Golang + Firebase Authenticationでログイン処理",
                "section": "posts",
                "date" : "2019.09.28",
                "body": "Vue.jsとGolangでWebアプリケーションでFirebase Authenticationを使い、ログイン処理を行います。\nhttps://firebase.google.com/docs/auth/?hl=ja\nFirebaseの説明や導入などは、Qiitaにもわかりやすい記事がたくさん上がっているので、そちらを参考にしてください。\nVue.js側 VueCLI3を使い、typescriptのプロジェクトを作成します。今回は、vueのrouter guardは省略します。\n準備 npmからインストールします。\n$ npm install firebase firebaseの認証情報は、以下のドキュメントを参考に取得しておきます。\nhttps://firebase.google.com/docs/auth/web/password-auth?authuser=1\nsrc/main.tsにfirebaseをimportし、上記で取得した認証情報を貼り付けます。\nimport firebase from \u0026#39;firebase/app\u0026#39;; const fireBaseConfig = { apiKey: \u0026#39;*****\u0026#39;, authDomain: \u0026#39;*****.firebaseapp.com\u0026#39;, databaseURL: \u0026#39;https://******.firebaseio.com\u0026#39;, projectId: \u0026#39;*****\u0026#39;, storageBucket: \u0026#39;*****.appspot.com\u0026#39;, messagingSenderId: \u0026#39;******\u0026#39;, }; firebase.initializeApp(fireBaseConfig); ログイン処理 firebaseではemail+passwordの通常の認証に加え、GoogleアカウントやTwitter、Githubのアカウントでのログインが1つのメソッドだけで行えます。\nGoogleなどのアカウントでログインしたい場合は、あらかじめfirebaseから認証を許可するよう設定しておく必要があります。\nsignIn with email+password コンポーネントにfirebaseのログイン画面を作ります。以下はその例です。\nimport {Component, Vue} from \u0026#39;vue-property-decorator\u0026#39;; import firebase from \u0026#39;firebase/app\u0026#39;; @Component export default class Home extends Vue { private email: string = \u0026#39;\u0026#39;; private password: string = \u0026#39;\u0026#39;; private showPassword: boolean = false; private siginInWithEmail() { firebase.auth().signInWithEmailAndPassword(this.email, this.password) .then((res: firebase.auth.UserCredential) =\u0026gt; { if (res.user === null) { return new Promise\u0026lt;string\u0026gt;((_, reject) =\u0026gt; reject(\u0026#39;user not found\u0026#39;)); } return res.user.getIdToken(); }) .then((idToken: string) =\u0026gt; alert(\u0026#34;login success\u0026#34;)) .catch((err) =\u0026gt; console.log(err)) .finally(() =\u0026gt; console.log(\u0026#39;end\u0026#39;)); } 見て分かる通り、以下のメソッドの引数にメールアドレスとパスワードを渡すだけです。\nfirebase.auth().signInWithEmailAndPassword(this.email, this.password)\nこのメソッドは、Promise\u0026lt;firebase.auth.UserCredential\u0026gt;を返します。 User情報などはここから取得できます。\nUserCredentialの中身は以下のドキュメントを参考。\nhttps://firebase.google.com/docs/reference/js/firebase.auth.html#usercredential\nfirebase.auth.UserCredentialのもつuserがgetIdTokenメソッドを持っており、ここからトークンが取得できます。\nPWAやスマホアプリでは、リフレッシュトークンを使いたいです。getIdTokenでは、トークンが失効していれば、firebase側でリフレッシュトークンを使ったトークンの更新処理を行ってくれるため、わざわざトークンの更新処理を追加する必要がありません。\nhttps://firebase.google.com/docs/reference/js/firebase.User.html#getidtoken\nsignIn with Google(popup) ログイン方法には、Popupで別ウィンドウが開きログインを行う方法とリダイレクトを用いた方法があります。 Googleアカウントを使いpopupでログインしたい場合は、以下のようにします。\nimport {Component, Vue} from \u0026#39;vue-property-decorator\u0026#39;; import firebase from \u0026#39;firebase/app\u0026#39;; private provider = new firebase.auth.GoogleAuthProvider(); @Component export default class Home extends Vue { private signInByGoogleWithPopup() { firebase.auth().signInWithPopup(this.provider) .then((res: firebase.auth.UserCredential) =\u0026gt; { // 省略 providerにGoogleを指定し、firebase.auth().signInWithPopup(this.provider)を呼ぶと、popupで見慣れたログイン画面が表示されます。 図\nfirebase.auth().signInWithPopup(this.provider)もEmail+Passwordの場合と同様に、Promise\u0026lt;firebase.auth.UserCredential\u0026gt;を返すので、同様の方法でtokenやuser情報を取得できます。\nsignIn with Google(redirect) PWAでは、リダイレクトでのログインが推奨されています。\nリダイレクトは以下のようにします。\nprivate siginInByGoogleWithRedirect() { firebase.auth().signInWithRedirect(this.provider) .catch((err) =\u0026gt; { console.log(err); }) .finally(() =\u0026gt; { console.log(\u0026#39;end\u0026#39;); }); } siginInByGoogleWithRedirectが呼ばれるとログイン画面にリダイレクトされます。\nログイン画面から元のページへ遷移後、リダイレクトによる認証が正しく行えたか確認します。\nprivate afterRedirect() { firebase.auth().getRedirectResult() .then((res: firebase.auth.UserCredential) =\u0026gt; { // 省略 getRedirectResultがfirebase.auth.UserCredentialを返すので、正しく認証できていれば、ここから先ほどと同様にユーザー情報やトークンを取得できます。\ngithub or twitter Googleアカウントを使った場合と同様に、プロバイダを指定しログインできます。 予めTwitterでデベロッパーアプリケーションとしてアプリを登録し、APIキーとAPI Secretを取得しておきます。 Twitterデベロッパー側に、OAuthコールバックとして、firebaseのURLを指定します。\nprovider = new firebase.auth.GithubAuthProvider(); // or provider = new firebase.auth.TwitterAuthProvider(); firebase.auth().signInWithPopup(provider) https://firebase.google.com/docs/auth/web/github-auth?hl=ja https://firebase.google.com/docs/auth/web/twitter-login?hl=ja\nパスワードの再設定 パスワードの再設定のメールを送信するのも、メソッド1つで行えます。\nfirebase.auth().sendPasswordResetEmail(this.email) 送信されるメールのbodyは、firebaseから設定が行えます。\nユーザーの更新、削除も同様に行えます。 詳しくはドキュメントを参考。\nhttps://firebase.google.com/docs/auth/web/manage-users?hl=ja#top_of_page\nメールアドレスの重複 Firebase Authenticationでは、メールアドレスの重複はデフォルトでは許可されていません。 Email+Passwordでアカウントを作成し、同じメールアドレスのGoogleアカウントでログインしようとすると、Googleアカウント側が有効となります。 このとき、ユーザー識別子は、Email+Passwordで作成したものが、Googleアカウントで作成したものに移行されるようです。\n重複を許可する設定も行えますが、1つのユーザーに複数のプロバイダを紐づけることもできるので、そちらを行った方が良いかと思います。 https://firebase.google.com/docs/auth/web/account-linking\n認証状態の永続性 https://firebase.google.com/docs/auth/web/auth-state-persistence?hl=ja\nGolang側 フロントで取得したJWTを、バックエンドに投げ、バックエンドからJWTが正しいかをfirebaeに問い合わせに行きます。\nユーザーのログインのフローは簡単に書くと以下の図のような流れになります。\nコード例は、以下の用になります。\nctx := context.Background() var opt option.ClientOption // localのjson keyから認証する場合 opt = option.WithCredentialsFile(\u0026#34;****.json\u0026#34;) // 環境変数から認証する場合 credentials, err := google.CredentialsFromJSON(ctx, []byte(authKey)) if err != nil { fmt.Println(err.Error()) os.Exit(1) } opt = option.WithCredentials(credentials) app, err := firebase.NewApp(context.Background(), nil, opt) if err != nil { fmt.Println(err.Error()) os.Exit(1) } client, err := app.Auth(context.Background()) if err != nil { log.Fatalf(\u0026#34;error getting Auth client: %v\\n\u0026#34;, err) os.Exit(1) } // ヘッダーからIDトークンを取得 authHeader := c.GetHeader((\u0026#34;Authorization\u0026#34;)) idToken := strings.Replace(authHeader, \u0026#34;Bearer \u0026#34;, \u0026#34;\u0026#34;, 1) // トークンを確認 token, err := client.VerifyIDToken(context.Background(), idToken) if err != nil { log.Fatalf(\u0026#34;error verifying ID token: %v\\n\u0026#34;, err) } log.Printf(\u0026#34;Verified ID token: %v\\n\u0026#34;, token) 参考 https://firebase.google.com/docs/auth/admin/verify-id-tokens?hl=ja\nまとめ Vue.jsとGolangからFirebase Authenticationを使いユーザーの管理を行いました。 これまで、ユーザー管理用のAPIを自分で書いていましたが、バックエンドもフロントエンドも少しコードを書くだけで認証が行えるfirebase、めっちゃ楽、すばらしいです.\n一方、firebaseは認証トークンにJWT(json web token)を使っており、JWTは使うべきでないというような記事も割と見かけるので、この辺りはもう少し詳しく調べて見ようかと思います。\n"
            }
    
        ,
            {
                "id": 67,
                "href": "https://www.if-blog.site/posts/edgwbs/book-reports-review/",
                "title": "三行で読書感想文を書いて、SNSでシェアできるサービスを作った",
                "section": "posts",
                "date" : "2019.09.25",
                "body": "サービス概要 「読書感想文をもっと気軽に」をコンセプトに読んだ本の感想文/レビューを三行だけ書き、SNSでシェアするサービスです。\n三行読書感想文\n使い方 googleアカウントもしくは、Twitterアカウントでログインできます。\nその他のSNSアカウントでのログインは、随時対応していこうと思います。\nログイン後に、感想を書く本を選びます。\nあとは、感想を書き、保存します。\n保存後にシェアします。\nこれだけの単純な作りです。\n作った動機 Firebaseを使いたかった 本に出会える場を増やしたかった 1.Firebaseを使いたかった 単純にFirebaseのみでサービスを稼働させたかっただけです。これまでFirebase Authは使ったことがありました。これがすごく便利で、びっくりしました。今まで、アカウント管理のAPIを頑張って作ってたのを肩代わりしてくれるなんて。そこで、それ以外のサービスを使ってみようと言うのが動機です。\n今回、Webアプリケーションで使えるDBやStorage、HostingなどFirebaseのサービスはほぼ使っています。すごく便利。使ってみた技術は、別の記事にまとめます。\n2.本に出会える場を増やしたかった 個人的な意見として、本とのファーストコンタクトは、タイトルや表紙、帯に書いてあるコメントなどパッと目に入ってくるものが多いです。本の感想やレビューを三行に絞ることで、書く側も読む側も気軽に使え、新たな本に出会える場となるのではないかと考えました。普段、本を読むことが少ない人は、こういった気軽な出会いから本を読んでみるのも良いのではないかと思います。\nまた、本を多く読む人も、多く読むからこそ、この本読んだけどどんな話だっけ？となることが多いかと思います。\nそういった場面で、短く感想を残しておくことで、自分自身の読んだ本の記録にもなるかと思います。\nまとめ Firebaseを使うことで、実質コーディングしたのは、フロントのVue.jsだけでした。DBやストレージへのアクセス、アカウント管理周りなどをFirebaseに任せられることで、APIを書く必要がないのは、コーディングする上で大きな負荷の軽減となりました。無料でここまで使わせてくれるのは、なにかサービスを簡単に始めたい時はとても便利だと思います。\n"
            }
    
        ,
            {
                "id": 68,
                "href": "https://www.if-blog.site/posts/python/python-matplotlib-base/",
                "title": "matplotlibのよく使う記法まとめ",
                "section": "posts",
                "date" : "2019.09.24",
                "body": "すぐ忘れるので、matplotlibのよく使う記法をまとめておく\n公式：https://matplotlib.org/gallery.html\n普段は散布図とかplotとかしか使わないけど、こうして見るといろんなグラフがかけるみたい\nmatplotlib,pyplot,pylabの違い matplotlibがパッケージ全体\npyplotはそのモジュール、スクリプトで作図するときに使う\npylabのimportは推奨されてない模様、インタラクティブな作図にはこっちを使うらしい\n基本的には、import matplotlib.pyplot as pltのように使う\n参考：\nhttps://matplotlib.org/faq/usage_faq.html#matplotlib-pyplot-and-pylab-how-are-they-related\nhttps://stackoverflow.com/questions/11469336/what-is-the-difference-between-pylab-and-pyplot\nグラフを描く シンプルな例は以下\nimport matplotlib.pyplot as plt import numpy as np x = np.arange(0,10,0.2) y = np.sin(x) plt.plot(x,y) plt.show() macでmatplotlibを呼ぶとValueError: unknown locale: UTF-8で怒られることがある。\nlocale周りの設定がおかしいため。以下を参考に直す。\nMac で ValueError: unknown locale: UTF-8 のエラーを解決したい\nhttps://www.lifewithpython.com/2016/09/python-ValueError-unknown-locale-UTF-8.html\nインスタンス化する plt.figure()でインスタンス化し、axesに対してプロットしていく\n多分この使い方が多いと思う\nimport matplotlib.pyplot as plt import numpy as np x = np.arange(0,10,0.2) y = np.sin(x) fig = plt.figure() ax = fig.add_subplot(111) ax.plot(x,y) plt.show() 以下のように描いても同じ\nfig, ax = plt.subplots(1, 1) ax.plot(x,y) plt.show() 複数のグラフを描く 複数のウィンドウに分けて描画 fig = plt.figure()の引数に適当にユニークな数字を入れておく。\nfigureに引数を与えることで、新たな図が生成される。\n参考： https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html\nimport matplotlib.pyplot as plt import numpy as np x = np.arange(0,10,0.2) y = np.sin(x) fig = plt.figure(1) ax = fig.add_subplot(111) ax.plot(x,y) fig = plt.figure(2) ax = fig.add_subplot(111) ax.plot(x,y) plt.show() 1つのウィンドウに複数のグラフを描画 1つのウィンドウに複数のグラフを描く場合は以下のようにする。\nfig = plt.figure() ax1 = fig.add_subplot(211) ax1.plot(x,y) ax2 = fig.add_subplot(212) ax2.plot(x,y) plt.show() 以下のように2つのグラフが描画される。\n以下のように描いても同じ、axesをタプルで受け取るのがちょっと気持ち悪い\nfig, (ax1, ax2) = plt.subplots(1, 2) ax1.plot(x,y) ax2.plot(x,y) plt.show() グラフの保存 savefigでグラフの保存が可能\nimport matplotlib.pyplot as plt import numpy as np x = np.arange(0,10,0.2) y = np.sin(x) fig = plt.figure() ax = fig.add_subplot(111) ax.plot(x,y) plt.savefig(\u0026#34;./test2\u0026#34;) 複数のグラフを描画した場合には、figureインスタンスに対してもsavefigが使える。\nimport matplotlib.pyplot as plt import numpy as np x = np.arange(0,10,0.2) y = np.sin(x) fig = plt.figure(1) ax = fig.add_subplot(111) ax.plot(x,y) fig.savefig(\u0026#34;./test1\u0026#34;) fig = plt.figure(2) ax = fig.add_subplot(111) ax.plot(x,y) fig.savefig(\u0026#34;./test2\u0026#34;) グラフを整形する グラフの線の色、太さ、線の種類などはよく使う。\nそれぞれ、color,linewidth,linestyleの引数を与えることで設定できる。\nimport matplotlib.pyplot as plt import numpy as np x = np.arange(0,10,0.2) y = np.sin(x) fig = plt.figure() ax = fig.add_subplot(111) ax.plot(x,y,color=\u0026#34;red\u0026#34;,linewidth=2,linestyle=\u0026#34;dashed\u0026#34;) plt.show() 引数の詳細は公式の以下のページから確認できる。\nhttps://matplotlib.org/api/_as_gen/matplotlib.pyplot.plot.html\nタイトルや軸名をつける シンプルな例は以下\nimport matplotlib.pyplot as plt import numpy as np x = np.arange(0,10,0.2) y = np.sin(x) plt.plot(x,y) plt.title(\u0026#34;test\u0026#34;) plt.xlabel(\u0026#34;x\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) plt.show() インスタンス化した場合は次のようにする。\nimport matplotlib.pyplot as plt import numpy as np x = np.arange(0,10,0.2) y = np.sin(x) fig = plt.figure() ax = fig.add_subplot(111) ax.plot(x,y) ax.set_title(\u0026#34;test\u0026#34;) ax.set_xlabel(\u0026#34;x\u0026#34;) ax.set_ylabel(\u0026#34;y\u0026#34;) plt.show() グラフの余白、幅を調整する plt.subplots_adjustで調整可能\nimport matplotlib.pyplot as plt import numpy as np x = np.arange(0,10,0.2) y = np.sin(x) plt.plot(x,y) plt.subplots_adjust(left=0.1, right=0.95, bottom=0.1, top=0.95) plt.show() わかりづらいが調整できてる。\n調整前 調整後 デフォルト値は以下\nleft = 0.125 # the left side of the subplots of the figure right = 0.9 # the right side of the subplots of the figure bottom = 0.1 # the bottom of the subplots of the figure top = 0.9 # the top of the subplots of the figure wspace = 0.2 # the amount of width reserved for space between subplots, # expressed as a fraction of the average axis width hspace = 0.2 # the amount of height reserved for space between subplots, # expressed as a fraction of the average axis height 1つのウィンドウに複数のグラフを描画し軸名など入れた場合、グラフが重なることがある。\nその場合は、plt.subplots_adjust(hspace=0.4)のように調整できる。\n横幅は、wspaceで調整可能。\nfig = plt.figure() plt.subplots_adjust(hspace=0.6) ax1 = fig.add_subplot(211) ax1.plot(x,y) ax2 = fig.add_subplot(212) ax2.plot(x,y) plt.show() 調整前\n調整後\n参考：https://www.haya-programming.com/entry/2018/10/11/030103\n軸を設定し直す ちょっとめんどくさい\nimport matplotlib.pyplot as plt import numpy as np x = np.arange(0,10,0.2) y = np.sin(x) fig = plt.figure() ax = fig.add_subplot(111) ax.plot(x,y) ticks = \u0026#34;abcdefghijklmnopqrstuvwxyz\u0026#34; plt.xticks(x,list(ticks)) plt.show() 設定し直したい次元と、元の軸の次元が合ってなくても設定できる模様。\nx軸に時刻を設定、フォーマット datetimeをそのままx軸に利用できる。\nimport matplotlib.pyplot as plt import pandas as pd x = [ \u0026#39;2018/08/08T12:15:45.000\u0026#39;, \u0026#39;2018/08/08T12:15:45.200\u0026#39;, \u0026#39;2018/08/08T12:17:45.600\u0026#39; ] x = pd.to_datetime(x,format=\u0026#39;%Y/%m/%dT%H:%M:%S.%f\u0026#39;) y = [1,2,3] fig = plt.figure() ax = fig.add_subplot(111) ax.plot(x,y) plt.show() フォーマットは、ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d\\n%H:%M'))を使って行う。\nimport matplotlib.pyplot as plt import pandas as pd import matplotlib.dates as mdates x = [ \u0026#39;2018/08/08T12:15:45.000\u0026#39;, \u0026#39;2018/08/08T12:15:45.200\u0026#39;, \u0026#39;2018/08/08T12:17:45.600\u0026#39; ] x = pd.to_datetime(x,format=\u0026#39;%Y/%m/%dT%H:%M:%S.%f\u0026#39;) y = [1,2,3] fig = plt.figure() ax = fig.add_subplot(111) ax.plot(x,y) ax.xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%m/%d\\n%H:%M\u0026#39;)) plt.show() 参考：https://hack-le.com/matplotlib-x-date-format/\n"
            }
    
        ,
            {
                "id": 69,
                "href": "https://www.if-blog.site/posts/neuralnet/pepar-summary/",
                "title": "Deeplearningまわりの最新論文を浅く広くみてまわる",
                "section": "posts",
                "date" : "2019.09.24",
                "body": "最新論文の情報を広く浅く集めたい場合の巡回するサイトたちをまとめておきます。\nおすすめがあれば教えてください。\nGoogle Scholar: https://scholar.google.com/schhp?hl=ja\n研究者やキーワードが決まっていると探しやすい。ただ、ザッピングには向かない。\nいろいろ論文の検索エンジン使ってたけど、結局これが残った。texの引用が楽\narXiv arXiv https://arxiv.org/\nトップページは慣れないとちょっと見づらい。有名な研究者で検索したものや分野で絞り込んだものをチェックするのが使いやすそう。\narXivTimes https://medium.com/@arxivtimes\n日本語、arXivのまとめ系、ツイッター\n論文まとめてくれてるGitHub https://github.com/dennybritz/deeplearning-papernotes\narXivのまとめ。古いものは2011年から、新しいものは2018まで更新がある。\n企業の論文 DeepMind https://deepmind.com/research/publications/\nMicrosoft Research Publications http://research.microsoft.com/apps/catalog/default.aspx?t=publications\u0026amp;ra=47200\nGoogle Research Publications http://research.google.com/pubs/ArtificialIntelligenceandMachineLearning.html\nYahoo! Labs http://labs.yahoo.com/publication/?area=machine-learning\n"
            }
    
        ,
            {
                "id": 70,
                "href": "https://www.if-blog.site/posts/others/http-portal-for-blog/",
                "title": "Hugoで作ったwebサイトを公開する",
                "section": "posts",
                "date" : "2019.09.24",
                "body": "はじめに GCE上にdockerコンテナを使って、hugoの静的ファイルを公開してます。\nいろいろwebサービスを作ったりはしますが、公開するのは初めてだったので、いろいろと苦労した点をまとめておきます。\nインフラ GCEを使います。ほんとは、image化してインスタンスグループで公開したかったのですが、とりあえず、コンテナをVM上にデプロイすることにします。\nドメイン ドメインは、お名前.comで取得します。これは特に説明なしでいけます。\nDNSは、route53を使いたかったんですが、今回はgoogleのcloud DNSを使います。\nGoogle Cloud Consoleから、「ネットワークサービス」→「Cloud DNS」を選択します。\n任意のゾーン名、取得したドメイン、(説明)を入力して作成。\nAレコードの登録 DNS名には、www.取得したドメイン、リソースタイプにはAレコードを設定し、作成します。\nレジストラへの登録 生成されるNSレコードのns-cloud-c1.googledomains.com.をお名前.com側に登録します。\nhttps化 let\u0026rsquo;s encryptを使ってhttps化しようと思っていたところ便利なものを発見したのでこちらを使います。\nhttps-portal\n自動で証明書の更新も行ってくれます。すごい。便利。\n親切にdocker-composeが公式に載せてあるので、参考にします。 docker-composeのインストールはpipなどで行ってください。\nhttps-portal: image: steveltn/https-portal:1 ports: - \u0026#39;80:80\u0026#39; - \u0026#39;443:443\u0026#39; links: - wordpress restart: always environment: DOMAINS: \u0026#39;wordpress.example.com -\u0026gt; http://wordpress:80\u0026#39; # STAGE: \u0026#39;production\u0026#39; # FORCE_RENEW: \u0026#39;true\u0026#39; wordpress: image: wordpress links: - db:mysql db: image: mariadb environment: MYSQL_ROOT_PASSWORD: \u0026#39;\u0026lt;a secure password\u0026gt;\u0026#39; imageのバージョンは:1と指定しましょう。最新版を取ってくるようになります。\nimage: steveltn/https-portal:1 1.2以下だと、let\u0026rsquo;s Encryptで証明書を取得する際にエラーが起きるので注意です。\nlinks: - wordpress linksで公開するサービス名を指定しましょう。\nenvironment: DOMAINS: \u0026#39;wordpress.example.com -\u0026gt; http://wordpress:80\u0026#39; 環境変数のDMAINSでは、リクエストを受けるドメイン -\u0026gt; https化したいエンドポイントを指定します。\nlinksで指定したエイリアスで指定できます。また、-\u0026gt;を=\u0026gt;に変更することでリダイレクトができます。\nSTAGEは、次の3つが指定できます。\nSTAGE: 'local'でオレオレ証明書を作成します。テストなどに STAGE: 'staging'でテスト用の証明書を作成します。 STAGE: 'production'で本番用に、Let\u0026rsquo;s Encryptで正式な証明書を作成します。 その他、環境変数も指定できます。この辺りを参考しながら、設定を行います。\nhttps://github.com/SteveLTN/https-portal/tree/master/fs_overlay/var/lib/nginx-conf\n"
            }
    
        ,
            {
                "id": 71,
                "href": "https://www.if-blog.site/posts/neuralnet/combine-model/",
                "title": "kerasでモデルを結合する",
                "section": "posts",
                "date" : "2019.09.24",
                "body": "autoencoderなどを作っていると、保存や推論を行う上での再利用性を考え、encoderとdecoderは分けてModelを作りたいことがある。\nautoencoderの学習の際には、作成したencoderのModelとdecoderのModelを結合する。\nModelの結合は前はできなかった気がするが、できるようになっていたのでメモ。\nKerasのバージョンは、2.1.1\nまずは、シンプルなモデルを2つ作る。input→model1→model2→outputを作る。\ndef model1(): layer_input = Input(shape=(None, 10)) layer_output = Dense(10)(layer_input) model = Model(layer_input, layer_output) model.summary() return model def model2(): layer_input = Input(shape=(None, 10)) layer_output = Dense(10)(layer_input) model = Model(layer_input, layer_output) model.summary() return model まず、model1へのインプットを作る。model1のアウトプットをmodel2のインプットにし、Modelのインスタンスを作る。\nm1 = model1() m2 = model2() inp = Input(shape=(None, 10)) model1_output = m1(inp) out = m2(model1_output) model = Model(inp, out) model.summary() すると、こんな感じでモデルが結合できたのが確認できる。\nUsing TensorFlow backend. _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, None, 10) 0 _________________________________________________________________ dense_1 (Dense) (None, None, 10) 110 ================================================================= Total params: 110 Trainable params: 110 Non-trainable params: 0 _________________________________________________________________ _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) (None, None, 10) 0 _________________________________________________________________ dense_2 (Dense) (None, None, 10) 110 ================================================================= Total params: 110 Trainable params: 110 Non-trainable params: 0 _________________________________________________________________ _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_3 (InputLayer) (None, None, 10) 0 _________________________________________________________________ model_1 (Model) (None, None, 10) 110 _________________________________________________________________ model_2 (Model) (None, None, 10) 110 ================================================================= Total params: 220 Trainable params: 220 Non-trainable params: 0 _________________________________________________________________ コード全体はこんな感じ。\nfrom keras.layers import Dense from keras.models import Model def model1(): layer_input = Input(shape=(None, 10)) layer_output = Dense(10)(layer_input) model = Model(layer_input, layer_output) model.summary() return model def model2(): layer_input = Input(shape=(None, 10)) layer_output = Dense(10)(layer_input) model = Model(layer_input, layer_output) model.summary() return model def main(): m1 = model1() m2 = model2() inp = Input(shape=(None, 10)) x = m1(inp) out = m2(x) model = Model(inp, out) model.summary() if __name__ == \u0026#39;__main__\u0026#39;: main() モデルインスタンスの再利用性が上がってて使いやすくなってる。\n"
            }
    
        ,
            {
                "id": 72,
                "href": "https://www.if-blog.site/posts/others/aws-cli-at-pyenv/",
                "title": "pyenv環境でaws cli入れたらコマンドが見つからない",
                "section": "posts",
                "date" : "2019.09.24",
                "body": "pyenv環境でaws cli入れたらコマンドが見つからないって言われたので調べたメモ\nバージョン：pyenv:1.0.10\n公式通りにインストールする。\n$ pip3 install awscli --upgrade --user すると、\n$ aws -bash: aws: command not found コマンドが見つからない\u0026hellip;.\n調べると、$HOME/.localにインストールされるらしいので、パスを通す。\naws --version aws-cli/1.16.29 Python/2.7.13 Darwin/16.7.0 botocore/1.12.19 できた！\npyenvが問題なのかはわからん\u0026hellip;.\n"
            }
    
        ,
            {
                "id": 73,
                "href": "https://www.if-blog.site/posts/others/rust-macro-use-another/",
                "title": "Rustでmodule間にまたがるmacro呼び出し",
                "section": "posts",
                "date" : "2019.09.24",
                "body": "Rustのmacroをモジュール化して、別のモジュールから呼ぼうと思ったらうまくいかなかった。\n日本語の記事があまりなかったのと、自分の備忘録ようにメモを残しておく。\nrustのバージョンは1.27.0\n参考 https://github.com/rust-lang/book/issues/401\nマクロの書き方とか、rustのimport周りは意外と日本語記事があったので、そっちを参考に\nファイル構成 lib.rsでmodをまとめて、main.rsで呼ぶようなファイル構成にする。 このようにしておくと、main.rsのなかでmodを宣言しなくてよくなるし、また誰が誰を公開しているかわかりやすくなる（たぶん）。\ncrate/ src/ main.rs lib.rs mod1/ mod.rs macro.rs mod2/ mod.rs macro_run.rs この時、main.rsを実行すると、macro_run.rsの中の関数が呼ばれ、この関数内でmacro.rsに定義してあるマクロが呼び出されるとする。\n簡単に言うと、モジュール間に跨ったマクロ呼び出しが行いたい。\n書き方 main.rsでは、macro_run内にあるrun関数を呼ぶ。\n// main.rs extern crate rust_test2; use rust_test2::mod2::macro_run; fn main() { macro_run::run(); } run関数内では、foo!というマクロを使う。\n// macro_run.rs #[macro_use] use mod1::macro_test::*; pub fn run() { let foo = foo!(1); println!(\u0026#34;{}\u0026#34;,foo); } fooマクロは、引数をそのまま返す単純なマクロ。\n// macro.rs #[macro_use] macro_rules! foo { ($e: expr) =\u0026gt; { $e } } macro_useをつける。\n// mod1/lib.rs #[macro_use] pub mod macro_test; ここのlibにもmacro_useが必要。\n// lib.rs #[macro_use] pub mod mod1; pub mod mod2; 次のように#[macro use]の順番が入れ変わるとだめ (参考：https://www.ncameron.org/blog/macros-in-rust-pt4/)\n// lib.rs pub mod mod2; #[macro_use] pub mod mod1; まとめ #[macro_use]をいっぱいかく lib.rsに書く順番が大切 コードの一部はネットの情報としてあったが、コードの全体がわからずハマったので全体をgithubにあげておく https://github.com/if001/rust_macro_test\n"
            }
    
        ,
            {
                "id": 74,
                "href": "https://www.if-blog.site/posts/nlp/seqgan-paper/",
                "title": "SeqGANの論文を読む",
                "section": "posts",
                "date" : "2019.09.24",
                "body": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient\nhttps://arxiv.org/pdf/1609.05473.pdf\nSeqGANの論文を読んだのでまとめておきます。実装を行おうと思って読んだので手法をメインに読みます。 結果などは余力があれば加筆します。\nはじめに 文章生成では、LSTMcellを使ったRNNが優れたパフォーマンスを行う。一般的な学習法は対数尤度を最大化する方法だが、次のような問題点がある。\nexposure bias(予測時には、自分の出力から次の語を予測することによるbias) BLEUを使っても良いけど、詩やchatbotだと難しい これらの問題に対して、General adversarial net(GAN)が有効そうである。ただし2つの問題がある。\nGANは連続データを生成するように設計されており、sequenceなどの離散データを直接生成するのは難しい。生成モデルGのパラメータの更新に識別モデルDの勾配を用いているが、微小な勾配により更新された生成モデルGに対応する出力値が離散のため存在しない可能性があるためである。 GANは、生成された文全体に対して、score/lossのみを与えることができる。部分的に生成されたsequenceには、文全体としての現在と将来のscoreのバランスをどのようにうまく取れば良いかが自明でない。 そこで、生成モデルに強化学習を用いたGANであるSeqGANを提案する。\nSequence Generative Adversarial Nets 生成モデル$G_ \\theta $はパラメータを$\\theta$として、$Y_ {1:T} = (y_ 1, y_ 2 , y_ T)$,$y_ t \\in \\mathcal Y$ を生成するために学習する。ここで、$\\mathcal Y$はvocabularyを表す。\n学習には、強化学習を用いる。時刻$t$において、状態$s$は現在の単語列$(y_ {1},y_ {2}, \\ldots ,y_ {t-1})$を表し、行動$a$により次の単語$y_ t$を選択する。このため、方策モデル$G_ {\\theta} (y_ {t}|Y_ {1:t-1})$ は確率的である。一方、行動を選択したあとでは、状態遷移は決定的である。つまり、もし現在の状態が$s=Y_ {1:t-1}$で行動が$a=y_ {t}$ならば、次の状態$s\u0026rsquo;=Y_ {1:t}$に対して、$\\delta^a_ {s,s\u0026rsquo;}=1$である。そうでないなら、次の状態$s\u0026rsquo;\u0026rsquo;$に対して$\\delta^a_ {s,s\u0026rsquo;\u0026rsquo;}=0$である。\n加えて、パラメーター$\\phi$を持つ識別モデル$D_ \\phi$は、生成モデル$G_ \\theta$を学習しながら正解を識別する。識別モデル$D_ \\phi$は、本物の文章かどうかを確率的に識別する。\n識別モデル$D_ \\phi$は、正解文章と生成モデル$G_ \\theta$から生成される不正解の文章を用いて学習する。同時に、生成モデル$G_ \\theta$は、期待値と$D_ \\phi$から得られる報酬に基づくMC searchにより学習する。\nSeqGAN via Policy Gradient 生成モデル(方策)$G_ \\theta(y_ t|Y_ {1:t-1})$の目的は、単語列$s_ 0$が与えら得た時に得られる期待値を最大化あすることである。\n$$ J(\\theta) = E[R_ t|s_ 0,\\theta ] = \\sum_ {y_ 1 \\in \\it Y} G_ \\theta (y_ 1|s_ 0)・Q^{G_ \\theta}_ {D_ \\phi}(s_ 0,y_ 1)$$\nここで、$R_ t$は、完全な文章に対する報酬を表す。この報酬は、$D_ {\\phi}$から得られる。これは後に記述する。$Q^{G_ \\theta}_ {D_ \\phi}(s_ 0,y_ 1)$は、方策$G_ \\theta$に従うaction-value functionである。\n続いて、どのようにaction-value functionを推定するかだが、これには強化学習と識別モデル$D_ \\phi$を報酬とし実際に推定される確率を用いる。\n式は次のように表される。 $$ Q^{G_ \\theta}_ {D_ \\phi}(s=Y_ {1:T-1},a=y_ T) = D_ \\phi(Y_ {1:T})$$\nただし、終了した文章に対してのみを識別モデルが生成する報酬として扱う。我々は、ステップごとに、長い期間の報酬を気にしているので、1つ前の単語の適応(fitness)だけについて考慮すべきでなく、将来の報酬についても考慮する必要がある。これは、ちょうど囲碁やチェスをゲームするのに似ている。\n中間状態に対して、action-valueを評価することについてだが、まだ観測していない$T-t$の単語をサンプリングするために、roll-out policy$G_ \\beta$を伴うMonte Carlo searchを適応する。\nN-time Monte Carlo searchは次のように定式化される。\n$$ { Y^1_ {1:T},\u0026hellip;,Y^N_ {1:T} } = MC^{G_ \\beta}(Y_ {1:t},N) $$\n実験では、$G_ \\beta$は、生成器と同様にセットされた。(速度が必要ならば簡易版でも良い)\n分散を下げ、またaction-valueのより良い精度を得るため、現在の状態から文章の最後までのN回のroll-out pulicyを実行して、出力サンプルのbatchを得る。 以上から、以下の定式化が行える。\n$$ Q^{G_ \\theta}_ {D_ \\phi}(s=Y_ {1:T-1},a=y_ T) = \\frac{1}{N} \\sum_ {n=1}^N D_ \\phi(T^n_ {1:T}, Y^n_ {1:T} \\in MC^{G_ \\beta}(Y_ {1:t};N)) , {\\rm for} , t \u0026lt; T$$\n$$ Q^{G_ \\theta}_ {D_ \\phi}(s=Y_ {1:T-1},a=y_ T) = D_ \\phi(T^n_ {1:T}, Y^n_ {1:T}) , {\\rm for} , t=T$$\n中間の報酬がないとき、関数は、状態$s\u0026rsquo;=Y_ {1:t}$から始まりrole-outの終わりまでのnext-state valueとして繰り返し定義される。\n識別モデル$D_ {\\phi}$を使った報酬は、生成モデルを繰り返し学習するために、動的に更新される。より現実的な文章のセットが生成されると、次のように識別モデルを更新する。\n$$ min_ \\phi - \\mathbb E_ {Y〜p_ {data}}[log D_ {\\phi}(Y)] - \\mathbb E_ {Y〜G_ {\\theta}}[log (1 - D_ {\\phi}(Y))] $$\nようは、クロスエントロピーである。 一方、生成モデルの目的関数$J(\\theta)$は、以下のようになる。\n$\\nabla_ {\\theta}J(\\theta) = \\sum_ {t=1}^{T} \\mathbb E_ {y_ {t}〜G_ {\\theta}(y_ t|Y_ {1:t-1})}[\\nabla_ {\\theta}logG_ \\theta(y_ t|Y_ {1:t-1})・Q_ {D_ \\phi}^{G_ \\theta}(Y_ {1:t-1,y_ t})]$\nここで、$Y_ {1:t-1}$は生成モデル$G_ \\theta$によりサンプリングされた観測される中間状態を表す。\nサンプリング手法で$\\mathbb E[・]$は近似することができるので、生成モデルのパラメーターは次のように更新される。\n$$ \\theta = \\theta + \\alpha \\nabla_ \\theta J(\\theta)$$\nここで、$\\alpha_ h$は$h$ステップ目の学習率を表す。勾配法としては、AdamかRMSpropを使う。\nThe Generative Model for Sequences 生成モデルとして、LSTMをつかう。\nThe Discriminative Model for Sequences 識別モデルには、CNNをつかう。\n入力文章を次のように表す。 $$ \\varepsilon_ {1:T} = x_ 1 \\otimes\tx_ 2 \\otimes \\dots \\otimes x_ t$$\nここで、$x_ t \\in \\mathbb R^k$はk次元にembeddingされた単語、$\\otimes$は行列$\\epsilon \\in \\mathbb R^{T \\times k}$ を生成するたのconcatenation operatorを表す。\nカーネル$ \\boldsymbol{w} \\in \\mathbb R^{l \\times k}$は、新しい特徴マップを生成するための$l$個の単語のwindow sizeへ適応する。(訳怪しい)\n$$c_ i = \\rho ( \\boldsymbol{w} \\otimes \\varepsilon_ {i:i+l-1}+b )$$\nここで、$\\otimes$は要素ごとの掛け算の和、$b$はバイアス、$\\rho$は非線形関数を表す。異なるwindow sizeの様々なカーネルを適応し特徴量を抽出する。最後に、特徴マップ$\\tilde{c}=max \\lbrace c_ 1,\\ldots,c_ {T-l+1} \\rbrace $上にmax-over-timeプーリングを適応する。\nパフォーマンス向上のためにthe pooled feature mapsに基づく、the highway architecture (Srivastava, Greff, and Schmidhuber 2015)を追加する。 最後に、活性化関数がシグモイド関数の全結合をつかい、正解か不正解の確率を出力する。 詳しい式は、appendixにかく。\n参考 http://hirotaka-hachiya.hatenablog.com/entry/2017/02/27/222611 https://mil-tokyo.github.io/paper-summary/papers/20170320-SeqGAN\n"
            }
    
        ,
            {
                "id": 75,
                "href": "https://www.if-blog.site/posts/nlp/word2vec-output-word/",
                "title": "word2vecでベクトルから単語を出力する",
                "section": "posts",
                "date" : "2019.09.24",
                "body": "word2vecで単語をベクトルにしたり、類似度判定した記事はたくさんあるが、ベクトルから類似単語を出力する日本語記事を見つけられなかったのでメモ。 stack overflowにあった。 https://stackoverflow.com/questions/32759712/how-to-find-the-closest-word-to-a-vector-using-word2vec\n結論 結論から言うと、以下のように適当にモデルを作りmost_simlar関数にベクトルを与えるだけ。\nsentences = gensim.models.word2vec.Text8Corpus(filename) model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4) model.most_similar( [ vector ], [], 1)[0][0] ただし、第3引数は出力する類似単語数を表す。\n使用例 青空文庫：江戸川乱歩の解析をしたので、その例を示す。\nmodel.wv[\u0026#34;ベクトル化したい単語\u0026#34;] で単語をベクトル化できる。\nsentences = gensim.models.word2vec.Text8Corpus(filename) model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4) vector = model.wv[\u0026#34;犯罪\u0026#34;] word = model.most_similar( [ vector ], [], 1) print(vector) print(word) 上記のようなコードを実行すると、\n[-0.01494294 -0.1509463 0.06123272 ..., 0.01335443 0.03439184 0.05130962] [(\u0026#39;犯罪\u0026#39;, 1.0000001192092896)] となり、「犯罪」をベクトル化し、そのベクトルから単語が出力できている。\n出力数を変更 vector = model.wv[\u0026#34;明智\u0026#34;] word = model.most_similar( [ vector ], [], 5) 以下のような結果が得られる。\n[(\u0026#39;明智\u0026#39;, 1.0000001192092896), (\u0026#39;名\u0026#39;, 0.8110731840133667), (\u0026#39;博士\u0026#39;,　0.8006758093833923), (\u0026#39;氏\u0026#39;, 0.7667115330696106), (\u0026#39;殿村\u0026#39;, 0.7487545609474182)] 単語のみを出力 単語のみを出力するために、以下のようにしておくとよい\nmodel.most_similar( [ vector ], [], 1)[0][0] 任意のベクトル入力 以下のようにして、任意のベクトルができる。\nvec = np.zeros(3000) vec[2] = 1 vec = np.array(vec,dtype=\u0026#39;f\u0026#39;) word = model.most_similar( [ vector ], [], 5) 出力結果\n[(\u0026#39;点い\u0026#39;, 0.04825586825609207), (\u0026#39;ワハハハハハハハハ\u0026#39;, 0.04788881167769432), (\u0026#39;珍奇\u0026#39;, 0.04669386148452759), (\u0026#39;憎らしい\u0026#39;, 0.04500409588217735), (\u0026#39;な\u0026#39;, 0.04486800357699394)] まとめ most_similar使えばいける。ちなみに公式には、model.wv.most_similarって書いてあったけど、wvなくてもできた。公式には、ベクトルは、model.wvのKeyedVectorsインスタンスに保存されてるって書いてあったけどよくわかってない。 今回は総単語数19002だったのだが、実行時間は1分ほど。word2vec高速すごい。\n"
            }
    
        ,
            {
                "id": 76,
                "href": "https://www.if-blog.site/posts/nlp/diversity-neural-conversation-model/",
                "title": "ニューラルネットワークを用いた対話モデルのための多様性を促進する目的関数",
                "section": "posts",
                "date" : "2019.09.24",
                "body": "Qiitaに投稿した記事、kerasでHREDを構築してみるの記事で、こちらの論文が参考になるとのコメント頂いて、読んで見たので簡単にまとめました。\nA Diversity-Promoting Objective Function for Neural Conversation Models https://arxiv.org/abs/1510.03055\n会話タスクにおける、入力文章(メッセージ)とそれに対する応答に多様性をもたせる手法を提案した論文です。 モデル周りをメインにそれ以外は軽く流し読みしているので、悪しからず。\nはじめに sequence to sequece(seq2seq)などの対話モデルでは、多様で文法的な応答が求められる。このモデルでは、入力される文章と出力される文章の対応のみを考慮しているため、I\u0026rsquo;m OKやI\u0026rsquo;dont knowのような高頻度フレーズを生成しがちである。したがって、メッセージに関する応答の依存性だけでなく、応答とメッセージの関係性についても考慮すべきである。\nそこで、私たちは、Maximum Mutual Information（MMI）を目的関数とする対話モデルを提案する。私たちは、MMIを使用することで、多様で興味深い文章を生成することを示します。\nMMIモデル seq2seqモデルの標準的な目的関数は以下のように表される。\n$$\\hat{T} = argmax_T{\\log p(T|S)}$$\n$N$は単語数を表し、入力文章(メッセージ)$S$とそれに対する応答$T$は以下のように表される。 $S = {s_1, s_2, \u0026hellip;, s_{N_s} }$ $T = {t_1, t_2, \u0026hellip;, t_{N_t}, EOS}$\nseq2seqモデルの目的関数を以下のように修正する。\n$$\\hat{T} = argmax_T {\\log p(T|S) - \\log p(T)}$$\nこのとき、argmaxの中身は、以下のように式変形から、相互情報量(wikipedia) を表していることがわかる。\n$${\\log p(T|S) - \\log p(T)} = \\frac{\\log p(S,T)}{\\log p(S) \\log p(T)}$$\nしたがって、この式は、相互情報量を最大化(MMI)する応答を出力することとなる。\nまた、$\\log p(T)$は、seq2seqの標準的な目的関数に対するペナルティ項とみなすことができる。メッセージに対するありふれた応答に対してペナルティを与えることで、応答の多様性を保つことを期待している。\nこのペナルティー項を調節できるように、(2)式に対して、パラメタ$\\lambda$を追加する。これを、MMI-antiLMと呼ぶ。\n$$\\hat{T} = argmax_T {\\log p(T|S) - \\lambda \\log p(T)} \u0026ndash;(1)$$\n式(3)をベイズの定理を用いて変形すると、以下のようになる。これをMMI-bidiと呼ぶ\n$$\\hat{T} = argmax_T {(1-\\lambda)\\log p(T|S) + \\lambda \\log p(S|T)} \u0026ndash;(2)$$\nMMI-bidiでは、$p(T|S)$と$p(S|T)$がトレードオフの関係にあることがわかる。\nseq2seqモデルの学習にMMIを適応させることは、重要ではない。さらに、モデルの訓練に時間がかかるので、訓練なしに$\\lambda$を調整したい。そこで、モデルを訓練するのではなく、学習時には最尤モデルを、テスト時にMMI基準を使用する。\n実用的な考察 MMI-antiLMとMMI-bidiを適応するにあたり、\n文法的におかしな応答 (MMI-antiLM) デコードを困難にする (MMI-bidi) という問題があり、直接decodingに適応するのは困難である。\nそこで、これらの問題について、以下で解決策を説明する。\nMMI-antiLM(an anti-language model) 式(1)の第2項の$p(T)$は、次のように表される。\n$$ p(T) = \\prod_ {k=1}^{N_ t} p(t_ k \\mid t_ 1 , \\cdots , t_ {k-1} ) $$\nanti-language modelでは、流暢な応答にもペナルティを与えるので、誤った文法の出力に繋がる可能性がある。そこで、$P(T)$に現在の単語$k$に基づく重み$g(k)$を乗算した、$U(T)$を導入する。\n$$ U(T)=\\prod_ {k=1}^{N_ t} p(t_ k \\mid t_ 1 \\cdots t_ {k-1} )・g(k) $$\n\\begin{eqnarray} g(k) \\begin{cases} 1 \u0026amp; ( k \\leq \\gamma ) \\\\\\ 0 \u0026amp; ( k \\gt \\gamma ) \\end{cases} \\end{eqnarray}\nここで、$\\gamma$は文頭から数えて何単語目かを表す閾値である。\n式（1）は、以下のように書き直すことができる。\n$$\\hat{T} = argmax_T {\\log p(T|S) - \\lambda \\log U(T) }$$\nこうすることで、メッセージにおける全ての単語に対して、ペナルティを与えるわけではなく、初めの何単語かにだけペナルティを与えることで、典型的なパターンがくるのを避けることを期待する。\nMMI-bidi 式（2）の第2項には、$p(S|T)$が含まれている。\n$$\\hat{T} = argmax_T {(1-\\lambda)\\log p(T|S) + \\lambda \\log p(S|T)}$$\n$p(S|T)$は、応答$T$の生成後でないと計算することはできない。また、応答$T$の探索空間が広すぎるために、全てを探索するのは難しい。したがって、この式を用いて直接decodeすることはできない。\nそこで、近似的なアプローチを行う。\nMMI-bidiの第1項である$p(T|S)$ (すなわち、標準的なseq2seqの目的関数)を与えられた第1世代N-bestリストを生成 MMI-bidiの第2項を使ってN-bestリストをrerankする 一般的に、標準的なseq2seqモデルから得られるN-bestは文法的であるため、最終的に選択されるoptionsは適切な文法である可能性が高い。 Training encoderとdecoderはそれぞれ4層のLSTMから構成 隠れ層は1000個の素子、単語埋め込み数の次元は1000次元 LSTMパラメタと埋め込みは a uniform distribution in [−0.08, 0.08]で初期化 SGDの学習率は0.1 バッチサイズは256 Gradient clipping is adopted by scaling gradients when the norm exceeded a threshold of 1 decoding MMI-antiLM MMI-antiLMの式は簡単に実装できる。 実験において、応答の長さも重要だと気づいた。そこで、損失関数に応答の長さのパラメタを加えた。\n$$ Score(T) = p(T|S) - \\lambda U(T) + \\gamma N_t$$\nここで、$N_t$はターゲットの長さ、$\\gamma$はその重みを表す。 応答の候補のN-best listに関するMERTを使って、$\\gamma$と$\\lambda$を最適化した。\nMMI-bidi 結果 評価には、BLEUを使う。さらに、生成された応答における異なるユニグラムおよびバイグラムの数を計算し、多様性を評価する。\nTwitter Conversation Triple Dataset 129Mのメッセージと応答の集合から、2300万の会話スニペットを使用。\nstatistical machine translation (SMT)とseq2seqとの比較を行った。　MMI-bidiのBLEU評価値が一番高い。\nOpenSubtitles Dataset 60M、70Mのノイズを多く含む、映画キャラクターの対話データ\n他のモデルに比べ、MMI-antiLMモデルがBLEUの評価値と多様性の評価値どちらにおいても最も高くなった。MMI-bidiがMMI-antiLMより低い性能となったのは、N-bestリストの初期世代に左右されるからである。\nまとめ seq2seqを使った対話モデルでは、メッセージに対する応答が、一般的でありふれたものになる傾向があるという問題点があった。我々の分析では、これは、入力（メッセージ）を与えられた出力（応答）の単方向尤度の使用に起因することを示唆している。そこで目的関数に最大相互情報量(MMI)を使用することを提案した。実験の結果、提案されたMMIモデルでは、より多様で興味深い応答を生成することができた。\n"
            }
    
        ,
            {
                "id": 77,
                "href": "https://www.if-blog.site/posts/others/tex-notation-numerical-formula/",
                "title": "機械学習系の数式によく出てくる記号などのTex表記まとめ",
                "section": "posts",
                "date" : "2019.09.24",
                "body": "すぐ忘れるのでTex表記をまとめて起きます。 Qiitaが使っているというEasy Copy MathJaxを用います。\n読み 表示 tex デルタ $\\Delta$ \\Delta ナブラ $\\nabla$ \\nabla 白抜き $\\mathbb E[\\cdot]$ \\mathbb E[\\cdot] 属する $\\in$ \\in 要素 $\\ni$ \\ni 条件付き確率 $p(x \\mid y)$ p(x \\mid y) ドット $x_1,x_2, \\ldots, x_t$ x_1,x_2, \\ldots, x_t カリグラフィーフォント $\\mathcal L$ \\cal L 太文字ベクトル $\\boldsymbol{w}$ \\boldsymbol{w} macだと、ギリシャ文字は日本語の読み方で変換するとスペルが出てくるので便利。\n参考：http://easy-copy-mathjax.xxxx7.com/\n追記 $\\KaTeX$だと、カリグラフィーフォントは、$\\mathcal L$:\\mathcal Lこう。\n"
            }
    
        ,
            {
                "id": 78,
                "href": "https://www.if-blog.site/posts/nlp/nlp-parse-overview-1/",
                "title": "自然言語処理シリーズの構文解析を読む（概要）",
                "section": "posts",
                "date" : "2019.09.24",
                "body": "はじめに 自然言語処理シリーズの構文解析を読んでいきます。\n全体の概要把握:1時間 中身の細かいとこ：3時間 という感じで読み進めて行こうと思います。\n概要 構文解析を用いることで、単語の並びの背後にある文法的な構造を明らかにすることができる。構文解析を学ぶことで、自然言語処理で用いられる様々な先人の知恵を学習できる。\nこの本では以下のような構成となっている。\n1章、はじめに 2章、品詞タグ付けのための手法と、機械学習の基礎的な事項について 3章、句構造解析について 4章、依存構造解析 5章、文法理論、深い構文解析 6章、構文解析の応用例 7章、構文解析ツールの紹介 8章、モデルやアルゴリズムの学習用・評価用データに用いられるツリーバンクの紹介 以下、各章の概要まとめです。\n2章：品詞解析と機械学習 品詞解析のためのさまざまな技術の解説し、その基盤となる機械学習の考え方と代表的なモデルを紹介する。\n品詞タグ付け：与えられた文章の各単語の品詞を判定し、品詞情報を付与する処理 品詞タガー：品詞タグ付けを行うプログラム ルールベースの手法：shoudの後には動詞がくると決めうちでタグ付けをする手法 素性: 品詞判定の手がかりとして利用する情報\n隠れマルコフモデル 機械学習に基づく品詞タグ付け手法の中で基本的なもの。 単純な計算では、文長に対し計算量が指数関数的に増加する問題がある。\nViterbiアルゴリズム 計算量増加の問題を解決。動的計画法の1種。 アンダーフローの問題がある、この問題に対し、対数をとっても良いが計算が遅くなるというデメリットもある。\n最大エントロピーモデル 最大エントロピーモデルは、品詞判定に役立つ手がかりを素性として利用し予測できるアルゴリズムの一種。品詞の前後のつながりを考慮せずに予測するというモデル。 計算コストは少ないが、素性を柔軟に設計できないため精度が低いという問題点がある。 自然言語処理では、実装が簡単なことからSGDがよく用いられる。学習データの数が多い場合には、短時間で最適化を行うことができる。\n最大エントロピーマルコフモデル 最大エントロピーモデルに、品詞のつながりを考慮させ正確な予測を行うことできるように改良されたモデル。 先行する単語の品詞に関数情報を素性として利用するできるが、最初にタグ付けを間違えると、その誤りによって別の誤りが引き起こされてしまう問題がある。\n条件付き確率場(CRF) 各単語の品詞を個別に予測するのではなく、文全体の品詞列全体を一度に予測しようとするアプローチに基づく代表的な確率モデル。\n構造化パーセプトロン CRFでは、登場する品詞列すべてに対し確率を求めるが、もっとも正解である確率の高い品詞列さえ得られれば良いという状況もある。そのような状況では、構造化パーセプトロンが役にたつ。動的計画法を用いて品詞タグ付けを行う。\nビーム探索 構造化パーセプトロンでは動的計画法を用いて品詞タグ付けを行ったが、素性が局所的な場合には、動的計画法が使えない。そこで、非局所的な様々な素性を利用するためによく用いられるのがビーム探索である。似たような手法として、Max Violationがある。\n生コーパスを利用した学習 これまでの学習は、コーパスを前提にしたものだった。しかしコーパスの構築には膨大な時間がかかる。与えられた文章のみで学習する手法を、半教師あり学習と呼ぶ。\n自己学習 生コーパスを用いる学習に自己学習と呼ばれる方法がある。 これは、CRFや構造化パーセプトロンでは精度向上に効果がないが、隠れマルコフモデルのような生成モデルでは大きな精度向上を得ることができる。これは、生成モデルの場合、EMアルゴリズムの1ステップに対応しているからである。隠れマルコフモデルで自己学習を行う場合、Baum-Welchアルゴリズムを用いる。\n3章：句構造解析 構文解析の表現方法の1つである句構造と、それに基づく構文解析の手法について説明する。\n句構造 文中の句同士の包含関係を階層的にまとめあげることで、その構造を明らかにする。 適切な句構造を得るための問題を2つに分けると、与えられた文に対して文法上可能な全ての句構造を計算することと、それらの中から最も適切な句構造を選択することとなる。\n文脈自由文法 文の句構造を表現するための、最も基本的な文法の一つ。 文脈自由文法のためのボトムアップな構文解析手法の一つであるCKY法と、任意の文脈自由文法を用いてトップダウンに構文解析を行うことが可能なEarly法がある。\n確率文脈自由文法(PCFG) 句構造を列挙した上で、最も確からしい句構造を選択する枠組みの1つ\n確率文脈自由文法(PCFG)の拡張 PCFGを拡張した、Collins Parserの手法について解説 生成的な確率モデルを注意深く設計することで、正しい構文木にたいして大きな確率がわりあてられるようにする。\n識別モデルによる際順位付け 正しい構文木とそうでない構文木を区別する特徴はいくつかあるが、Collins Parserでは限られたものしか考慮することができない。\n評価法 句構造解析の精度の評価には、構文木を構成する句の適合率(precision)と再現率(recall)がよく用いられる。\n4章:依存構造解析 依存構造とは、文中の単語間の関係をグラフで表したものである。\nCKY法 依存構造を句構造だと考え解析する。\nEisner法 同じ出力を得るための解析プロセスが複数存在することを、擬似曖昧性と呼ぶ。 依存構文解析において、擬似曖昧性が生じないようにCKY法を改良したものがEisner法である。\nMST法 これまでの手法は、句構造解析のアルゴリズムを依存構造解析に応用したものであったが、依存構文解析に特有の手法として、最大全域木(MST)法を応用したものが知られている。 最大全域木とは、エッジにスコアが与えられたグラフを入力として、全ノードを被覆する木(=全域木)のうち、スコアのわが最大となるものを求めるアルゴリズムである。\n遷移型依存構造解析 句造像解析と同様のアルゴリズムを依存構造解析でも考えることができる。\n日本語構文解析 p102(スキップ)\n評価法 依存構造解析の評価には、ラベル付きかかり受け精度およびラベルなしかかり受け精度が用いられる。\n5章：文法理論、深い構文解析 3、4章で解説した、句構造・依存構造では明示されない意味的構造をも計算する、構文解析のことを、深い構文解析と呼ぶ。\n組み合わせ範疇文法(CCG) 組み合わせ範疇文法(CCG)とは、各単語にその文法機能を表すカテゴリを与え、そのカテゴリを組み合わせることで文の構造を計算する。\n主辞駆動句構造文法(HPSG) 主辞駆動句構造文法(HPSG)とは、主辞(句の中で意味的もしくは文法的に中心的な役割を果たす語)が持つ文法機能を素性構造というデータ構造で表し、それに基づき文全体の構造を決定する文法理論である。素性構造では、C++の構造体やオブジェクト指向言語におけるクラスに似た形でデータを表す。\n深い構文解析 CCGやHPSGのような文法理論を用いて、CFGより多くの情報をもたせた文法構造を解析する手法を深い文法構造と呼ぶ。\n"
            }
    
        ,
            {
                "id": 79,
                "href": "https://www.if-blog.site/posts/nlp/char-vec/",
                "title": "文字をベクトル化する",
                "section": "posts",
                "date" : "2019.09.24",
                "body": "文章生成にchar-level lstmを使ってみる。英語ではうまくいっている例があるが日本語では難しい。これは、日本語は英語に比べ文字数が多く、ニューラルネットワークの次元数(パラメータ数)が増やす必要があるのが原因の1つだと思う。また、次元削減のため、日本語では文章を単語に区切り単語をベクトル化し、lstmで文章を生成する手法もあるが、単語に区切る時点でしゃべり言葉やネットの言葉ではうまく区切れないという問題がある。そこで、日本語の文字を画像として生成し、その画像をauto-encoderを用いてベクトル化することで、文字のベクトル化を行い、lstmに食わせるという手法を試して見ようと思う。\n今回は、auto-encodeを用いた文字レベルのベクトル化までを行ってみようと思う。\nコードはここ、https://github.com/if001/fifc.git\n以下の3工程で行う。\nフォントファイルからフォント画像を生成 文字列とフォント画像をマッピング フォント画像から特徴量を生成 フォントファイルからフォント画像を生成 フォントファイルは、PILのImageFontのturetypeを使い読み込むことができる。\nフォントファイルとフォントのサイズを引数に与えることで、fontオブジェクトが生成できる。\nfrom PIL import ImageFont font_size=28 font = ImageFont.truetype(font_file, font_size, encoding=\u0026#39;unic\u0026#39;) 読み込んだフォントは下記のように保存する。\nfrom PIL import Image pict_height=28 pict_width=28 image = Image.new(\u0026#39;RGB\u0026#39;, (pict_height, pict_width), (255, 255, 255)) draw = ImageDraw.Draw(image) draw.text(pos, yomi, font=font, fill=\u0026#39;#000000\u0026#39;) image.save(\u0026#34;./font\u0026#34;, \u0026#39;PNG\u0026#39;) Image.new() で空のイメージを生成し、\nDrawオブジェクトに対しのtext関数を用いてフォントファイルを書き込む。\ndraw.text(pos, yomi, font=font, fill=\u0026#39;#000000\u0026#39;) 引数は、上記のtext関数のリンクを参照。\n保存するファイル名は、以下のように文字を16進数変換したものを使う。\nyomi=\u0026#34;歩\u0026#34; bytes_yomi = yomi.encode(\u0026#34;UTF-8\u0026#34;).hex() \u0026gt;\u0026gt; \u0026#39;e6ada9\u0026#39; 文字列とフォント画像をマッピング ファイル名を読み仮名を16進数変換し保存しているので、文字から毎回画像ファイルを読み込んでも良いが、高速に呼び出せるように、1文字と画像をファイルとkvsを使ってマッピングしておく。\nkvsにはplyvelを使う。以下のようにインスタンス化する。\nimport plyvel db = plyvel.DB(db_path, create_if_missing=True) 以下のように、保存と取り出しを行う。\ndb.put(key, value) # 保存 db.get(key) # 取り出す フォント画像から特徴量を生成 auto-encoderを使って、保存した画像から特徴量を抽出する。\nモデルの構造はkerasの公式ブログを参考にし、以下の構造とした。\n各層の次元は以下のようにした。\nfont_size = 32 input_img = Input(shape=(font_size, font_size, 1)) x = Conv2D(16, (3, 3), padding=\u0026#39;same\u0026#39;)(input_img) x = BatchNormalization()(x) x = ReLU()(x) x = MaxPooling2D((2, 2), padding=\u0026#39;same\u0026#39;)(x) x = Conv2D(8, (3, 3), padding=\u0026#39;same\u0026#39;)(x) x = BatchNormalization()(x) x = ReLU()(x) x = MaxPooling2D((2, 2), padding=\u0026#39;same\u0026#39;)(x) x = Conv2D(self.hidden_dim, (3, 3), padding=\u0026#39;same\u0026#39;)(x) x = ReLU()(x) encoded = MaxPooling2D((2, 2), padding=\u0026#39;same\u0026#39;, name=\u0026#34;encoder\u0026#34;)(x) x = Conv2D(8, (3, 3), padding=\u0026#39;same\u0026#39;, name=\u0026#34;decoder\u0026#34;)(encoded) x = ReLU()(x) x = UpSampling2D((2, 2))(x) x = Conv2D(8, (3, 3), padding=\u0026#39;same\u0026#39;)(x) x = BatchNormalization()(x) x = ReLU()(x) x = UpSampling2D((2, 2))(x) x = Conv2D(16, (3, 3), padding=\u0026#39;same\u0026#39;)(x) x = BatchNormalization()(x) x = ReLU()(x) x = UpSampling2D((2, 2))(x) decoded = Conv2D(1, (3, 3), activation=\u0026#39;sigmoid\u0026#39;, padding=\u0026#39;same\u0026#39;)(x) autoencoder = Model(input_img, decoded) autoencoder.summary() 学習 損失関数と最適化関数はそれぞれ、mean_squared_errorとadamを用いた。\n教師データとバリデーションデータはそれぞれ、66918個、16730個とした。また、バッチサイズは128とした。\n学習は、kerasのearlystoppingを使いval_lossが減少がなくなるまで行った。\n結果 最中的なスコア\nepoch :4 acc :0.591411 loss :0.046486 val_acc :0.529843 val_loss:0.073542 だいぶ早めに学習が打ち切られているのがわかる。\n2epoch目くらいからほとんどlossもaccも変化しなくなっている。次元数や層の数が少なくパラメタが足りないのか???\n一応デコードされた文字を確認しておく。\n左が教師データ、右がdecodeされた文字となる。auto-encoderなのでぼやけているのはしょうがないが割と綺麗にデコードできている。ただし、画数の多い文字となるとやはりぼやけて元の文字がわからない。この辺は、モデルや学習のパラメタを調整することでも少し改善されると思う。\n生成された特徴量も確認しておく。\n以下に、\u0026lsquo;寂\u0026rsquo;, \u0026lsquo;,\u0026rsquo;, \u0026lsquo;ッ\u0026rsquo;, \u0026lsquo;鏡\u0026rsquo;, \u0026lsquo;奸\u0026rsquo;, \u0026lsquo;Ｐ\u0026rsquo;, \u0026lsquo;跳\u0026rsquo;, \u0026lsquo;・\u0026rsquo;, \u0026lsquo;“\u0026rsquo;, \u0026lsquo;ょ\u0026rsquo;の記号を含む10文字を与えた際の特徴量を図示する。 特徴量は、1文字あたり(4,4,8)次元となるので、4×4の画像を8枚横に並べたものを表示している。\n図をみると、文字ごとの偏りはなく学習できているように見える。\nlossは0.04とある程度小さい値となったが、accは0.6とそこまで高い値とはならなかった。改善のため何をすれば良いかの指標がいまいちわからん。損失関数と最適化関数は良さそうなので、とりあえず、層の数やunit数、学習率などのパラメタや教師データを増やすなど試してみようと思う。随時更新していく。\n"
            }
    
        ,
            {
                "id": 80,
                "href": "https://www.if-blog.site/posts/python/colaboratory-google-drive-mount/",
                "title": "Google ColaboratoryでGoogle driveをマウントする",
                "section": "posts",
                "date" : "2019.09.23",
                "body": "久しぶりにColaboratory使ったらGoogle driveのマウントが簡単になってた。\nbefore 認証\n!apt-get install -y -qq software-properties-common python-software-properties module-init-tools !add-apt-repository -y ppa:alessandro-strada/ppa 2\u0026gt;\u0026amp;1 \u0026gt; /dev/null !apt-get update -qq 2\u0026gt;\u0026amp;1 \u0026gt; /dev/null !apt-get -y install -qq google-drive-ocamlfuse fuse from google.colab import auth auth.authenticate_user() from oauth2client.client import GoogleCredentials creds = GoogleCredentials.get_application_default() import getpass !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} \u0026lt; /dev/null 2\u0026gt;\u0026amp;1 | grep URL vcode = getpass.getpass() !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} マウント\n!mkdir -p drive !google-drive-ocamlfuse drive after from google.colab import drive drive.mount(\u0026#39;/gdrive\u0026#39;) "
            }
    
        ,
            {
                "id": 81,
                "href": "https://www.if-blog.site/posts/emacs/emacs-hinamatsuri/",
                "title": "emacsひな祭りいってきたのでためになったpackageなどをまとめておく",
                "section": "posts",
                "date" : "2019.03.08",
                "body": "2019年3月3日に開催された東京Emacsひなまつりにいってきました。\nhttps://tokyo-emacs.connpass.com/event/121603/\n随時コメント追加していきます。\nox-hugo\nhttps://ox-hugo.scripter.co/\nOx-hugo\nhttps://sfus.net/blog/2018/12/org-mode-with-ox-hugo/\nAtomic-chrome\nhttps://qiita.com/alpha22jp/items/f67bda3ed0c4ac84c29f\nLSP\nhttps://qiita.com/blue0513/items/acc962738c7f4da26656\neglot\nhttps://howm.osdn.jp/index-j.html\nIvy\nhttps://qiita.com/tadsan/items/33ebb8db2271897a462b#ivycounsel\nOrg-chapture\nhttps://www.5ing-myway.com/org-capture/\nchild-frame\nhttps://www.gnu.org/software/emacs/manual/html_node/elisp/Child-Frames.html\nhighlight-indent-guides.el\nhttps://qiita.com/blue0513/items/acc962738c7f4da26656\ndoom-theme\nhttps://github.com/hlissner/emacs-doom-themes\ndoom-modeline\nhttps://github.com/seagle0128/doom-modeline\ngit-gutter\nhttps://emacs-jp.github.io/packages/vcs/git-gutter\nhttps://qiita.com/syohex/items/a669b35fbbfcdda0cbf2\nWhite-space mode\nhttps://qiita.com/itiut@github/items/4d74da2412a29ef59c3a\nProjectile\nhttps://github.com/bbatsov/projectile\nDdss\nhttp://emacs.rubikitch.com/sd1408-ddskk/\nripgrep\nhttps://github.com/dajva/rg.el\nhttp://emacs.rubikitch.com/ripgrep/\n感想 もっとゴリゴリな変態emacs使い(褒め言葉)ばかりかと思ってたけど、実用性の高い話が聞けてよかったです。emacs歴は7,8年ほどですが普段はpythonやmarkdownなどちょっとした開発にemacsを使う程度で、scalaやhtml、jsや規模の大きいプロジェクトなどではIDEを使っています。今回ので良い知見が得られたのでこれからはしっかりemacs使っていこうと思います。 ORGモード人気。\n"
            }
    
        ,
            {
                "id": 82,
                "href": "https://www.if-blog.site/posts/python/python-get-extn/",
                "title": "pythonで拡張子を取得する",
                "section": "posts",
                "date" : "2019.03.01",
                "body": "ファイルの拡張子を取得する方法をいつも忘れるのでメモ\nimport os file_path = \u0026#34;hoge/test.md\u0026#34; root, ext = os.path.splitext(file_path) print(root, ext) 出力\nhoge/test .md "
            }
    
        ,
            {
                "id": 83,
                "href": "https://www.if-blog.site/posts/emacs/lsp-mode/",
                "title": "lsp-modeを導入する",
                "section": "posts",
                "date" : "2018.11.20",
                "body": "大学時代から使っていたemacsの設定たちを綺麗にするという目的で、ついでにlsp-modeを導入してみようと思う。\nlsp-mode\nhttps://github.com/emacs-lsp/lsp-mode\nlanguage server protocolとは language server protocol(lsp)とは、IDEやエディタとIDEが提供する機能(補間、定義jump、ドキュメントの表示など)の間で使用されるプロトコルを定義したものである。\nこれまで、IDEなどの提供する機能(補間、定義へのjump、ドキュメントの表示など)は言語ごと、IDEごとに提供されていた。\nそれを取りまとめるプロトコルが定義されることで、IDEごとの格差が緩和され、emacsでもIDEと同様の機能が実現でき流ようになるということです。\n詳しくは、以下が参考になると思う。\nhttps://microsoft.github.io/language-server-protocol/\nhttps://github.com/Microsoft/language-server-protocol\nhttps://qiita.com/atsushieno/items/ce31df9bd88e98eec5c4\nlsp-mode emacsでは、lsp-modeとeglotがlspをサポートしているみたい。今回はlsp-modeを導入する。\nlsp-mode: https://github.com/emacs-lsp/lsp-mode\nを見ながら設定していく。\nmelpaからインストール\nM-x package-install [RET] lsp-mode [RET] 以下のように設定する。\n(use-package lsp-mode :custom (lsp-print-io t) (lsp-prefer-flymake \u0026#39;flymake) :commands lsp) lsp-print-ioで全てのログを*lsp-log*に表示する。今回はflymakeを使うので、(lsp-prefer-flymake 'flymake) を設定しておく。\n続いて、lsp-modeのバックエンドにcompanyを使うためにcompany-lspを、lsp-modeのUIをリッチにしてくれるlsp-uiを入れる。\nそれぞれmelpaからインストールします。\ncompany-lsp https://github.com/tigersoldier/company-lsp\ncompanyのバックエンドに設定\n(use-package \u0026#39;company-lsp :after (lsp-mode company) :init (push \u0026#39;company-lsp company-backends) ) lsp-ui https://github.com/emacs-lsp/lsp-ui\n(use-package lsp-ui :custom ;; lsp-ui-doc (lsp-ui-doc-enable nil) (lsp-ui-doc-header t) (lsp-ui-doc-include-signature t) (lsp-ui-doc-position \u0026#39;at-point) ;; top, bottom, or at-point (lsp-ui-doc-max-width 150) (lsp-ui-doc-max-height 30) (lsp-ui-doc-use-childframe t) (lsp-ui-doc-use-webkit t) ;; lsp-ui-flycheck (lsp-ui-flycheck-enable nil) ;; lsp-ui-peek (lsp-ui-peek-enable t) (lsp-ui-peek-peek-height 20) (lsp-ui-peek-list-width 50) (lsp-ui-peek-fontify \u0026#39;on-demand) ;; never, on-demand, or always ;; lsp-ui-imenu (lsp-ui-imenu-enable nil) (lsp-ui-imenu-kind-position \u0026#39;top) ;; lsp-ui-sideline (lsp-ui-sideline-enable nil) :commands lsp-ui-mode :config (add-hook \u0026#39;lsp-mode-hook \u0026#39;lsp-ui-mode) (eval-after-load \u0026#34;flymake\u0026#34; (setq flymake-fringe-indicator-position nil) ) ) 主な機能は以下\nlsp-ui-doc:\nリファレンス表示\n(lsp-ui-doc-use-childframe t)でchildframeを使ったドキュメント表示ができる。webkitでもできるみたいだけどうまくいかない lsp-ui-peek:\n定義ジャンプ lsp-ui-sideline\n現在行の情報を表示(肩情報はchildframeで表示され、表示情報が重複しているので非表示にした) 動いている様子は、各GitHubのページで確認\n以上で、lsp-modeの設定は終了\n各言語の設定 goのLanguage Serverはbingoを使うと良いそうです。\nbingo\nhttps://github.com/saibing/bingo\nこれもmelpaでインストールし、以下のように設定\n(use-package go-mode :config (add-hook \u0026#39;go-mode-hook #\u0026#39;lsp) ) まとめ lsp-modeを使って、IDEで提供されている機能をemacsでも使えるように設定しました。これでgo langで補間、リファレンス参照、定義ジャンプなどできるようになりました。その他設定の見直しは今後記事に落としていくということで。\n"
            }
    
        ,
            {
                "id": 84,
                "href": "https://www.if-blog.site/posts/emacs/emacs-use-package/",
                "title": "use-packagesを使ってみる",
                "section": "posts",
                "date" : "2018.11.20",
                "body": "これまで長い間放置しててきたemacsの設定たちを見直すついでに、use-pacakgeを使ってみようと思う。\nhttps://github.com/jwiegley/use-package\nuse-packageとは、emacsのパッケージ管理を強化するパッケージです。\nuse-packageのマクロを使用することで、\n統一した記述による可読性が向上 パッケージの遅延ロードの設定を簡潔にかけることによる起動の高速化 などのメリットがあります。\nシンプルな例 requireでのライブラリの読み込みは次のように書き換えることができます。\n(require \u0026#39;foo) (use-package foo) use-packageではキーワードを使うことで、パッケージロード時の振る舞いを指定することができます。\n:init パッケージが読み込まれる前に実行される。\n(use-package foo :init (setq foo-variable t)) :config パッケージがロードされた後に実行される。\n(use-package foo :init (setq foo-variable t) :config (foo-mode 1)) :commands auto-loadするコマンドを指定する。\n(use-package color-moccur :commands (isearch-moccur isearch-all) :bind ((\u0026#34;M-s O\u0026#34; . moccur) :map isearch-mode-map (\u0026#34;M-o\u0026#34; . isearch-moccur) (\u0026#34;M-O\u0026#34; . isearch-moccur-all)) :init (setq isearch-lazy-highlight t) :config (use-package moccur-edit)) この場合、isearch-moccurとisearch-allがcolor-moccur.elからauto-loadされる。\nkey-binding :bindキーワードを使用し以下のように設定する。\n(use-package ace-jump-mode :bind (\u0026#34;C-.\u0026#34; . ace-jump-mode)) リストを使い以下のように複数のキーバインドを設定することも可能\n(use-package hi-lock :bind ((\u0026#34;M-o l\u0026#34; . highlight-lines-matching-regexp) (\u0026#34;M-o r\u0026#34; . highlight-regexp) (\u0026#34;M-o w\u0026#34; . highlight-phrase))) キーバインドは:initと::commandsキーワードを使用し、以下のようにも設定できる。\n(use-package ace-jump-mode :commands ace-jump-mode :init (bind-key \u0026#34;C-.\u0026#34; \u0026#39;ace-jump-mode)) :mode 拡張子とメジャーモードの対応づけはauto-mode-alistで行なっていたが、:modeキーワードを使うことで簡潔に設定できるようになります。\n(use-package python-mode :mode ((\u0026#34;\\\\.py\\\\\u0026#39;\u0026#34;) (\u0026#34;\\\\.pyx\\\\\u0026#39;\u0026#34;)) ) :interpreter シェバンとメジャーモードの対応づけは、:interpreterキーワードを使って設定できます。\n(use-package python-mode :interpreter ((\u0026#34;python\u0026#34; . python-mode) (\u0026#34;python\u0026#34; . python-mode)) :hooks :hookキーワードを使用して、以下のように設定できます。\n(use-package ace-jump-mode :hook prog-mode) (use-package ace-jump-mode :hook (prog-mode . ace-jump-mode)) また、従来のようにadd-hookを用いても設定が可能。\n(use-package ace-jump-mode :commands ace-jump-mode :init (add-hook \u0026#39;prog-mode-hook #\u0026#39;ace-jump-mode)) まとめ use-packageの使い方を簡単にまとめてみました。このくらいあればある程度の設定はできるでしょう。余力があれば加筆します。\n"
            }
    
        ,
            {
                "id": 85,
                "href": "https://www.if-blog.site/posts/falcon/falcon-img-upload/",
                "title": "falconでアップロードサーバーを作る",
                "section": "posts",
                "date" : "2018.10.16",
                "body": "はじめに 現在、サーバー上にHugoとHugoが生成した静的ファイルを置いてブログを公開している。 静的ファイルの生成とmdファイルや画像ファイルのアップロードのためfalconを使ってapiサーバーを作った。 falconをつかったのは、flaskやbottleは使ったことあったので、使ったことないものをということで。\nクライアント クライアントにはpythonのrequestsを使います。\nfileにタプルでファイル名を明示的に渡せる。 postの引数にfilesで渡すと、contents-typeもよしなにしてくれるんですね。\nimage_name = \u0026#34;test.png\u0026#34; with open(image_file_path, \u0026#34;rb\u0026#34;) as image: files = {\u0026#39;file\u0026#39;: (image_name, image)} response = requests.post(url, files=files) print(response.text) サーバー falconはコンテナ上で動かします。ファイルアップロードのリクエストを受けると、 サーバー上のストレージに画像を保存します。\nなんかできないと思ってたら、middlewareにMultipartMiddlewareを指定しないとだめだった。\nfileは次のようにrequestから取得できる\nimage = req.get_param(\u0026#39;file\u0026#39;) raw = image.file.read() ファイル名は次のように取得\nimage_name = image.filename 実際は、corsとかあるがそれを省いた、アップロードだけの全体は以下のようになる。\nfrom falcon_multipart.middleware import MultipartMiddleware import falcon import json class UploadImage(object): def on_post(self, req, resp): image = req.get_param(\u0026#39;file\u0026#39;) raw = image.file.read() image_name = image.filename filepath = os.path.join(BASE_DIR, \u0026#34;static\u0026#34;, image_name) try: with open(filepath, \u0026#39;wb\u0026#39;) as f: f.write(raw) except IOError: print(\u0026#34;save file faild :\u0026#34; + filepath) resp.body = to_resp(200, \u0026#34;save img \u0026#34; + filepath) def to_resp(status_code, contents): resp = { \u0026#34;status\u0026#34;: status_code, \u0026#34;contents\u0026#34;: contents } return json.dumps(resp) app = falcon.API(middleware=[MultipartMiddleware()]) app.add_route(\u0026#34;/image\u0026#34;, UploadImage()) if __name__ == \u0026#34;__main__\u0026#34;: from wsgiref import simple_server httpd = simple_server.make_server(\u0026#34;0.0.0.0\u0026#34;, 8000, app) httpd.serve_forever() "
            }
    
        ,
            {
                "id": 86,
                "href": "https://www.if-blog.site/posts/falcon/falcon-tips/",
                "title": "falconを使ってみた",
                "section": "posts",
                "date" : "2018.10.16",
                "body": "はじめに Hugoをおいているwebサーバーに画像や、mdファイルをアップロードするためのapiサーバーとしてfalconを使ってみた。ORMなど必要なく、簡単なアップロードができれば良いということで、軽量さとシンプルさが特徴のfalconを使う。\n簡単なtipsをメモしておく。\nシンプルな例 falconの公式に乗っているように、シンプルな例は次のようになる。\n# sample.py import falcon class QuoteResource: def on_get(self, req, resp): \u0026#34;\u0026#34;\u0026#34;Handles GET requests\u0026#34;\u0026#34;\u0026#34; quote = { \u0026#39;quote\u0026#39;: ( \u0026#34;I\u0026#39;ve always been more interested in \u0026#34; \u0026#34;the future than in the past.\u0026#34; ), \u0026#39;author\u0026#39;: \u0026#39;Grace Hopper\u0026#39; } resp.media = quote api = falcon.API() api.add_route(\u0026#39;/quote\u0026#39;, QuoteResource()) 処理シーケンス falconにはmiddlewareの他に、hooksというものが使える。 hooksを含めた処理シーケンスは次のようになる。\nMiddleware\u0026#39;s process_request Middleware\u0026#39;s process_resource Hook\u0026#39;s before Resource\u0026#39;s on_**** Hook\u0026#39;s after Middleware\u0026#39;s process_response 引用：https://qiita.com/yohjizzz/items/f46bb3bc7b7c40768836\nmiddlewareを使った、認証系の共通処理はよく見るが、hookが使えるのは良いですね。\nhooks hooksは以下のように使う。(公式)\ndef validate_image_type(req, resp, resource, params): .... @falcon.before(validate_image_type) def on_post(self, req, resp): pass 処理を差し込みたいメソッドにでコレーターでつけるだけ。\nmiddleware corsくらいしか使わなかったが、dbのconnection poolとかに使うようのものとかあるんだろう。(公式)\nclass CORSMiddleware: def process_request(self, req, resp): resp.set_header(\u0026#39;Access-Control-Allow-Origin\u0026#39;, \u0026#39;*\u0026#39;) エラーハンドリング routeが見つからなかった時に、エラーリスポンスを返したいときは以下のようにする。(公式)\nclass Index(object): def on_get(self, req, resp): resp.body = json.dumps({\u0026#34;message\u0026#34;:\u0026#34;ok\u0026#34;}) app = falcon.API() app.add_route(\u0026#34;/\u0026#34;, Index()) app.add_sink(handle_404, \u0026#39;\u0026#39;) if __name__ == \u0026#34;__main__\u0026#34;: from wsgiref import simple_server httpd = simple_server.make_server(\u0026#34;127.0.0.7\u0026#34;, 8000, app) httpd.serve_forever() exceptionなどが起きた場合でも、200のリスポンスを返し、レスポンスボディの中でエラーコードとメッセージを返したかったが、うまく行かなかったので、これは次の機会にやる。\nrequestの取得 requsetの取得は以下のようになる。(公式)\nclass HealthCheck(object): def on_get(self, req, resp): print(req.headers) # headerの取得 print(req.params[\u0026#39;name\u0026#39;]) # request paramの取得 # request bodyの取得 body = req.stream.read().decode(\u0026#34;utf-8\u0026#34;) data = json.loads(body) name = data[\u0026#39;name\u0026#39;] app = falcon.API() app.add_route(\u0026#34;/\u0026#34;, index()) if __name__ == \u0026#34;__main__\u0026#34;: from wsgiref import simple_server httpd = simple_server.make_server(\u0026#34;127.0.0.7\u0026#34;, 8000, app) httpd.serve_forever() "
            }
    
        ,
            {
                "id": 87,
                "href": "https://www.if-blog.site/posts/hugo/hugo-start-article/",
                "title": "Hugoでブログを作る",
                "section": "posts",
                "date" : "2018.10.16",
                "body": "はじめに Hugoとは、go言語で書かれた、静的なwebサイトをbuildingするためのframeworkです。 Hugoを使ってブログを作ってみたので構築方法を簡単にまとめておきます。\nサイトの作成 インストール brew install hugo テンプレートの作成 hugo new site hugo-test これで、次のようにテンプレートが作成されます。\n$ ls hugo-test/ archetypes/ config.toml content/ data/ layouts/ static/ themes/ テーマの適応 https://themes.gohugo.io/ ここからテーマを探す。\n例として、https://themes.gohugo.io/hugo-theme-learn/ を使います。\n$ cd hugo-test/themes $ git clone https://github.com/matcornic/hugo-theme-learn.git テーマを適応させるために、config.tomlにthemeを記述します。\ntheme = \u0026#34;hugo-theme-learn\u0026#34; サーバーの起動 $ hugo server -D -t hugo-theme-learn オプションDでDraftフラグのついた記事の確認、オプションtでテーマの適応です。 cloneしてきたtheme内の、layoutとstaticなどを、作成したテンプレート内のlayout、staticに移動しておくと、templateのマイナーチェンジが行えます。\nデフォルトでhttp://localhost:1313/で起動するのでアクセスしてみる。\n記事の追加 記事の作成\n$ hugo new post/test.md hugo_test/content/post/test.md created 以下のようなmarkdownファイルが作成される\ncat content/post/test.md --- title: \u0026#34;Test\u0026#34; date: 2018-10-15T18:51:47+09:00 draft: false --- ここで、draftは記事を後悔するかのフラグ この出力フォーマットも、独自に設定が可能\n$ hugo server --watch watchフラグをつけると、mdファイルが更新されるたびに、web側に更新が反映される。 markdownの記事を書きながら、フォーマットを確認するのに使えそう。\nデプロイ 以下のコマンドで実際にpublic以下にstaticファイルが吐き出される\n$ hugo -t hugo-theme-learn $ ls public/ 404.html categories css fonts images index.html index.xml js mermaid sitemap.xml tags webfonts このpublicディレクトリをwebサーバーに与えるだけでおk。\nまとめ Hugoのインストールからデプロイまでをまとめました。 html内で使える変数や、数式、画像の表示はまた別でまとめます。\n"
            }
    
        ,
            {
                "id": 88,
                "href": "https://www.if-blog.site/posts/hugo/hugo-href-to-target-blank/",
                "title": "Hugoでリンクを新しいウィンドウで開く",
                "section": "posts",
                "date" : "2018.10.16",
                "body": "リンクは新しいウィンドウで開いて欲しい派なんだけど、Hugoではデフォルトで通常のリンクの開きかた(?)をする。\nhrefではtarget='_blank'を指定すれば良いのだが、markdownでどうすれば良いのか調べた。 Hugoでは次のようにconfig.tomlするといける。\n[blackfriday] hrefTargetBlank = true 簡単！\n参考：\nhttps://gohugo.io/getting-started/configuration/ https://www.meganii.com/blog/2017/02/25/hugo-markdown-href-target-blank/ "
            }
    
        ,
            {
                "id": 89,
                "href": "https://www.if-blog.site/posts/hugo/hugo-ga/",
                "title": "Hugoにgoogle analyticsを導入する",
                "section": "posts",
                "date" : "2018.10.16",
                "body": "Hugoにgoogle analytics(ga)を導入しようと思ったら意外と簡単だった。\nまず、gaアカウントを作成。\nconfig.tomlに\ngoogleAnalytics = \u0026#34;{ga tracking ID}\u0026#34; を設定するだけ。\n"
            }
    
        ,
            {
                "id": 90,
                "href": "https://www.if-blog.site/posts/hugo/hugo-long-time-no-see/",
                "title": "Hugoのディレクトリ構造やデプロイ方法の備忘録",
                "section": "posts",
                "date" : "2018.10.16",
                "body": "久しぶりにHugoを触ったら、ディレクトリ構造からデプロイ方法までさっぱり忘れていたので、久しぶりに触った際のために備忘録を残す。\nディレクトリ 基本的なディレクトリは以下\nmarkdownを書いたりと普段触るのはcontentディレクトリになる。\nデプロイ後生成されるpublicディレクトリはサーバーなどで公開する。\narchetypes/ ・・・ markdown記事を生成する際のテンプレートなど config.toml ・・・ 設定ファイル content/ ・・・　markdownの記事を配置 layouts/　・・・ Htmlの共通部分 public/ ・・・ デプロイ後に生成される静的ファイル群 static/ ・・・ cssなどの静的ファイルを配置 themes/ ・・・ テーマを配置 デプロイ hugoのプロジェクトルートで以下のコマンドを打つ\n$ hugo -t ./ 記事を書くときには以下のコマンドを使う。\n$ hugo server -t ./ localにサーバーが立ち上がり、markdownを書きながら生成される記事を確認できる。\n"
            }
    
        ,
            {
                "id": 91,
                "href": "https://www.if-blog.site/posts/hugo/hugo-code-highlight/",
                "title": "Hugoの記事でコードハイライトする",
                "section": "posts",
                "date" : "2018.10.16",
                "body": "Hugoでコードのシンタックスハイライトする場合のメモ\nHugo自体の機能を使う方法と、ハイライトする機能を持つjavascriptを読み込む2つの方法がある。\nHugo自体の機能使う Hugo 0.28からGo機能でシンタックスハイライトが可能になった。\nSyntax Highlighting\nconfig.tomlに以下のように記述\npygmentsCodefences = true pygmentsUseClasses = true 以下のコマンドでcssを生成する。\n$hugo gen chromastyles --style=monokai \u0026gt; syntax.css 生成したcssをindex.htmlなどから読み込むと反映される。\n--styleオプションの指定は、pygmentsから探す。\nhighlightjsを使う highlightjs\n対応言語が多いのがメリット。\n以下のようにcdnからとってくるようにしておくと良い。\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/default.min.css\u0026#34;\u0026gt; \u0026lt;script src=\u0026#34;//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt;hljs.initHighlightingOnLoad();\u0026lt;/script\u0026gt; 一行目の記述を以下のように書き換えることでテーマを選択できる。\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/zenburn.min.css\u0026#34;\u0026gt; "
            }
    
        ,
            {
                "id": 92,
                "href": "https://www.if-blog.site/posts/hugo/hugo-add-site-description/",
                "title": "Hugoの記事にdescriptionを追加する",
                "section": "posts",
                "date" : "2018.10.16",
                "body": "Hugoで書いた記事が検索で引っかからないなと思っていたら、descriptionタグが設定されていませんでした。 これでは検索に引っかからないのでdescriptionタグをつけましょう!\nhtml側で次のように書くと、markdownに書いたdescriptionが参照できます。\n{{ .Description }} markdown側では、ヘッダーに次のように記述します。\ndescription=\u0026#34;hogehoge\u0026#34; これまで書いた記事全てに、descriptionをつけていくのは面倒ですね。\n次のようにサイトサマリーを参照することも可能です\n{{ .Summary }} ただし、デフォルトでサマリーが大きくなりすぎるので、config.tomlに次のような記述を追加します。\nhasCJKLanguage = true こうすることでサマリーがいい感じのサイズになります。\n参考：https://blog.awm.jp/2016/01/02/hugo/\n最終的に、header.htmlなどに次のように記述しておけば、descriptionを書いておくとそちらが使われるようになります。\n{{ if .Description }} \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;{{ .Description }}\u0026#34;\u0026gt; {{ else }} \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;{{ .Summary }}\u0026#34;\u0026gt; {{ end }} サイトのディスクリプションと記事のディスクリプションを分ける(追記) {{ if .IsHome }}でトップページかどうか判定できるので、トップページならば、configのディスクリプションを使い、記事内ならば記事に設定してあるディスクリプションを使う。\nhtml側は次のように記述する。\n{{ if .IsHome }} {{ with .Site.Params.description }} \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;{{ . }}\u0026#34;\u0026gt; {{ end }} {{ end }} {{ if .Description }} \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;{{ .Description }}\u0026#34;\u0026gt; {{ else }} \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;{{ .Summary }}\u0026#34;\u0026gt; {{ end }} configに次のように記述する。\n[params] description=\u0026#34;hogehoge\u0026#34; configで任意の変数を指定するのは、paramsの中じゃないとだめなので注意。 参考：https://gohugo.io/getting-started/configuration/\n他のもっとスマートな方法があれば教えてください。\n"
            }
    
        ,
            {
                "id": 93,
                "href": "https://www.if-blog.site/posts/hugo/hugo-on-image/",
                "title": "Hugoの記事に画像を載せる",
                "section": "posts",
                "date" : "2018.10.16",
                "body": "Hugoに画像を載せるときは、shortcodesを使うと良いらしい。\n画像を配置するディクトリを以下のように作っておく。\n{root}/content/post/images/ 以下のように記述すると表示できる。\n{{\u0026lt; figure src=\u0026#34;../../images/test.png\u0026#34; title=\u0026#34;test\u0026#34; width=\u0026#34;320\u0026#34; height=\u0026#34;640\u0026#34; \u0026gt;}} or\n[!tag](path) 相対パスで記述でき、width、heightも設定可能。\n生成されるHTMLは以下のようになる。\n\u0026lt;figure\u0026gt; \u0026lt;img src=\u0026#34;../../images/test.png\u0026#34; /\u0026gt; \u0026lt;figcaption\u0026gt; \u0026lt;h4\u0026gt;test\u0026lt;/h4\u0026gt; \u0026lt;/figcaption\u0026gt; \u0026lt;/figure\u0026gt; 画像を実際に表示\n記事と同じディレクトリに画像を配置しておくと、シンプルなパスで記述できるので、その辺は好みで。\n画像が多くなりすぎるとgitで管理したくないので、クラウドストレージとかに移行するとかしないとな\u0026hellip;\n"
            }
    
        ,
            {
                "id": 94,
                "href": "https://www.if-blog.site/posts/hugo/hugo-date-sort/",
                "title": "Hugoの記事を更新順にソートする",
                "section": "posts",
                "date" : "2018.10.16",
                "body": "Hugoの記事を更新順にソートするメモです。\nまず、Hugoの記事に更新時刻情報を付与します。markdownのヘッダーにdate情報を書いても良いのですが、gitで管理している場合には、Lastmodで更新時刻が取得できるのでそれを使います。\nそのほかにもgitから情報が取得できます。https://gohugo.io/variables/git/\nconfig.tomlに以下のように記述します。\nenableGitInfo=true 以下のように使う。\n{{ .Lastmod.Format \u0026#34;2006/01/02\u0026#34; }} この更新事項を使って、記事を並び替えます。\n記事のリストを表示するhtmlはもともと以下のように記述してありました。\n{{ $paginator := .Paginate (where .Data.Pages \u0026#34;Type\u0026#34; \u0026#34;post\u0026#34;) 10 }} {{ range $paginator.Pages }} 以下のリンクを参考にします。\nhttps://gohugo.io/templates/lists/\nlastmod以外にも並び替えが行えます。\n結局、以下のように記述しました。\n{{ $paginator := .Paginate (where .Data.Pages \u0026#34;Type\u0026#34; \u0026#34;post\u0026#34;).ByLastmod.Reverse 10 }} {{ range $paginator.Pages }} "
            }
    
        ,
            {
                "id": 95,
                "href": "https://www.if-blog.site/posts/hugo/hugo-git-pages/",
                "title": "HugoをGitHub Pagesで公開する",
                "section": "posts",
                "date" : "2018.10.16",
                "body": "GitHub Pagesを使ってHugoを公開する手順をメモ\nやりたいこととしては、\nHugoのプロジェクトを丸ごとgitで管理 プロジェクトの下に作られるpublicをGitHub Pagesで公開 独自ドメインの設定 の3つです。\nGitHub Pagesで公開 まずは、Hugoのプロジェクトルートをgitで管理します。\nGitHub Pagesで公開するために、GitHubのリポジトリの設定からPagesで公開するための設定を行います。このとき、そのまま公開すると、Hugoをビルドして生成されるファイルはプロジェクトルート直下のpublic以下に生成されるので、\nhttps://{ユーザー名}.github.io/{リポジトリ名}/{public} というように、URLにpublicが付いてしまいます。これは嫌なので、git subtreeを使います。\ngh-pagesという新しいブランチを作成し、public以下のファイルのみをpushしています。github pagesではgh-pagesを公開するよう設定します。\n$ git subtree push --prefix public/ . gh-pages $ git push origin gh-pages:gh-pages GitHubから独自ドメインの設定をすると、ブランチgh-pagesにCNAMEファイルが追加、commitされるのですが、この変更をローカルのmasterに取り込めないという問題が発生しました。このため、ローカルで記事を追加したときなど、non-fast-forwardでpush、pullもできなくなりました。そこで、しょうがなくmaster側にローカルからCNAMEファイルを追加しました。\nそれでもうまくいかないときは、エラー文のcommitのハッシュを使って無理やりpushしてとりあえず回避してます。\n例\n$ git subtree push --prefix public/ . gh-pages git push using: . gh-pages To . 59 (58) ! [rejected] 95625bda7b4247ff0d05b2e76c8db1cbb7e87dad -\u0026gt; gh-pages (non-fast-forward) error: failed to push some refs to \u0026#39;.\u0026#39; hint: Updates were rejected because a pushed branch tip is behind its remote hint: counterpart. Check out this branch and integrate the remote changes hint: (e.g. \u0026#39;git pull ...\u0026#39;) before pushing again. hint: See the \u0026#39;Note about fast-forwards\u0026#39; in \u0026#39;git push --help\u0026#39; for details. $ git push origin 95625bda7b4247ff0d05b2e76c8db1cbb7e87dad:gh-pages 独自ドメインの設定 今回は、お名前.comでドメインを取得しました。お名前.comでDNSの設定をします。\nGitHub Pagesのドメインは\nhttps://{ユーザー名}.github.io/{リポジトリ名} なので、取得したドメインでGitHub PagesのドメインがひけるようにCNAMEの設定を行います。\nGitHub側では、リポジトリの設定からCustom domainを取得したドメインで設定します。Custom domainを設定すると、Enforce HTTPSにチェックを入れることができるので、設定するとhttpsで通信が行えるようになります。\n"
            }
    
        ,
            {
                "id": 96,
                "href": "https://www.if-blog.site/posts/typescript/react-render-prop-typescript/",
                "title": "reactのrender propsをtypescriptでやる",
                "section": "posts",
                "date" : "0001.01.01",
                "body": "childrenに引数渡したい時、render propを使えばできる。 functional componentでのやり方がわからなくていろいろハマったのでメモ\nhttps://reactjs.org/docs/render-props.html\n結論 // children interface ChildrenSampleProps { userName: string } const ChildrenSample: React.FC\u0026lt;ChildrenSampleProps\u0026gt; = ({ userName }) =\u0026gt; { return \u0026lt;\u0026gt;user: {userName}\u0026lt;/\u0026gt; } // parent interface ParentProps { children: (props: ChildrenSampleProps) =\u0026gt; JSX.Element } const Parent: React.FC\u0026lt;ParentProps\u0026gt; = ({ children }) =\u0026gt; { const userName = \u0026#39;user!!!\u0026#39; return \u0026lt;\u0026gt;{children({ userName })}\u0026lt;/\u0026gt; } // sample to use const Sample: React.FC = () =\u0026gt; { return ( \u0026lt;Parent\u0026gt; {(props) =\u0026gt; \u0026lt;ChildrenSample {...props} /\u0026gt;} \u0026lt;/Parent\u0026gt; ); } childrenの型をchildren: (props: ChildrenSampleProps) =\u0026gt; JSX.Elementのようにするといける\n"
            }
    
]