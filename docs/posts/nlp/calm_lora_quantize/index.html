<!DOCTYPE html>
<html lang="ja">
  <head>
    <meta charset="utf-8" />
<title>OpenCALM-7Bをloraで学習して、quantizeするまで</title>

  
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-127416809-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-127416809-1');
  </script>
  


<script
  data-ad-client="ca-pub-7303877370233278"
  async
  src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"
></script>

<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="alternate"
  type="application/rss+xml"
  href="https://www.if-blog.siteindex.xml"
  title="アンドロイドは推理小説を書くか?"
/>

<link rel="stylesheet" href="https://www.if-blog.site/fontawesome/css/all.min.css" />




<link
  crossorigin="anonymous"
  href="/css/styles.min.css"
  integrity=""
  rel="preload stylesheet"
  as="style"
/>




<link
  id="dark-mode-theme"
  crossorigin="anonymous"
  href="/css/dark.min.css"
  integrity=""
  rel="preload stylesheet"
  as="style"
/>




<link
  rel="stylesheet"
  href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.css"
/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/contrib/auto-render.min.js"></script>
<script>
  $(document).ready(function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: '[[', right: ']]', display: true },
        { left: '$', right: '$', display: false },
      ],
    })
  })
</script>


<script>
  var darkTheme = document.getElementById('dark-mode-theme')
  var storedTheme = localStorage.getItem('dark-mode-storage')

  if (storedTheme === 'dark') {
    darkTheme.disabled = false
  } else if (storedTheme === 'light') {
    darkTheme.disabled = true
  }
</script>


<script defer crossorigin="anonymous" src="/js/theme.js" integrity=""></script>


<script defer crossorigin="anonymous" src="/js/instantpage.min.js" integrity=""></script>

  
    <meta name="description" content="cyberagent/open-calm-7bをLoraを用いて学習し、quantizeするまでやっていきます
https://huggingface.co/cyberagent/open-calm-7b
gpt-neoxのlora weight mergeの記事が見つからなかったのでメモとして
動" />
  


<meta name="generator" content="Hugo 0.110.0">
  </head>
  <body>
    
  




  <header>
    <nav class="navbar">
  <div class="nav">
    

    <ul class="nav-links">
      
        
          <li>
            <a href="/" name="Home" class="tooltip"
              ><i class="fas fa-home fa-lg"></i>
              <span>Home</span>
            </a>
          </li>
          
      
        
          <li>
            <a href="/pages/about" name="About" class="tooltip"
              ><i class="fas fa-user fa-lg"></i>
              <span>About</span>
            </a>
          </li>
          
      
        
          <li>
            <a href="/tags" name="Tags" class="tooltip"
              ><i class="fas fa-tag fa-lg"></i>
              <span>Tags</span>
            </a>
          </li>
          
      
        
          <li>
            <a href="/pages/search" name="Search" class="tooltip"
              ><i class="fas fa-search fa-lg"></i>
              <span>Search</span>
            </a>
          </li>
          
      
    </ul>
  </div>
</nav>

    <div class="intro-header">
      <div class="container">
        <div class="posts-heading">
          
            

              
              <h1>
		
		OpenCALM-7Bをloraで学習して、quantizeするまで
		
		
                </h1>
              

              

              
            
          
          
          
          
        </div>
      </div>
    </div>
  </header>
  

    
  <div class="container" role="main">
    <article class="article" class="blog-post">
      
        <div class="blog-tags">
          <i class="fas fa-tag" style="color:#111111"></i>
          
            <a href="https://www.if-blog.site/tags/nlp/">nlp</a
            >&nbsp;
          
            <a href="https://www.if-blog.site/tags/deeplearning/">deeplearning</a
            >&nbsp;
          
            <a href="https://www.if-blog.site/tags/python/">python</a
            >&nbsp;
          
            <a href="https://www.if-blog.site/tags/huggingface/">huggingface</a
            >&nbsp;
          
        </div>
      

      
        <div style="margin-bottom: 10px;">
          <i class="fa fa-calendar-alt"></i
          >&nbsp;2023-05-22
        </div>
      

      


      <div class="article-main">
        <p><code>cyberagent/open-calm-7b</code>をLoraを用いて学習し、quantizeするまでやっていきます</p>
<p><a href="https://huggingface.co/cyberagent/open-calm-7b">https://huggingface.co/cyberagent/open-calm-7b</a></p>
<p>gpt-neoxのlora weight mergeの記事が見つからなかったのでメモとして</p>
<p>動作環境はcolab T4 ハイメモリ</p>
<h2 id="lora学習">lora学習</h2>
<p>rinnaと同様に、gpt-neoxベースなので、以下を使ってfinetuningしていく。</p>
<p><a href="https://github.com/leehanchung/lora-instruct/blob/main/finetune.py">https://github.com/leehanchung/lora-instruct/blob/main/finetune.py</a></p>
<p>templateをそれっぽく直す</p>
<div class="code-block">
  <div class="body"><pre tabindex="0"><code>{
    &#34;description&#34;: &#34;Template used by Alpaca-LoRA.&#34;,
    &#34;prompt_input&#34;: &#34;以下は、タスクを記述する命令と、さらなるコンテキストを提供する入力の組み合わせです。依頼を適切に完了させる応答を書きなさい。\n\n### 指示:\n{instruction}\n\n### 入力:\n{input}\n\n### 応答:\n&#34;,
    &#34;prompt_no_input&#34;: &#34;以下は、ある作業を記述した指示です。依頼を適切に完了させる応答を書きなさい\n\n### 指示:\n{instruction}\n\n### 応答:\n&#34;,
    &#34;response_split&#34;: &#34;### 応答:&#34;
}</code></pre>
  </div>

</div>
<p>trainingはいつも通り<br>
パラメタはそれぞれの環境に合わせて調整</p>
<div class="code-block">
  <div class="body"><pre tabindex="0"><code>base_model=&#34;cyberagent/open-calm-7b&#34;
data_path=&#34;kunishou/databricks-dolly-15k-ja&#34;
template = &#39;dolly_ja&#39;

!python3 finetune.py \
--base_model=$base_model \
--batch_size=128 \
--micro_batch_size=2 \
--prompt_template_name=$template \
--cutoff_len=1024 \
--output_dir=$output_path \
--num_epochs=2 \
--data_path=$data_path</code></pre>
  </div>

</div>
<p>(trainingはcolab T4 ハイメモリで試してないので動かないかも)</p>
<h2 id="lora-weightをmerge">lora weightをmerge</h2>
<p>以下を参考にmerge</p>
<p><a href="https://github.com/tloen/alpaca-lora/blob/main/export_hf_checkpoint.py">https://github.com/tloen/alpaca-lora/blob/main/export_hf_checkpoint.py</a></p>
<p>huggingfaceのpeftライブラリがmerge用のメソッドを提供しているので、それを使う</p>
<div class="code-block">
  <div class="body"><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>lora_model <span style="color:#ff79c6">=</span> PeftModel<span style="color:#ff79c6">.</span>from_pretrained(
</span></span><span style="display:flex;"><span>    <span style="color:#f1fa8c">&#39;base_model&#39;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f1fa8c">&#39;lora_weight_dir&#39;</span>,
</span></span><span style="display:flex;"><span>    device_map<span style="color:#ff79c6">=</span>{<span style="color:#f1fa8c">&#34;&#34;</span>: <span style="color:#f1fa8c">&#34;cpu&#34;</span>},
</span></span><span style="display:flex;"><span>    torch_dtype<span style="color:#ff79c6">=</span>torch<span style="color:#ff79c6">.</span>float16,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>lora_model <span style="color:#ff79c6">=</span> lora_model<span style="color:#ff79c6">.</span>merge_and_unload()</span></span></code></pre></div>
  </div>

</div>
<p>mergeのコードはここを参照<br>
<a href="https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora.py#L343">https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora.py#L343</a></p>
<p><code>export_hf_checkpoint.py</code>の中で、モデルをロードしている部分をGPTNeoXに変更<br>
<code>AutoModelForCausalLM</code>は<code>save_pretrained</code>が呼び出せないので、ここだけ<code>GPTNeoXForCausalLM</code>を利用する。</p>
<div class="code-block">
  <div class="body"><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tokenizer <span style="color:#ff79c6">=</span> LlamaTokenizer<span style="color:#ff79c6">.</span>from_pretrained(BASE_MODEL)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>base_model <span style="color:#ff79c6">=</span> LlamaForCausalLM<span style="color:#ff79c6">.</span>from_pretrained(
</span></span><span style="display:flex;"><span>    BASE_MODEL,
</span></span><span style="display:flex;"><span>    load_in_8bit<span style="color:#ff79c6">=</span><span style="color:#ff79c6">False</span>,
</span></span><span style="display:flex;"><span>    torch_dtype<span style="color:#ff79c6">=</span>torch<span style="color:#ff79c6">.</span>float16,
</span></span><span style="display:flex;"><span>    device_map<span style="color:#ff79c6">=</span>{<span style="color:#f1fa8c">&#34;&#34;</span>: <span style="color:#f1fa8c">&#34;cpu&#34;</span>},
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>(省略)
</span></span><span style="display:flex;"><span>LlamaForCausalLM<span style="color:#ff79c6">.</span>save_pretrained(
</span></span><span style="display:flex;"><span>    base_model, <span style="color:#f1fa8c">&#34;./hf_ckpt&#34;</span>, state_dict<span style="color:#ff79c6">=</span>deloreanized_sd, max_shard_size<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;400MB&#34;</span>
</span></span><span style="display:flex;"><span>)</span></span></code></pre></div>
  </div>

</div>
<p>↓</p>
<div class="code-block">
  <div class="body"><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> transformers <span style="color:#ff79c6">import</span> AutoModelForCausalLM, AutoTokenizer, GPTNeoXForCausalLM
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#ff79c6">=</span> AutoTokenizer<span style="color:#ff79c6">.</span>from_pretrained(BASE_MODEL)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>base_model <span style="color:#ff79c6">=</span> AutoModelForCausalLM<span style="color:#ff79c6">.</span>from_pretrained(
</span></span><span style="display:flex;"><span>    BASE_MODEL,
</span></span><span style="display:flex;"><span>    load_in_8bit<span style="color:#ff79c6">=</span><span style="color:#ff79c6">False</span>,
</span></span><span style="display:flex;"><span>    torch_dtype<span style="color:#ff79c6">=</span>torch<span style="color:#ff79c6">.</span>float16,
</span></span><span style="display:flex;"><span>    device_map<span style="color:#ff79c6">=</span>{<span style="color:#f1fa8c">&#34;&#34;</span>: <span style="color:#f1fa8c">&#34;cpu&#34;</span>},
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>(省略)
</span></span><span style="display:flex;"><span>GPTNeoXForCausalLM<span style="color:#ff79c6">.</span>save_pretrained(
</span></span><span style="display:flex;"><span>    base_model,
</span></span><span style="display:flex;"><span>    save_directory<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">f</span><span style="color:#f1fa8c">&#34;./hf_ckpt&#34;</span>, state_dict<span style="color:#ff79c6">=</span>deloreanized_sd, max_shard_size<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;400MB&#34;</span>
</span></span><span style="display:flex;"><span>)</span></span></code></pre></div>
  </div>

</div>
<h2 id="ggml形式に変換">ggml形式に変換</h2>
<p>gpt-neoxのggml形式への変換とquantizeは以下を参考にする<br>
<a href="https://github.com/togethercomputer/redpajama.cpp/blob/master/examples/redpajama/scripts/convert_gptneox_to_ggml.py">https://github.com/togethercomputer/redpajama.cpp/blob/master/examples/redpajama/scripts/convert_gptneox_to_ggml.py</a></p>
<p><code>convert_gptneox_to_ggml.py</code>のモデルのロードを修正
tokenizerはベースモデルを指定、AutoModelForCausalLMはマージしたモデルが保存されたdirを指定</p>
<div class="code-block">
  <div class="body"><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>merged_weight_dir <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;./hf_ckpt&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#ff79c6">=</span> AutoTokenizer<span style="color:#ff79c6">.</span>from_pretrained(<span style="color:#f1fa8c">&#39;cyberagent/open-calm-7b&#39;</span>)
</span></span><span style="display:flex;"><span>model <span style="color:#ff79c6">=</span> AutoModelForCausalLM<span style="color:#ff79c6">.</span>from_pretrained(merged_weight_dir, torch_dtype<span style="color:#ff79c6">=</span>torch<span style="color:#ff79c6">.</span>float16 <span style="color:#ff79c6">if</span> ftype <span style="color:#ff79c6">==</span> <span style="color:#bd93f9">1</span> <span style="color:#ff79c6">else</span> torch<span style="color:#ff79c6">.</span>float32)                                             </span></span></code></pre></div>
  </div>

</div>
<h2 id="quantize">quantize</h2>
<p>ここはreadme通り</p>
<p>compile</p>
<div class="code-block">
  <div class="body"><pre tabindex="0"><code>!make quantize-gptneox</code></pre>
  </div>

</div>
<p>q8_0を指定してrun</p>
<div class="code-block">
  <div class="body"><pre tabindex="0"><code>%cd (project_dir)/redpajama.cpp

model_name=&#34;ggml形式のmodelを指定&#34;
!python ./examples/redpajama/scripts/quantize-gptneox.py $model_name --quantize-output-type q8_0</code></pre>
  </div>

</div>
<h2 id="infarence">infarence</h2>
<p>動作確認<br>
ここもreadme通り<br>
このスクリプトだとgpu使わないみたいなので遅い、動作確認用</p>
<div class="code-block">
  <div class="body"><pre tabindex="0"><code>%cd (project_dir)/redpajama.cpp
!make redpajama</code></pre>
  </div>

</div>
<p>run</p>
<div class="code-block">
  <div class="body"><pre tabindex="0"><code>%cd (project_dir)/redpajama.cpp


instruction = &#34;日本で1番高い山を教えてください。&#34;
prompt=f&#34;&#34;&#34;
以下は、ある作業を記述した指示です。依頼を適切に完了させる応答を書きなさい

### 指示:
{instruction}

### 応答:
&#34;&#34;&#34;

!./redpajama -m &#34;$model&#34; \
  -c 1024 \
  -b 128 \
  -n 1 \
  -t 8 \
  --color \
  --top_k 30 \
  --top_p 0.95 \
  --temp 0.2 \
  --repeat_last_n 3 \
  --repeat_penalty 1.1 \
  --seed 0 \
  --n_predict 256 \
  --verbose-prompt \
  -p &#34;$prompt&#34;</code></pre>
  </div>

</div>
<p>出力</p>
<div class="code-block">
  <div class="body"><pre tabindex="0"><code>以下は、ある作業を記述した指示です。依頼を適切に完了させる応答を書きなさい

### 指示:
日本で��番高い山を教えてください。

### 応答:
山は、富士山と北アルプスです。富士山と北アルプスの標高は、3300m、北アルプスの標高は、3180mです。富士山は、3867m、北アルプスは、3180mです。</code></pre>
  </div>

</div>
<p>一部文字化けしているが、それっぽい回答</p>
<p>ちなみに、正解は</p>
<blockquote>
<p>富士山の標高 3,776 m</p>
</blockquote>
<blockquote>
<p>北アルプスは，日本で3番目に高い奥穂高岳 (標高3190m) や4番目の槍ヶ岳 (同3180m) などを含めて2500～3000m級の峰が連なる日本有数の山岳地帯を形成している．</p>
</blockquote>

      </div>
      <div class="related-article">
        

  <h2>See Also</h2>
  <ul>
    
      <li><a href="/posts/nlp/rinna_lora_ja/">loraで学習する場合のpromptって何でも良いんだっけ？rinna instruction 3Bで試す</a></li>
    
      <li><a href="/posts/nlp/rinna_3b_cpp/">rinna 3Bをcppで動かす</a></li>
    
      <li><a href="/posts/nlp/rs-tokenizer/">Rust TokenizerをC&#43;&#43;から呼び出す</a></li>
    
      <li><a href="/posts/nlp/bloom_lora_start/">BloomをLoRaで日本語finetuning</a></li>
    
      <li><a href="/posts/nlp/gpt_index_llama_index_start/">LlamaIndex(GPTIndex)を触る</a></li>
    
  </ul>


      </div>
    </article>
    
    
    
      

    
  </div>

    <footer>
  
  <div>
    
      <a href="https://twitter.com/if_004" name="twitter"
        ><i class="fab fa-twitter"></i
      ></a>
    
  </div>


  <div class="container">
    <div class="credits copyright">
      <a href="https://www.if-blog.site/pages/about">if_004</a>
      &nbsp;&copy;
      2025
      
        &nbsp;/&nbsp;
        <a href="https://www.if-blog.site">アンドロイドは推理小説を書くか?</a>
      
      &nbsp;&ndash;&nbsp;
      <i class="fas fa-moon" id="dark-mode-toggle"></i>

      <p class="credits theme-by">
         <a href="https://gohugo.io">Hugo</a>&nbsp;Theme
        <a href="https://github.com/matsuyoshi30/harbor">Harbor</a>
      </p>
    </div>
  </div>
</footer>

  </body>
</html>
