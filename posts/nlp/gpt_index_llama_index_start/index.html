<!DOCTYPE html>
<html lang="ja">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8" />
<title>LlamaIndex(GPTIndex)を触る</title>


<script
  data-ad-client="ca-pub-7303877370233278"
  async
  src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"
></script>

<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="alternate"
  type="application/rss+xml"
  href="http://localhost:1313/index.xml"
  title="アンドロイドは推理小説を書くか?"
/>

<link rel="stylesheet" href="http://localhost:1313//fontawesome/css/all.min.css" />




<link
  crossorigin="anonymous"
  href="/css/styles.min.css"
  integrity=""
  rel="preload stylesheet"
  as="style"
/>




<link
  id="dark-mode-theme"
  crossorigin="anonymous"
  href="/css/dark.min.css"
  integrity=""
  rel="preload stylesheet"
  as="style"
/>




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '\\[', right: '\\]', display: true},   
        {left: '$$', right: '$$', display: true},     
        {left: '\\(', right: '\\)', display: false},  
        {left: '$', right: '$', display: false},  
      ],
      throwOnError : false
    });
  });
</script>


<script>
  var darkTheme = document.getElementById('dark-mode-theme')
  var storedTheme = localStorage.getItem('dark-mode-storage')

  if (storedTheme === 'dark') {
    darkTheme.disabled = false
  } else if (storedTheme === 'light') {
    darkTheme.disabled = true
  }
</script>


<script defer crossorigin="anonymous" src="/js/theme.js" integrity=""></script>


<script defer crossorigin="anonymous" src="/js/instantpage.min.js" integrity=""></script>

  
    <meta name="description" content="LlamaIndexを動かしていきます。最近名前がGPTIndexから変わったみたい。
LlamaIndexとは、LLMのPromptの作成などを外部のデータソースと連携して行えるようにしたツールキット。
https://gpt-index.readthedocs.io/en/latest/guid" />
  


<meta name="generator" content="Hugo 0.110.0">
  </head>
  <body>
    
  




  <header>
    <nav class="navbar">
  <div class="nav">
    

    <ul class="nav-links">
      
        
          <li>
            <a href="/" name="Home" class="tooltip"
              ><i class="fas fa-home fa-lg"></i>
              <span>Home</span>
            </a>
          </li>
          
      
        
          <li>
            <a href="/pages/about" name="About" class="tooltip"
              ><i class="fas fa-user fa-lg"></i>
              <span>About</span>
            </a>
          </li>
          
      
        
          <li>
            <a href="/tags" name="Tags" class="tooltip"
              ><i class="fas fa-tag fa-lg"></i>
              <span>Tags</span>
            </a>
          </li>
          
      
        
          <li>
            <a href="/pages/search" name="Search" class="tooltip"
              ><i class="fas fa-search fa-lg"></i>
              <span>Search</span>
            </a>
          </li>
          
      
    </ul>
  </div>
</nav>

    <div class="intro-header">
      <div class="container">
        <div class="posts-heading">
          
            

              
              <h1>
		
		LlamaIndex(GPTIndex)を触る
		
		
                </h1>
              

              

              
            
          
          
          
          
        </div>
      </div>
    </div>
  </header>
  

    
  <div class="container" role="main">
    <article class="article" class="blog-post">
      
        <div class="blog-tags">
          <i class="fas fa-tag" style="color:#111111"></i>
          
            <a href="http://localhost:1313//tags/nlp/">nlp</a
            >&nbsp;
          
            <a href="http://localhost:1313//tags/deeplearning/">deeplearning</a
            >&nbsp;
          
            <a href="http://localhost:1313//tags/python/">python</a
            >&nbsp;
          
            <a href="http://localhost:1313//tags/huggingface/">huggingface</a
            >&nbsp;
          
        </div>
      

      
        <div style="margin-bottom: 10px;">
          <i class="fa fa-calendar-alt"></i
          >&nbsp;2023-03-04
        </div>
      

      


      <div class="article-main">
        <p>LlamaIndexを動かしていきます。最近名前がGPTIndexから変わったみたい。</p>
<p>LlamaIndexとは、LLMのPromptの作成などを外部のデータソースと連携して行えるようにしたツールキット。<br>
<a href="https://gpt-index.readthedocs.io/en/latest/guides/primer.html">https://gpt-index.readthedocs.io/en/latest/guides/primer.html</a><br>
データソースにはGoogle DocsやSlackなどが使えるらしい。</p>
<p>メインの機能は、DocsやSlack、Webの情報を体系化したIndexとして保持して、抽出できる機能でしょう。</p>
<p>OpenAIのAPIを使うことが多いと思いますが、今回はHugging faceのモデルを使ってみます。<br>
LangChainをwrapしている部分がいくつかあるので、LangChainについても知っておくと良いと思う。</p>
<h2 id="install">Install</h2>
<p>doc通り</p>
<p><code>pip install llama-index</code></p>
<p>このあたりも使うので入れておく。</p>
<p><code>pip install transformers sentencepiece torch langchain</code></p>
<h2 id="とりあえず動かす">とりあえず動かす</h2>
<p>大まかな流れは、ドキュメントからIndexを作成、Indexに対してqueryを投げ、抽出する、といった感じ。</p>
<p>ドキュメントからIndexを作成する際に、以下の４つを使う。</p>
<ul>
<li>llm_predictor</li>
<li>embed_model</li>
<li>prompt_helper</li>
<li>template</li>
</ul>
<h3 id="llm_predictorの作成">llm_predictorの作成</h3>
<p>llm_predictorは最終的な文章生成の際に使われる。インデックスの構築、挿入、クエリの走査中にも使用される場合がある。</p>
<p>Huggingfaceにあるmodelを使う例。モデルは好きなものを使ってください。</p>
<p>transformersのpipelineをlangchainのHuggingFacePipelineでwrap、<br>
langchainのHuggingFacePipelineをllama_indexのLLMPredictorでwrapする。</p>
<div class="code-block">
  <div class="body"><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model_name <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;facebook/xglm-564M&#34;</span>
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#ff79c6">=</span> XGLMTokenizer<span style="color:#ff79c6">.</span>from_pretrained(model_name)
</span></span><span style="display:flex;"><span>model <span style="color:#ff79c6">=</span> XGLMForCausalLM<span style="color:#ff79c6">.</span>from_pretrained(model_name)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pipe <span style="color:#ff79c6">=</span> pipeline(<span style="color:#f1fa8c">&#34;text2text-generation&#34;</span>,
</span></span><span style="display:flex;"><span>                model<span style="color:#ff79c6">=</span>model,
</span></span><span style="display:flex;"><span>                tokenizer<span style="color:#ff79c6">=</span>tokenizer,
</span></span><span style="display:flex;"><span>                torch_dtype<span style="color:#ff79c6">=</span>torch<span style="color:#ff79c6">.</span>float16)                
</span></span><span style="display:flex;"><span>llm <span style="color:#ff79c6">=</span> HuggingFacePipeline(pipeline<span style="color:#ff79c6">=</span>pipe)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>llm_predictor <span style="color:#ff79c6">=</span> LLMPredictor(llm<span style="color:#ff79c6">=</span>llm)</span></span></code></pre></div>
  </div>

</div>
<h3 id="embed_modelの作成">embed_modelの作成</h3>
<p>embed_modelはdocumentをsentense vectorにしindexを作成する際に使われる。　　
(queryによる抽出の方法がいくつかあるが、vectorの類似度から抽出する方法があるので、そこのembeddingでも使ってそう)　　</p>
<div class="code-block">
  <div class="body"><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model_name <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;sonoisa/sentence-bert-base-ja-en-mean-tokens&#34;</span>
</span></span><span style="display:flex;"><span>embeddings <span style="color:#ff79c6">=</span> HuggingFaceEmbeddings(model_name <span style="color:#ff79c6">=</span> model_name)
</span></span><span style="display:flex;"><span>embed_model <span style="color:#ff79c6">=</span> LangchainEmbedding(embeddings)</span></span></code></pre></div>
  </div>

</div>
<h3 id="prompt-helperの作成">prompt helperの作成</h3>
<p>tokenの制限に使われる。OpenAIのモデルを使う場合に設定するものだと思う。</p>
<div class="code-block">
  <div class="body"><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>prompt_helper<span style="color:#ff79c6">=</span>PromptHelper<span style="color:#ff79c6">.</span>from_llm_predictor(
</span></span><span style="display:flex;"><span>    llm_predictor<span style="color:#ff79c6">=</span>llm_predictor,
</span></span><span style="display:flex;"><span>    max_chunk_overlap<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0</span>,
</span></span><span style="display:flex;"><span>    chunk_size_limit<span style="color:#ff79c6">=</span><span style="color:#bd93f9">512</span>,
</span></span><span style="display:flex;"><span>    embedding_limit<span style="color:#ff79c6">=</span><span style="color:#bd93f9">1023</span>,
</span></span><span style="display:flex;"><span>)</span></span></code></pre></div>
  </div>

</div>
<h3 id="promptの作成">promptの作成</h3>
<p>日本語のpromptが欲しかったので作成。</p>
<div class="code-block">
  <div class="body"><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>QA_PROMPT_TMPL <span style="color:#ff79c6">=</span> (
</span></span><span style="display:flex;"><span>    <span style="color:#f1fa8c">&#34;以下は文脈情報です。</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f1fa8c">&#34;---------------------</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">{context_str}</span><span style="color:#f1fa8c">&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">---------------------</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f1fa8c">&#34;予備知識ではなく、文脈情報が与えられます。&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f1fa8c">&#34;質問: </span><span style="color:#f1fa8c">{query_str}</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f1fa8c">&#34;回答:</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>QA_PROMPT <span style="color:#ff79c6">=</span> QuestionAnswerPrompt(QA_PROMPT_TMPL)</span></span></code></pre></div>
  </div>

</div>
<h3 id="indexの作成">indexの作成</h3>
<p>これまでに作成した４つをあわせてindexを作成</p>
<ul>
<li>llm_predictor</li>
<li>embed_model</li>
<li>prompt_helper</li>
<li>template</li>
</ul>
<div class="code-block">
  <div class="body"><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>index <span style="color:#ff79c6">=</span> GPTSimpleVectorIndex(documents, 
</span></span><span style="display:flex;"><span>                            llm_predictor<span style="color:#ff79c6">=</span>llm_predictor,
</span></span><span style="display:flex;"><span>                            embed_model<span style="color:#ff79c6">=</span>embed_model,
</span></span><span style="display:flex;"><span>                            prompt_helper<span style="color:#ff79c6">=</span>prompt_helper,
</span></span><span style="display:flex;"><span>                            text_qa_template<span style="color:#ff79c6">=</span>QA_PROMPT,
</span></span><span style="display:flex;"><span>                            )</span></span></code></pre></div>
  </div>

</div>
<h3 id="query">query</h3>
<p>GPTSimpleVectorIndexに渡すtext_qa_templateと同じものを、queryにも渡します。</p>
<div class="code-block">
  <div class="body"><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>query_str <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;podについて教えてください。&#34;</span>
</span></span><span style="display:flex;"><span>response <span style="color:#ff79c6">=</span> index<span style="color:#ff79c6">.</span>query(query_str, text_qa_template<span style="color:#ff79c6">=</span>QA_PROMPT)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;response:&#34;</span>, response<span style="color:#ff79c6">.</span>response, <span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;source_nodes:&#34;</span>, response<span style="color:#ff79c6">.</span>source_nodes, <span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>)</span></span></code></pre></div>
  </div>

</div>
<h2 id="indexについて">Indexについて</h2>
<p>documentを読み込み、チャンクに分割、embeddingしたものNodeと呼ぶ。</p>
<p>このNode郡をIndexと呼ぶ<br>
Nodeの束ね方はには種類がいくつかあり、sequentialに並べただけのList Indexや階層構造を持つTree Indexがある。</p>
<p>Indexの種類ごとにqueryの走査が異なる。<br>
<a href="https://gpt-index.readthedocs.io/en/latest/guides/index_guide.html">https://gpt-index.readthedocs.io/en/latest/guides/index_guide.html</a></p>
<p>とりあえず動かす場合、vector store使うのがおすすめらしい<br>
<a href="https://gpt-index.readthedocs.io/en/latest/guides/use_cases.html#use-case-just-starting-out">https://gpt-index.readthedocs.io/en/latest/guides/use_cases.html#use-case-just-starting-out</a></p>
<p>vector storeは、いろいろなinmemory dbに対応してそう<br>
<a href="https://github.com/jerryjliu/gpt_index/blob/v0.4.20/gpt_index/indices/vector_store/vector_indices.py">https://github.com/jerryjliu/gpt_index/blob/v0.4.20/gpt_index/indices/vector_store/vector_indices.py</a></p>
<p>作成したindexに対し、追加/削除も可能<br>
<a href="https://gpt-index.readthedocs.io/en/latest/how_to/update.html">https://gpt-index.readthedocs.io/en/latest/how_to/update.html</a></p>
<h2 id="query-modeについて">query modeについて</h2>
<p>色々試し中<br>
<a href="https://gpt-index.readthedocs.io/en/latest/guides/usage_pattern.html#setting-response-mode">https://gpt-index.readthedocs.io/en/latest/guides/usage_pattern.html#setting-response-mode</a></p>
<h2 id="コードサンプル">コードサンプル</h2>
<p><a href="https://github.com/jerryjliu/gpt_index/tree/v0.4.20/examples">https://github.com/jerryjliu/gpt_index/tree/v0.4.20/examples</a></p>

      </div>
      <div class="related-article">
        

  <h2>See Also</h2>
  <ul>
    
      <li><a href="/posts/nlp/calm_lora_quantize/">OpenCALM-7Bをloraで学習して、quantizeするまで</a></li>
    
      <li><a href="/posts/nlp/rinna_lora_ja/">loraで学習する場合のpromptって何でも良いんだっけ？rinna instruction 3Bで試す</a></li>
    
      <li><a href="/posts/nlp/rinna_3b_cpp/">rinna 3Bをcppで動かす</a></li>
    
      <li><a href="/posts/nlp/rs-tokenizer/">Rust TokenizerをC&#43;&#43;から呼び出す</a></li>
    
      <li><a href="/posts/nlp/bloom_lora_start/">BloomをLoRaで日本語finetuning</a></li>
    
  </ul>


      </div>
    </article>
    
    
    
      

    
  </div>

    <footer>
  
  <div>
    
      <a href="https://twitter.com/if_004" name="twitter"
        ><i class="fab fa-twitter"></i
      ></a>
    
  </div>


  <div class="container">
    <div class="credits copyright">
      <a href="http://localhost:1313//pages/about">if_004</a>
      &nbsp;&copy;
      2025
      
        &nbsp;/&nbsp;
        <a href="http://localhost:1313/">アンドロイドは推理小説を書くか?</a>
      
      &nbsp;&ndash;&nbsp;
      <i class="fas fa-moon" id="dark-mode-toggle"></i>

      <p class="credits theme-by">
         <a href="https://gohugo.io">Hugo</a>&nbsp;Theme
        <a href="https://github.com/matsuyoshi30/harbor">Harbor</a>
      </p>
    </div>
  </div>
</footer>

  </body>
</html>
