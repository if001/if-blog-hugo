<!DOCTYPE html>
<html lang="ja">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8" />
<title>Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases (AI論文要約)</title>


<script
  data-ad-client="ca-pub-7303877370233278"
  async
  src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"
></script>

<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="alternate"
  type="application/rss+xml"
  href="http://localhost:1313/index.xml"
  title="アンドロイドは推理小説を書くか?"
/>

<link rel="stylesheet" href="http://localhost:1313//fontawesome/css/all.min.css" />




<link
  crossorigin="anonymous"
  href="/css/styles.min.css"
  integrity=""
  rel="preload stylesheet"
  as="style"
/>




<link
  id="dark-mode-theme"
  crossorigin="anonymous"
  href="/css/dark.min.css"
  integrity=""
  rel="preload stylesheet"
  as="style"
/>




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '\\[', right: '\\]', display: true},   
        {left: '$$', right: '$$', display: true},     
        {left: '\\(', right: '\\)', display: false},  
        {left: '$', right: '$', display: false},  
      ],
      throwOnError : false
    });
  });
</script>


<script>
  var darkTheme = document.getElementById('dark-mode-theme')
  var storedTheme = localStorage.getItem('dark-mode-storage')

  if (storedTheme === 'dark') {
    darkTheme.disabled = false
  } else if (storedTheme === 'light') {
    darkTheme.disabled = true
  }
</script>


<script defer crossorigin="anonymous" src="/js/theme.js" integrity=""></script>


<script defer crossorigin="anonymous" src="/js/instantpage.min.js" integrity=""></script>

  
    <meta name="description" content="AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。
https://ceur-ws.org/Vol-3290/short_paper2780.pdf
背景と目的 既存の語彙多様度指標（TTR、Shannon entropy 等）は 表層文字列のユニークさに依存するため，OCR" />
  


<meta name="generator" content="Hugo 0.110.0">
  </head>
  <body>
    
  




  <header>
    <nav class="navbar">
  <div class="nav">
    

    <ul class="nav-links">
      
        
          <li>
            <a href="/" name="Home" class="tooltip"
              ><i class="fas fa-home fa-lg"></i>
              <span>Home</span>
            </a>
          </li>
          
      
        
          <li>
            <a href="/pages/about" name="About" class="tooltip"
              ><i class="fas fa-user fa-lg"></i>
              <span>About</span>
            </a>
          </li>
          
      
        
          <li>
            <a href="/tags" name="Tags" class="tooltip"
              ><i class="fas fa-tag fa-lg"></i>
              <span>Tags</span>
            </a>
          </li>
          
      
        
          <li>
            <a href="/pages/search" name="Search" class="tooltip"
              ><i class="fas fa-search fa-lg"></i>
              <span>Search</span>
            </a>
          </li>
          
      
    </ul>
  </div>
</nav>

    <div class="intro-header">
      <div class="container">
        <div class="posts-heading">
          
            

              
              <h1>
		
		Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases (AI論文要約)
		
		
                </h1>
              

              

              
            
          
          
          
          
        </div>
      </div>
    </div>
  </header>
  

    
  <div class="container" role="main">
    <article class="article" class="blog-post">
      
        <div class="blog-tags">
          <i class="fas fa-tag" style="color:#111111"></i>
          
            <a href="http://localhost:1313//tags/nlp/">nlp</a
            >&nbsp;
          
            <a href="http://localhost:1313//tags/deeplearning/">deeplearning</a
            >&nbsp;
          
            <a href="http://localhost:1313//tags/paper_summary/">paper_summary</a
            >&nbsp;
          
        </div>
      

      
        <div style="margin-bottom: 10px;">
          <i class="fa fa-calendar-alt"></i
          >&nbsp;2025-05-18
        </div>
      

      


      <div class="article-main">
        <p>AIを使った論文要約です。簡単なサーベイ用で詳細は論文を参照してください。</p>
<p><a href="https://ceur-ws.org/Vol-3290/short_paper2780.pdf">https://ceur-ws.org/Vol-3290/short_paper2780.pdf</a></p>
<hr>
<h3 id="背景と目的">背景と目的</h3>
<ul>
<li>既存の語彙多様度指標（TTR、Shannon entropy 等）は <strong>表層文字列</strong>のユニークさに依存するため，OCR 誤りや歴史的綴り揺れを含むコーパスでは過大評価を招く。</li>
<li>生態学で用いられる <strong>属性多様度 (attribute diversity)</strong> の枠組みを導入し，「語彙項目同士の機能的 / 意味的距離」を考慮した <strong>Functional Diversity (FD)</strong> を Humanities 研究に適用することが本研究の目的。</li>
</ul>
<hr>
<h3 id="提案手法">提案手法</h3>
<ol>
<li>
<p><strong>語彙間距離の計算</strong></p>
<ul>
<li>歴史英語 LL﻿M <em>MacBERTh</em> で各トークンの文脈依存埋め込みを取得し，コサイン距離で語彙間距離行列 Δ を作成。</li>
</ul>
</li>
<li>
<p><strong>機能グループの定義</strong></p>
<ul>
<li>距離しきい値 θ を設定し，Δ ≤ θ の語を同一 <strong>functional group</strong> とみなす。</li>
</ul>
</li>
<li>
<p><strong>多様度の定式化</strong></p>
<ul>
<li>
<p>各語タイプ <em>i</em> の出現頻度 $f_i$ と機能グループ頻度 $g_i(θ)$ を用い，</p>
<p>$$
FD_{q}(θ) ;=;\Bigl(\sum_{i=1}^{A} \bigl[,\tfrac{f_i}{g_i(θ)},\bigr]^{q}\Bigr)^{\tfrac{1}{1-q}}
$$</p>
<p>と定義（$q=0$ は機能的リッチネス，$q=1$ は Shannon，$q=2$ は Gini–Simpson の逆数に対応）。</p>
</li>
<li>
<p>これは Chao &amp; Chiu の <strong>Functional Hill Numbers</strong> 拡張に一致する。</p>
</li>
</ul>
</li>
</ol>
<hr>
<h3 id="実験設定">実験設定</h3>
<ul>
<li><strong>コーパス</strong>: ARCHER 3.2（1600–1999 年，約 3.3 M 語，12 ジャンル）。</li>
<li><strong>ノイズシミュレーション</strong>: 確率 <em>p</em> = 0, 0.1, 0.2, 0.35, 0.5, 0.75 で各語内の文字をランダム置換し OCR / 綴り誤りを再現。</li>
<li>各設定で 5 回再生成し，FD と従来のタイプ数 (θ = θ_min) を比較。</li>
</ul>
<hr>
<h3 id="主な結果">主な結果</h3>
<table>
<thead>
<tr>
<th>観点</th>
<th>従来の「タイプ数」</th>
<th>Functional Diversity</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ノイズ耐性</strong></td>
<td><em>p</em> 上昇に伴いほぼ線形で過大評価</td>
<td>θ = θ_mean では変化が小さく安定</td>
</tr>
<tr>
<td><strong>ジャンル差</strong></td>
<td>広告 vs. 小説で顕著差出ず</td>
<td>広告は「列挙型」で functional group への凝集度が高く，小説は幅広い機能範囲を示す</td>
</tr>
<tr>
<td><strong>語数補正</strong></td>
<td>TTR では短文優遇</td>
<td>Hill Numbers により頻度分布まで統一尺度で比較可能</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<hr>
<h3 id="考察意義">考察・意義</h3>
<ol>
<li><strong>実務的利点</strong>: OCR エラーや歴史綴りを多く含むコーパスでも，前処理なしで頑健に語彙多様度を評価できる。</li>
<li><strong>理論的利点</strong>: 「語彙の幅 (breadth) と深さ (depth)」を分離評価でき，語彙豊富さに新しい次元を導入。</li>
<li><strong>ツールキットへの貢献</strong>: 生態学的属性多様度を計算人文学へ転用し，計量語彙研究の標準指標に機能的側面を追加。</li>
</ol>
<hr>
<h3 id="結論">結論</h3>
<p>Functional Diversity は</p>
<ul>
<li><strong>綴りノイズに強く</strong>，</li>
<li><strong>意味的内実を捉える</strong> 多様度指標であり，</li>
<li>歴史コーパスを含む計算人文学研究に有益である。</li>
</ul>
<p>著者らは FD を「従来のカテゴリカル指標の補完」と位置付け，公開済みモデル (<em>MacBERTh</em>) とスクリプトで再現可能な手法として提供している。</p>

      </div>
      <div class="related-article">
        

  <h2>See Also</h2>
  <ul>
    
      <li><a href="/posts/paper/paper19/">Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases (AI論文要約)</a></li>
    
      <li><a href="/posts/paper/paper17-2/">TinyHelen’s First Curriculum: Training and Evaluating Tiny Language Models in a Simpler Language Environment (AI論文要約)</a></li>
    
      <li><a href="/posts/paper/paper18/">Towards Data-Efficient Language Models: A Child-Inspired Approach to Language Learning (AI論文要約)</a></li>
    
      <li><a href="/posts/paper/paper16/">Language Model Can Listen While Speaking (AI論文要約)</a></li>
    
      <li><a href="/posts/paper/paper13/">MiniMax-01: Scaling Foundation Models with Lightning Attention (AI論文要約)</a></li>
    
  </ul>


      </div>
    </article>
    
    
    
      

    
  </div>

    <footer>
  
  <div>
    
      <a href="https://twitter.com/if_004" name="twitter"
        ><i class="fab fa-twitter"></i
      ></a>
    
  </div>


  <div class="container">
    <div class="credits copyright">
      <a href="http://localhost:1313//pages/about">if_004</a>
      &nbsp;&copy;
      2025
      
        &nbsp;/&nbsp;
        <a href="http://localhost:1313/">アンドロイドは推理小説を書くか?</a>
      
      &nbsp;&ndash;&nbsp;
      <i class="fas fa-moon" id="dark-mode-toggle"></i>

      <p class="credits theme-by">
         <a href="https://gohugo.io">Hugo</a>&nbsp;Theme
        <a href="https://github.com/matsuyoshi30/harbor">Harbor</a>
      </p>
    </div>
  </div>
</footer>

  </body>
</html>
